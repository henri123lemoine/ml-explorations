{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6Xcz8PzXxFi2"
   },
   "source": [
    "# Bandit algorithms\n",
    "\n",
    "*By*\n",
    "\n",
    "*Henri Lemoine; 261056402; henri.lemoine@mail.mcgill.ca*\n",
    "\n",
    "*Frederic Baroz; 261118133; frederic.baroz@mail.mcgill.ca*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWYbBcWnO_nz"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "urWG2cNcO_n1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED: int = 42\n",
    "SAVE_FILES: bool = True\n",
    "SHOW_GRAPHS: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2yfM-1iO_n2"
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZUUgcn8hO_n3"
   },
   "outputs": [],
   "source": [
    "def plot(\n",
    "    data: np.array,\n",
    "    title: str,\n",
    "    main_labels: list[str],\n",
    "    ax_titles: list[str],\n",
    "    algs_info: list[tuple[str, str, int]],\n",
    "    range_y: list[tuple[float, float]] = None,\n",
    "    size: list[int] = (10, 10),\n",
    "    fill_std: list[int] = None,\n",
    "    filename: str = None,\n",
    "    show: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (np.array): All that needs to be plotted. Shape: (n_figs, n_curves, n_steps, n_runs).\n",
    "        title (str): The title of the plot.\n",
    "        main_labels (list[str]): A list of strings, the first element being the label of the x axis, and the rest being the labels of the y axes.\n",
    "        ax_titles (list[str]): A list of titles for each subplot.\n",
    "        algs_info (list[tuple[str, str, int]]): A list of tuples containing the information of each curve. Each tuple contains the following: (name, color, marker type). The marker type is an integer denoting the type of marker to be used. 0: normal full line, 1: dashed line, 2: scatter plot.\n",
    "        range_y (list[tuple[float, float]], optional): A list of tuples of length n_figs, each denoting the range of the y axis of the corresponding subplot. Defaults to None.\n",
    "        size (list[int], optional): The size of the plot. Defaults to (10, 10).\n",
    "        fill_std (list[int], optional): A list of integers of length n_figs, each denoting whether and how to fill the standard deviation of the corresponding subplot. If None, no standard deviation will be filled. If 0, the standard deviation will be filled with a transparent color. If 1, the standard deviation will be a dashed line above and bellow the mean. If 2, just plot the standard deviation with a dashed line. Defaults to None.\n",
    "        filename (str, optional): The name of the file to save the plot to. If none, the plot will not be saved. Defaults to None.\n",
    "        show (bool, optional): Whether to show the plot or not. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    nb_figs, nb_curves, nb_steps, nb_runs = data.shape\n",
    "    range_x = (0, nb_steps)\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=nb_figs, ncols=1, figsize=size)\n",
    "    fig.suptitle(title, fontsize=18, fontweight=\"bold\", y=0.94)\n",
    "\n",
    "    for i in range(nb_figs):\n",
    "        if nb_figs == 1:\n",
    "            ax = axes\n",
    "        else:\n",
    "            ax = axes[i]\n",
    "\n",
    "        ax.set_title(ax_titles[i], fontsize=12, fontweight=\"bold\", loc=\"left\")\n",
    "\n",
    "        for j in range(nb_curves):\n",
    "            if algs_info[j][2] == 0:\n",
    "                mean = np.mean(data[i, j, :, :], axis=1)\n",
    "                std = np.std(data[i, j, :, :], axis=1) / np.sqrt(nb_runs)\n",
    "\n",
    "                ax.plot(mean, label=algs_info[j][0], color=algs_info[j][1])\n",
    "\n",
    "                if fill_std is not None:\n",
    "                    if fill_std[i] == None:\n",
    "                        pass\n",
    "                    elif fill_std[i] == 0:\n",
    "                        ax.fill_between(\n",
    "                            np.arange(nb_steps),\n",
    "                            mean - std,\n",
    "                            mean + std,\n",
    "                            alpha=0.2,\n",
    "                            color=algs_info[j][1],\n",
    "                        )\n",
    "                    elif fill_std[i] == 1:\n",
    "                        ax.plot(mean - std, color=algs_info[j][1], linestyle=\"--\")\n",
    "                        ax.plot(mean + std, color=algs_info[j][1], linestyle=\"--\")\n",
    "                    elif fill_std[i] == 2:\n",
    "                        ax.plot(std, color=algs_info[j][1], linestyle=\"--\")\n",
    "                    else:\n",
    "                        raise ValueError(\"Invalid fill_std value.\")\n",
    "\n",
    "            elif algs_info[j][2] == 1:\n",
    "                ax.axhline(\n",
    "                    y=np.mean(data[i, j, -1, :]),\n",
    "                    label=algs_info[j][0],\n",
    "                    color=algs_info[j][1],\n",
    "                    linestyle=\"--\",\n",
    "                )\n",
    "\n",
    "            elif algs_info[j][2] == 2:\n",
    "                ax.scatter(\n",
    "                    np.arange(nb_steps),\n",
    "                    np.mean(data[i, j, :, :], axis=1),\n",
    "                    s=10,\n",
    "                    label=algs_info[j][0],\n",
    "                    color=algs_info[j][1],\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid marker type.\")\n",
    "\n",
    "        ax.legend(prop={\"size\": 10}, loc=\"right\", bbox_to_anchor=(1.5, 0.5))\n",
    "\n",
    "        if i == nb_figs - 1:\n",
    "            ax.set_xlabel(main_labels[0], fontsize=14, labelpad=10)\n",
    "        ax.set_ylabel(main_labels[i + 1], fontsize=14, labelpad=10)\n",
    "\n",
    "        ax.set_xlim(range_x)\n",
    "        if range_y is not None and range_y[i] is not None:\n",
    "            ax.set_ylim(range_y[i])\n",
    "\n",
    "        ax.locator_params(nbins=10, axis=\"x\")\n",
    "        ax.locator_params(nbins=5, axis=\"y\")\n",
    "        ax.grid()\n",
    "\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename)  # , bbox_inches=Bbox([[0, 0], [13, 20]]))\n",
    "    if show:\n",
    "        plt.show()  # bbox_inches=Bbox([[0, 0], [13, 20]]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eyvZhcl1O_n5"
   },
   "source": [
    "\n",
    "## Question 1: k-armed bandit\n",
    "\n",
    "Write a small simulator for a Bernoulli bandit with $k$ arms. The probability of success $p_i$ for each arm $i \\in \\{1,...,k\\}$ should be provided as an input. The bandit should have a function called ”sample” which takes as input the index of an action and provides a reward sample. Recall that a Bernoulli bandit outputs either 1 or 0, drawn from a binomial distribution of parameter $p_k$. Test your code with 3 arms of parameters $q\\ast = [0.5, 0.5 - \\delta, 0.5 - 2\\delta]$, with $\\delta = 0.1$.\n",
    "Generate and save a set of 50 samples for each action. For the test, plot one graph for each action, containing the reward values obtained over the 100 draws, the empirical mean of the values, and the true $q∗$ for each arm. Each graph will have an x-axis that goes to 50, two horizontal lines (true value and estimated value) and a set of points of value 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKv5tw-8O_n7"
   },
   "source": [
    "### 1.1. Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kYjkkDircD-M"
   },
   "outputs": [],
   "source": [
    "class MAB_Q1:\n",
    "    def __init__(self, q_star):\n",
    "        self.q_star = q_star\n",
    "        self.k = len(q_star)  # number of arms\n",
    "\n",
    "        self.q = np.zeros(self.k)  # estimated values\n",
    "        self.n = np.zeros(self.k)  # number of times each arm was pulled\n",
    "\n",
    "        self.reward_history = [[] for _ in range(self.k)]\n",
    "        self.q_history = [[] for _ in range(self.k)]\n",
    "\n",
    "    def sample(self, a_i):\n",
    "        return np.random.binomial(1, self.q_star[a_i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MixocIjzIbU7"
   },
   "source": [
    "### 1.2. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Ke3FFwvFIfFa"
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "delta = 0.1\n",
    "q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "\n",
    "# Instantiating MAB with true expected values\n",
    "mab = MAB_Q1(q_star)\n",
    "\n",
    "nb_steps = 50\n",
    "\n",
    "data_Q1 = np.zeros((len(q_star), 3, nb_steps, 1))\n",
    "\n",
    "# Performing 50 samples for each action and storing the result in the reward history, and the estimated q values in the q history\n",
    "for i in range(nb_steps):\n",
    "    rewards = np.zeros(mab.k)\n",
    "    for action_ix in range(mab.k):\n",
    "        reward = mab.sample(action_ix)\n",
    "        mab.n[action_ix] += 1\n",
    "        mab.q[action_ix] += (reward - mab.q[action_ix]) / mab.n[action_ix]\n",
    "\n",
    "        mab.reward_history[action_ix].append(reward)\n",
    "        mab.q_history[action_ix].append(mab.q[action_ix])\n",
    "        rewards[action_ix] = mab.sample(action_ix)\n",
    "\n",
    "data_Q1[:, 0, :, 0] = mab.reward_history\n",
    "data_Q1[:, 1, :, 0] = mab.q_history\n",
    "data_Q1[:, 2, -1, 0] = q_star\n",
    "\n",
    "# Additionally, to ensure we use the same samples in future tests, we can store the reward history\n",
    "samples = mab.reward_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Plotting the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "tCCtPZFsO_oA"
   },
   "source": [
    "#### 1.3.1. First interpretation\n",
    "\n",
    "Although the question asked for the plot to have 2 horizonal lines and a series of 1-0 points, we found informative to plot the averaged reward over time (blue line). Section 1.3.2. shows another set of plots with two horizontal lines (final action value for each action)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Q1.shape\n",
    "# (3, 3, 50, 1)\n",
    "# 3 arms: 0, 1, 2\n",
    "# 3 curves: reward, q, q_star\n",
    "# 50 steps\n",
    "# 1 run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "zTX0oIKlO_oA",
    "outputId": "fe546953-bee1-4e8c-cfc1-2f48cbd9680d"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q1,\n",
    "    title=\"Question 1\\nK-armed Bandit (1)\",\n",
    "    main_labels=[\"Time steps\", \"Reward\", \"Reward\", \"Reward\"],\n",
    "    ax_titles=[\"Action 0\", \"Action 1\", \"Action 2\"],\n",
    "    algs_info=[(\"Reward values\", \"r\", 2), (\"Empirical mean\", \"b\", 0), (\"True q∗\", \"r\", 1)],\n",
    "    range_y=[(0, 1), (0, 1), (0, 1)],\n",
    "    filename=\"q1_p1.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SdIEWTi-I4NC"
   },
   "source": [
    "#### 1.3.2. Second interpretation\n",
    "\n",
    "Set of plot showing two horizontal lines as asked in the question with the blue dashed line indicating the final value estimation after 50 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "Bzx5CpiXJJQB",
    "outputId": "238ab6d7-3645-4fce-d036-e5be1ae9d3c7"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q1,\n",
    "    title=\"Question 1\\nK-armed Bandit (2)\",\n",
    "    main_labels=[\"Time steps\", \"Reward\", \"Reward\", \"Reward\"],\n",
    "    ax_titles=[\"Action 0\", \"Action 1\", \"Action 2\"],\n",
    "    algs_info=[\n",
    "        (\"Reward values\", \"r\", 2),\n",
    "        (\"Empirical expected action value\", \"b\", 1),\n",
    "        (\"True q∗\", \"r\", 1),\n",
    "    ],\n",
    "    range_y=[(0, 1), (0, 1), (0, 1)],\n",
    "    filename=\"q1_p2.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EiqaSZ7nrVHN"
   },
   "source": [
    "## Question 2: Update and UpdateAvr\n",
    "\n",
    "Code the rule for estimating action values discussed in lecture 2, with a fixed learning rate $\\alpha$, in a function called $\\text{update}$, and using the incremental computation of the mean presented in lecture 2, in a function called $\\text{updateAvg}$. Using the previous data, plot for each action a graph showing the estimated $q$ value as a function of the number of samples, using averaging as well as $\\alpha = 0.01$ and $\\alpha = 0.1$, and the true value. Each graph should have three curves and a horizontal line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78qjHjwGO_oD"
   },
   "source": [
    "\n",
    "### 2.1. Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Z-Z2lwhQxbNb"
   },
   "outputs": [],
   "source": [
    "class MAB_Q2:\n",
    "    def __init__(self, q_star: list[float], alpha: int = None):\n",
    "        self.q_star = q_star\n",
    "        self.k = len(q_star)\n",
    "\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.q = np.zeros(self.k)\n",
    "        self.n = np.zeros(self.k)\n",
    "\n",
    "        self.reward_history = [[] for _ in range(self.k)]\n",
    "        self.q_history = [[] for _ in range(self.k)]\n",
    "\n",
    "    def sample(self, action_ix: int) -> int:\n",
    "        return np.random.binomial(1, self.q_star[action_ix])\n",
    "\n",
    "    # Estimating action values with fixed alpha\n",
    "    def update(self, action_ix: int, reward=None):\n",
    "        # getting the reward from the binomial distribution of that action; if reward is given, use it instead\n",
    "        if reward is None:\n",
    "            reward = self.sample(action_ix)\n",
    "\n",
    "        if self.alpha is None:\n",
    "            raise ValueError(\"alpha is not defined\")\n",
    "\n",
    "        # updating the action value\n",
    "        self.n[action_ix] += 1\n",
    "        self.q[action_ix] += self.alpha * (reward - self.q[action_ix])\n",
    "\n",
    "        self.reward_history[action_ix].append(reward)\n",
    "        self.q_history[action_ix].append(self.q[action_ix])\n",
    "\n",
    "    # Estimating action values with incrementally computed means\n",
    "    def updateAvg(self, action_ix: int, reward=None):\n",
    "        # getting the reward from the binomial distribution of that action; if reward is given, use it instead\n",
    "        if reward is None:\n",
    "            reward = self.sample(action_ix)\n",
    "\n",
    "        # updating the action value\n",
    "        self.n[action_ix] += 1\n",
    "        self.q[action_ix] += (reward - self.q[action_ix]) / self.n[action_ix]\n",
    "\n",
    "        self.reward_history[action_ix].append(reward)\n",
    "        self.q_history[action_ix].append(self.q[action_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OPd0jg3O_oF"
   },
   "source": [
    "### 2.2. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "eCSy1SNKO_oF"
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "delta = 0.1\n",
    "q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "\n",
    "# Instantiating 3 MAB (one for each update method)\n",
    "mab_avg = MAB_Q2(q_star)\n",
    "mab_001 = MAB_Q2(q_star, 0.01)\n",
    "mab_01 = MAB_Q2(q_star, 0.1)\n",
    "\n",
    "nb_steps = 50\n",
    "\n",
    "data_Q2 = np.zeros((len(q_star), 4, nb_steps, 1))\n",
    "\n",
    "# Taking the same 50 samples for each action as in Q1, and storing the result in the reward history, and the estimated q values in the q history\n",
    "rewards = samples\n",
    "for step_ix in range(nb_steps):\n",
    "    for action_ix in range(mab_avg.k):\n",
    "        mab_avg.updateAvg(action_ix, rewards[action_ix][step_ix])\n",
    "        mab_001.update(action_ix, rewards[action_ix][step_ix])\n",
    "        mab_01.update(action_ix, rewards[action_ix][step_ix])\n",
    "\n",
    "data_Q2[:, 0, :, 0] = mab_avg.q_history\n",
    "data_Q2[:, 1, :, 0] = mab_001.q_history\n",
    "data_Q2[:, 2, :, 0] = mab_01.q_history\n",
    "data_Q2[:, 3, -1, 0] = q_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufWEF0NuO_oF"
   },
   "source": [
    "### 2.3. Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "osW8OJb1O_oF",
    "outputId": "ab1b1a8e-69ec-4da6-af5d-6fa546deeade"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q2,\n",
    "    title=\"Question 2\\nUpdate & UpdateAvg\",\n",
    "    main_labels=[\"Time steps\", \"Action value\", \"Action value\", \"Action value\"],\n",
    "    ax_titles=[\"Action 0\", \"Action 1\", \"Action 2\"],\n",
    "    algs_info=[\n",
    "        (\"Empirical mean\", \"tab:blue\", 0),\n",
    "        (r\"$\\alpha = 0.01$\", \"tab:green\", 0),\n",
    "        (r\"$\\alpha = 0.1$\", \"tab:orange\", 0),\n",
    "        (\"True q∗\", \"r\", 1),\n",
    "    ],\n",
    "    range_y=[(0, 1), (0, 1), (0, 1)],\n",
    "    filename=\"q2.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiaybVuq3VQh"
   },
   "source": [
    "## Question 3: Best averages experiment\n",
    "\n",
    "Repeat the above experiment 100 times, starting with action value estimates of 0. Each run will still contain 100 samples for each action. Plot the same graph as above, but where the curves have the average and standard error over the 100 runs. \n",
    "Explain in 1-2 sentences what you observe. Which of the α values is better? How do they compare to averaging? If you wanted to optimize further, in what range of α would you look for better values?\n",
    "\n",
    "Using MAB_Q2 from question 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07H6eTPpO_oH"
   },
   "source": [
    "### 3.1. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "gG1W7wc9xxAO"
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "delta = 0.1\n",
    "q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "nb_runs = 100\n",
    "nb_steps = 100\n",
    "\n",
    "data_Q3 = np.zeros((len(q_star), 4, nb_steps, nb_runs))\n",
    "\n",
    "for run_ix in range(nb_runs):\n",
    "    mab_avg = MAB_Q2(q_star, None)\n",
    "    mab_001 = MAB_Q2(q_star, 0.01)\n",
    "    mab_01 = MAB_Q2(q_star, 0.1)\n",
    "\n",
    "    for step_ix in range(nb_steps):\n",
    "        for action_ix in range(mab_avg.k):\n",
    "            reward = mab_avg.sample(action_ix)\n",
    "            mab_avg.updateAvg(action_ix, reward)\n",
    "            mab_001.update(action_ix, reward)\n",
    "            mab_01.update(action_ix, reward)\n",
    "    data_Q3[:, 0, :, run_ix] = mab_avg.q_history\n",
    "    data_Q3[:, 1, :, run_ix] = mab_001.q_history\n",
    "    data_Q3[:, 2, :, run_ix] = mab_01.q_history\n",
    "    data_Q3[:, 3, -1, run_ix] = q_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5aLaD-07O_oI"
   },
   "source": [
    "### 3.2. Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 659
    },
    "id": "xfe9LBPwO_oI",
    "outputId": "69f8d52c-332c-411d-f6e5-fcc901d1c7b5"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q3,\n",
    "    title=\"Question 3\\nBest averages experiment\",\n",
    "    main_labels=[\"Time steps\", \"\", f\"Empirical expected value (averaged over {nb_runs} runs)\", \"\"],\n",
    "    ax_titles=[\"Action 0\", \"Action 1\", \"Action 2\"],\n",
    "    algs_info=[\n",
    "        (\"Empirical mean\", \"tab:blue\", 0),\n",
    "        (r\"$\\alpha = 0.01$\", \"tab:green\", 0),\n",
    "        (r\"$\\alpha = 0.1$\", \"tab:orange\", 0),\n",
    "        (\"True q∗\", \"r\", 1),\n",
    "    ],\n",
    "    range_y=[(0, 1), (0, 1), (0, 1)],\n",
    "    fill_std=[0, 0, 0, None],\n",
    "    filename=\"q3.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh9RXCLB8HAt"
   },
   "source": [
    "As we can see, incremental averaging converges quickly; it is stable and its standard error is relatively small and continuously decreasing.\n",
    "\n",
    "Though Alpha = 0.1 has a good initial convergence rate, its standard error remains large after 100 steps. For Alpha = 0.01, the convergence rate is slow, so we should expect longer nb_steps to provide better results. It has a small standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hF1c7e29LTb"
   },
   "source": [
    "## Question 4: $\\epsilon$-greedy Algorithm\n",
    "\n",
    "Code the ε-greedy algorithm discussed in class, with averaging updates, with ε provided as an input. You will run 100 independent runs, each consisting of 1000 time steps. Plot the following graphs:\n",
    "\n",
    "a.  The reward received over time, averaged at each time step over the 100 independent runs (with no smoothing over the time steps), and the standard error over the 100 runs\n",
    "\n",
    "b.  The fraction of runs (out of 100) in which the first action (which truly is best) is also estimated best based on the action values\n",
    "\n",
    "c.  The instantaneous regret l_t (as discussed in lecture 3) (averaged over the 100 runs)\n",
    "\n",
    "d.  The total regret L_t up to time step t (as discussed in lecture 3) (averaged over the 100 runs)\n",
    "\n",
    "Generate this set of graphs, for the following values of ε: 0, 1/8, 1/4, 1/2, 1.\n",
    "\n",
    "Explain what you observe in the graphs and discuss the effect of ε you observe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iQ9o3iADhTin"
   },
   "source": [
    "**Remarks**\n",
    "- *** I definitely think the fill for standard error was nice for Q3 but here... what we could do is provide this as first version and provide alternate view with only the means and SE deviation as second (dotted/dashed) lines since they do not appear at the same value (similar to what I originaly did)... so a set of 4 graphs with only reward  with SE, and the 1 additional graph for reward with alternative way of plotting SE (like absolute value of SE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs51ZWdGO_oK"
   },
   "source": [
    "### 4.1. Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "miBFXtVm9GCd"
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedy_MAB_Q4(MAB_Q2):\n",
    "    def __init__(self, q_star: list[float], epsilon: int = 0, alpha: int = None):\n",
    "        super().__init__(q_star, alpha)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.currentStep = 0\n",
    "        self.total_reward = 0\n",
    "        self.total_regret = 0\n",
    "        self.fraction_correct = 0\n",
    "\n",
    "        self.mean_reward_history = []\n",
    "        self.fraction_correct_history = []\n",
    "        self.instant_regret_history = []\n",
    "        self.total_regret_history = []\n",
    "\n",
    "    def draw_sample(self):\n",
    "        action_ix = self.epsilon_greedy_choice()\n",
    "        self.update(action_ix)\n",
    "\n",
    "    def epsilon_greedy_choice(self) -> int:\n",
    "        # Choosing action with epsilon-greedy policy\n",
    "        e_greedy_choice = np.random.choice([\"random\", \"greedy\"], p=[self.epsilon, 1 - self.epsilon])\n",
    "        if e_greedy_choice == \"random\":\n",
    "            return np.random.choice(self.k)\n",
    "        elif e_greedy_choice == \"greedy\":\n",
    "            return np.random.choice(np.flatnonzero(self.q == self.q.max()))\n",
    "\n",
    "    def update(self, action_ix: int, reward=None) -> None:\n",
    "        # getting the reward from the binomial distribution of that action; if reward is given, use it instead\n",
    "        if reward is None:\n",
    "            reward = self.sample(action_ix)\n",
    "\n",
    "        self.n[action_ix] += 1\n",
    "        if self.alpha is None:\n",
    "            self.q[action_ix] += (reward - self.q[action_ix]) / self.n[action_ix]\n",
    "        else:\n",
    "            self.q[action_ix] += self.alpha * (reward - self.q[action_ix])\n",
    "\n",
    "        self.q_history.append(self.q)\n",
    "\n",
    "        self.total_reward += reward\n",
    "        self.mean_reward_history.append(reward)  # this will be averaged over nb_runs in plot\n",
    "        self.instant_regret_history.append(max(self.q_star) - self.q_star[action_ix])\n",
    "        self.total_regret += self.instant_regret_history[-1]\n",
    "        self.total_regret_history.append(self.total_regret)\n",
    "        self.fraction_correct_history.append(\n",
    "            1 if action_ix == np.argmax(self.q_star) else 0\n",
    "        )  # this will be averaged over nb_runs in plot\n",
    "\n",
    "        self.currentStep += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eG4dlrdD7P77"
   },
   "source": [
    "### 4.2. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hxOAk4y-O_oM"
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "\n",
    "delta = 0.1\n",
    "q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "nb_runs = 100\n",
    "nb_steps = 1000\n",
    "\n",
    "epsilons = [0, 1 / 8, 1 / 4, 1 / 2, 1]\n",
    "\n",
    "data_Q4 = np.zeros((4, len(epsilons), nb_steps, nb_runs))\n",
    "\n",
    "for run_ix in range(nb_runs):\n",
    "    for e_ix, epsilon in enumerate(epsilons):\n",
    "        mab = EpsilonGreedy_MAB_Q4(q_star, epsilon)\n",
    "        for step_ix in range(nb_steps):\n",
    "            mab.draw_sample()\n",
    "        data_Q4[0, e_ix, :, run_ix] = mab.mean_reward_history\n",
    "        data_Q4[1, e_ix, :, run_ix] = mab.fraction_correct_history\n",
    "        data_Q4[2, e_ix, :, run_ix] = mab.instant_regret_history\n",
    "        data_Q4[3, e_ix, :, run_ix] = mab.total_regret_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Plotting the results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2oznmflwO_oM"
   },
   "source": [
    "#### 4.3.1. With standard errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SmbYYBhcO_oN",
    "outputId": "aaa36270-22cb-4c6e-f6ce-6b6c4c74600f"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q4,\n",
    "    title=\"Question 5\\nepsilon-greedy bandit\",\n",
    "    main_labels=[\n",
    "        \"Time steps\",\n",
    "        \"Mean reward\",\n",
    "        \"% optimal actions\",\n",
    "        \"Instantaneous regret (averaged)\",\n",
    "        \"Total regret (averaged)\",\n",
    "    ],\n",
    "    ax_titles=[\n",
    "        f\"Mean reward and SE averaged over {nb_runs} runs\",\n",
    "        f\"Fraction of selected best actions averaged over {nb_runs} runs\",\n",
    "        f\"Instantaneous regret averaged over {nb_runs} runs\",\n",
    "        f\"Total regret averaged over {nb_runs} runs\",\n",
    "    ],\n",
    "    algs_info=[\n",
    "        (r\"$\\epsilon = 0$\", \"tab:blue\", 0),\n",
    "        (r\"$\\epsilon = \\frac{1}{8}$\", \"tab:green\", 0),\n",
    "        (r\"$\\epsilon = \\frac{1}{4}$\", \"tab:orange\", 0),\n",
    "        (r\"$\\epsilon = \\frac{1}{2}$\", \"tab:red\", 0),\n",
    "        (r\"$\\epsilon = 1$\", \"tab:purple\", 0),\n",
    "    ],\n",
    "    size=(10, 20),\n",
    "    range_y=[\n",
    "        (0, 1),\n",
    "        (0, 1),\n",
    "        (0, np.max(np.mean(data_Q4[2, :, :, :], axis=2) * 1.1)),\n",
    "        (0, np.max(np.mean(data_Q4[3, :, :, :], axis=2) * 1.1)),\n",
    "    ],\n",
    "    fill_std=[0, None, None, None],\n",
    "    filename=\"q4.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ayvFRGvFO_oO"
   },
   "source": [
    "#### 4.3.2 Without standard errors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO CHANGE; I'M NOT SURE WHAT YOU WERE TRYING TO SAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BhRNNdZhO_oO",
    "outputId": "f4764333-4d3a-47d4-fb8d-24bace6eed39"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q4[:1, 1:2, :, :],\n",
    "    title=\"Question 4\\nEpsilon-greedy bandit (no standard errors)\",\n",
    "    main_labels=[\"Time steps\", \"Mean reward\"],\n",
    "    ax_titles=[\"\"],\n",
    "    algs_info=[(r\"$\\epsilon = 0$\", \"tab:blue\", 0)],\n",
    "    #    (r'$\\epsilon = \\frac{1}{8}$', 'tab:green', 0),\n",
    "    #    (r'$\\epsilon = \\frac{1}{4}$', 'tab:orange', 0),\n",
    "    #    (r'$\\epsilon = \\frac{1}{2}$', 'tab:red', 0),\n",
    "    #    (r'$\\epsilon = 1$', 'tab:purple', 0)],\n",
    "    size=(10, 10),\n",
    "    range_y=[(0, 1)],\n",
    "    fill_std=[None],\n",
    "    filename=\"q4_2.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqa4yz7J5zdy"
   },
   "source": [
    "## Question 5: Experiment\n",
    "\n",
    "For $\\epsilon = 1/4$ and $\\epsilon = 1/8$, plot the same graphs for $\\alpha = 0.1$, $\\alpha = 0.01$, $\\alpha = 0.001$ and averaging.\n",
    "\n",
    "Explain in 2 sentences what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwR1nx70O_oP"
   },
   "source": [
    "\n",
    "We use the class MAB_Q4 of question 4 which already contains the update() method for fixed stepsize parameter alpha.\n",
    "\n",
    "Again we draw plots for mean reward + SE, % optimal actions, instantaneous and total regret. Each graph contains 8 curves (as understood from the assignement), one for each combination of epsilon (1/8 and 1/4) and alpha (0.1, 0.01, 0.001) + incremental averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWPV_Up4O_oP"
   },
   "source": [
    "### 5.1. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vbQcyYG5O_oQ"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "delta = 0.1\n",
    "q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "nb_runs = 100\n",
    "nb_steps = 1000\n",
    "\n",
    "epsilons = [1 / 8, 1 / 4]\n",
    "alphas = [0.1, 0.01, 0.001, None]\n",
    "\n",
    "data_Q5 = np.zeros((4, len(epsilons) * len(alphas), nb_steps, nb_runs))\n",
    "\n",
    "for run_ix in range(nb_runs):\n",
    "    for e_ix, epsilon in enumerate(epsilons):\n",
    "        for a_ix, alpha in enumerate(alphas):\n",
    "            mab = EpsilonGreedy_MAB_Q4(q_star, epsilon, alpha)\n",
    "            for step_ix in range(nb_steps):\n",
    "                mab.draw_sample()\n",
    "            data_Q5[0, e_ix * len(alphas) + a_ix, :, run_ix] = mab.mean_reward_history\n",
    "            data_Q5[1, e_ix * len(alphas) + a_ix, :, run_ix] = mab.fraction_correct_history\n",
    "            data_Q5[2, e_ix * len(alphas) + a_ix, :, run_ix] = mab.instant_regret_history\n",
    "            data_Q5[3, e_ix * len(alphas) + a_ix, :, run_ix] = mab.total_regret_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCHfwBM5O_oQ"
   },
   "source": [
    "### 5.2. Plotting the results (with std error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y45x0DbPO_oR",
    "outputId": "e4718f2e-15c0-4c63-b668-0d43a57eff07"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q5,\n",
    "    title=\"Question 5\\nepsilon-greedy bandit with alpha updates\",\n",
    "    main_labels=[\n",
    "        \"Time steps\",\n",
    "        \"Mean reward\",\n",
    "        \"% optimal actions\",\n",
    "        \"Instantaneous regret (averaged)\",\n",
    "        \"Total regret (averaged)\",\n",
    "    ],\n",
    "    ax_titles=[\n",
    "        f\"Mean reward and SE averaged over {nb_runs} runs\",\n",
    "        f\"Fraction of selected best actions averaged over {nb_runs} runs\",\n",
    "        f\"Instantaneous regret averaged over {nb_runs} runs\",\n",
    "        f\"Total regret averaged over {nb_runs} runs\",\n",
    "    ],\n",
    "    algs_info=[\n",
    "        (r\"$\\epsilon = 1/8, \\alpha = 0.1$\", \"tab:blue\", 0),\n",
    "        (r\"$\\epsilon = 1/8, \\alpha = 0.01$\", \"tab:green\", 0),\n",
    "        (r\"$\\epsilon = 1/8, \\alpha = 0.001$\", \"tab:orange\", 0),\n",
    "        (r\"$\\epsilon = 1/8$, Empirical mean\", \"tab:red\", 0),\n",
    "        (r\"$\\epsilon = 1/4, \\alpha = 0.1$\", \"tab:purple\", 0),\n",
    "        (r\"$\\epsilon = 1/4, \\alpha = 0.01$\", \"tab:brown\", 0),\n",
    "        (r\"$\\epsilon = 1/4, \\alpha = 0.001$\", \"tab:pink\", 0),\n",
    "        (r\"$\\epsilon = 1/4$, Empirical mean\", \"tab:gray\", 0),\n",
    "    ],\n",
    "    size=(10, 20),\n",
    "    range_y=[\n",
    "        (0, 1),\n",
    "        (0, 1),\n",
    "        (0, np.max(np.mean(data_Q5[2, :, :, :], axis=2) * 1.1)),\n",
    "        (0, np.max(np.mean(data_Q5[3, :, :, :], axis=2) * 1.1)),\n",
    "    ],\n",
    "    fill_std=[0, None, None, None],\n",
    "    filename=\"q5.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEEuDwt-O_oS"
   },
   "source": [
    "### 5.2. Plotting the results (without std error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rdnp72g2O_oS",
    "outputId": "03d3dc97-338e-4785-d104-f9ecdc56755a"
   },
   "outputs": [],
   "source": [
    "# plot(\n",
    "#     data=data_Q5,\n",
    "#     title='Question 5\\nEpsilon-greedy bandit',\n",
    "#     main_labels=['Time steps', 'Average reward', 'Fraction of correct actions', 'Instant regret', 'Total regret'],\n",
    "#     ax_titles=['Average reward', 'Fraction of correct actions', 'Instant regret', 'Total regret'],\n",
    "#     algs_info=[('Epsilon = 1/8, Alpha = 0.1', 'tab:blue', 0), ('Epsilon = 1/8, Alpha = 0.01', 'tab:green', 0), ('Epsilon = 1/8, Alpha = 0.001', 'tab:orange', 0), ('Epsilon = 1/8, Alpha = None', 'tab:red', 0), ('Epsilon = 1/4, Alpha = 0.1', 'tab:purple', 0), ('Epsilon = 1/4, Alpha = 0.01', 'tab:brown', 0), ('Epsilon = 1/4, Alpha = 0.001', 'tab:pink', 0), ('Epsilon = 1/4, Alpha = None', 'tab:gray', 0)],\n",
    "#     size=(10, 20),\n",
    "#     fill_std=False,\n",
    "#     filename='Files/q5_no_std_errors.png'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jH69iFxJgy_"
   },
   "source": [
    "## Question 6: UCB Algorithm\n",
    "\n",
    "Write a function that implements the UCB algorithm. Set c = 2.\n",
    "\n",
    "Plot the same graphs as above for α = 0.1, α = 0.01, α = 0.001 and averaging.\n",
    "\n",
    "Explain briefly the behavior you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RggoeZw9O_oT"
   },
   "source": [
    "### 6.1. Class definition\n",
    "\n",
    "First we define in MAB_Q6 a multi arm bandit with the UCB policy for action selection (inside the method play())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "huCChB1WO_oT"
   },
   "outputs": [],
   "source": [
    "class UCB_MAB_Q6(EpsilonGreedy_MAB_Q4):\n",
    "    def __init__(self, q_star: list[float], alpha: int = None, c: int = 2):\n",
    "        super().__init__(q_star, alpha)\n",
    "        self.c = c\n",
    "\n",
    "    def draw_sample(self):\n",
    "        action_ix = self.ucb_choice()\n",
    "        self.update(action_ix)\n",
    "\n",
    "    def ucb_choice(self):\n",
    "        # Choose the greedy UCB action. Break ties randomly\n",
    "        ucb_choice = np.argmax(self.q + (self.c * np.sqrt(np.log(self.currentStep) / self.n)))\n",
    "        return np.random.choice(np.flatnonzero(self.q == self.q[ucb_choice]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6GGm6IDO_oU"
   },
   "source": [
    "### 6.2. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_znWjKW_O_oV"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "delta = 0.1\n",
    "q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "nb_runs = 100\n",
    "nb_steps = 1000\n",
    "\n",
    "alphas = [0.1, 0.01, 0.001, None]\n",
    "c = 2\n",
    "\n",
    "data_Q6 = np.zeros((4, len(alphas), nb_steps, nb_runs))\n",
    "\n",
    "for run_ix in range(nb_runs):\n",
    "    for a_ix, alpha in enumerate(alphas):\n",
    "        mab = UCB_MAB_Q6(q_star, alpha, c)\n",
    "        for action_ix in range(len(q_star)):\n",
    "            mab.update(action_ix)\n",
    "        for step_ix in range(nb_steps - len(q_star)):\n",
    "            mab.draw_sample()\n",
    "        data_Q6[0, a_ix, :, run_ix] = mab.mean_reward_history\n",
    "        data_Q6[1, a_ix, :, run_ix] = mab.fraction_correct_history\n",
    "        data_Q6[2, a_ix, :, run_ix] = mab.instant_regret_history\n",
    "        data_Q6[3, a_ix, :, run_ix] = mab.total_regret_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VsvLDWVyO_oV"
   },
   "source": [
    "### 6.3. Plotting the results (with std error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bTJIPeAOO_oV",
    "outputId": "1d04e6ca-b505-4155-f125-098622e24629"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q6,\n",
    "    title=\"Question 6\\nUCB bandit\",\n",
    "    main_labels=[\n",
    "        \"Time steps\",\n",
    "        \"Mean reward\",\n",
    "        \"% optimal actions\",\n",
    "        \"Instantaneous regret (averaged)\",\n",
    "        \"Total regret (averaged)\",\n",
    "    ],\n",
    "    ax_titles=[\n",
    "        f\"Mean reward and SE averaged over {nb_runs} runs\",\n",
    "        f\"Fraction of selected best actions averaged over {nb_runs} runs\",\n",
    "        f\"Instantaneous regret averaged over {nb_runs} runs\",\n",
    "        f\"Total regret averaged over {nb_runs} runs\",\n",
    "    ],\n",
    "    algs_info=[\n",
    "        (rf\"$\\alpha = {alphas[0]}$, $C = {c}$\", \"tab:blue\", 0),\n",
    "        (rf\"$\\alpha = {alphas[1]}$, $C = {c}$\", \"tab:green\", 0),\n",
    "        (rf\"$\\alpha = {alphas[2]}$, $C = {c}$\", \"tab:orange\", 0),\n",
    "        (rf\"Empirical mean, $C = {c}$\", \"tab:red\", 0),\n",
    "    ],\n",
    "    size=(10, 20),\n",
    "    range_y=[\n",
    "        (0, 1),\n",
    "        (0, 1),\n",
    "        (0, np.max(np.mean(data_Q6[2, :, :, :], axis=2) * 1.1)),\n",
    "        (0, np.max(np.mean(data_Q6[3, :, :, :], axis=2) * 1.1)),\n",
    "    ],\n",
    "    fill_std=[0, None, None, None],\n",
    "    filename=\"q6.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mC_5aGlSO_oX"
   },
   "source": [
    "### 6.3. Plotting the results (without std error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OrSfucrtO_oX",
    "outputId": "abb09703-a3f9-4548-e772-7fe9a9bb251c"
   },
   "outputs": [],
   "source": [
    "# plot(\n",
    "#     data=data_Q6,\n",
    "#     title='Question 6\\nUCB bandit',\n",
    "#     main_labels=['Time steps', 'Average reward', 'Fraction of correct actions', 'Instant regret', 'Total regret'],\n",
    "#     ax_titles=['Average reward', 'Fraction of correct actions', 'Instant regret', 'Total regret'],\n",
    "#     algs_info=[('Alpha = 0.1, C = 2', 'tab:blue', 0), ('Alpha = 0.01, C = 2', 'tab:green', 0), ('Alpha = 0.001, C = 2', 'tab:orange', 0), ('Alpha = None, C = 2', 'tab:red', 0)],\n",
    "#     size=(10, 20),\n",
    "#     fill_std=False,\n",
    "#     filename='Files/q6_no_std_errors.png'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R3x9fq5X0Nx"
   },
   "source": [
    "## Question 7: Thomson Sampling Algorithm\n",
    "\n",
    "Write a function that implements the Thompson sampling.\n",
    "\n",
    "Plot the same graphs as above for $\\alpha = 0.1$, $\\alpha = 0.01$, $\\alpha = 0.001$ and averaging.\n",
    "\n",
    "Explain briefly the behavior you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAr9sRLFO_oY"
   },
   "source": [
    "### 7.1. Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "NT7--QDmX2X_"
   },
   "outputs": [],
   "source": [
    "class ThomsonSampling_MAB_Q7(EpsilonGreedy_MAB_Q4):\n",
    "    def __init__(self, q_star: list[float], alpha: int = None):\n",
    "        super().__init__(q_star, alpha)\n",
    "\n",
    "        # storing the number of successes for each actions to parametrize beta distribution\n",
    "        # in the case of bernouilli, for a given action, number of failure is number of times action has been selected minus number of success\n",
    "        self.nb_successes = np.zeros(len(q_star))\n",
    "\n",
    "    def draw_sample(self):\n",
    "        action_ix = self.thomson_sampling_choice()\n",
    "        self.update(action_ix)\n",
    "        self.nb_successes[action_ix] += reward\n",
    "\n",
    "    def thomson_sampling_choice(self):\n",
    "        # Choose the greedy thomson sampling action. Break ties randomly\n",
    "        thomson_sampling_choice = np.argmax(\n",
    "            np.random.beta(self.nb_successes + 1, self.n - self.nb_successes + 1)\n",
    "        )\n",
    "        return np.random.choice(np.flatnonzero(self.q == self.q[thomson_sampling_choice]))\n",
    "        # ucb_choice = np.argmax(self.q + (self.c * np.sqrt(np.log(self.currentStep)/self.n)))\n",
    "        # return np.random.choice(np.flatnonzero(self.q == self.q[ucb_choice]))\n",
    "\n",
    "        # thomson_choice = np.random.beta(self.nb_successes + 1, self.n - self.nb_successes + 1)\n",
    "        # return np.random.choice(np.flatnonzero(thomson_choice == thomson_choice.max()))\n",
    "\n",
    "    # def update(self, action_ix: int, reward=None) -> float:\n",
    "    #   # getting the reward from the binomial distribution of that action; if reward is given, use it instead\n",
    "    #   if reward is None:\n",
    "    #     reward = self.sample(action_ix)\n",
    "    #   self.nb_successes[action_ix] += reward\n",
    "\n",
    "    #   self.n[action_ix] += 1\n",
    "    #   if self.alpha is None:\n",
    "    #     self.q[action_ix] += (reward - self.q[action_ix]) / self.n[action_ix]\n",
    "    #   else:\n",
    "    #     self.q[action_ix] += self.alpha * (reward - self.q[action_ix])\n",
    "\n",
    "    #   self.q_history.append(self.q)\n",
    "\n",
    "    #   self.total_reward += reward\n",
    "    #   self.mean_reward_history.append(self.total_reward / (self.currentStep + 1))\n",
    "    #   self.instant_regret_history.append(max(self.q_star) - self.q_star[action_ix])\n",
    "    #   self.total_regret += self.instant_regret_history[-1]\n",
    "    #   self.total_regret_history.append(self.total_regret)\n",
    "    #   is_correct_action = 1 if action_ix == np.argmax(self.q_star) else 0\n",
    "    #   self.fraction_correct = (self.fraction_correct * self.currentStep + is_correct_action) / (self.currentStep + 1)\n",
    "    #   self.fraction_correct_history.append(self.fraction_correct)\n",
    "\n",
    "    #   self.currentStep += 1\n",
    "\n",
    "\n",
    "# class ThompsonSampling(Bandit):\n",
    "#     def __init__(self, p, seed=0):\n",
    "#         super().__init__(p, seed)\n",
    "#         self.N = np.zeros((self.k, 2))\n",
    "\n",
    "#     def run_thompson_sampling(self, n):\n",
    "#         for i in range(n):\n",
    "#             action = np.argmax(np.random.beta(self.N + 1, 1 - self.N))\n",
    "#             reward, regret = self.sample(action)\n",
    "#             self.N[action] += reward\n",
    "#             self.total_regret += regret\n",
    "#             self.regret_history.append(self.total_regret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CmRSOQrrO_oZ"
   },
   "source": [
    "### 7.2. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Jt4zqEaIO_oZ"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "delta = 0.1\n",
    "q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "nb_runs = 30\n",
    "nb_steps = 1000\n",
    "\n",
    "alphas = [0.1, 0.01, 0.001, None]\n",
    "\n",
    "data_Q7 = np.zeros((4, len(alphas), nb_steps, nb_runs))\n",
    "\n",
    "for run_ix in range(nb_runs):\n",
    "    for a_ix, alpha in enumerate(alphas):\n",
    "        mab = ThomsonSampling_MAB_Q7(q_star, alpha)\n",
    "        for step_ix in range(nb_steps):\n",
    "            mab.draw_sample()\n",
    "        data_Q7[0, a_ix, :, run_ix] = mab.mean_reward_history\n",
    "        data_Q7[1, a_ix, :, run_ix] = mab.fraction_correct_history\n",
    "        data_Q7[2, a_ix, :, run_ix] = mab.instant_regret_history\n",
    "        data_Q7[3, a_ix, :, run_ix] = mab.total_regret_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBJZiwQBO_oa"
   },
   "source": [
    "### 7.3. Plotting the results (with std error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "lf-lEmwwO_oa",
    "outputId": "a424c56f-77fd-4e69-f988-de91d097bb6f"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q7,\n",
    "    title=\"Question 6\\nThomson Sampling bandit\",\n",
    "    main_labels=[\n",
    "        \"Time steps\",\n",
    "        \"Mean reward\",\n",
    "        \"% optimal actions\",\n",
    "        \"Instantaneous regret (averaged)\",\n",
    "        \"Total regret (averaged)\",\n",
    "    ],\n",
    "    ax_titles=[\n",
    "        f\"Mean reward and SE averaged over {nb_runs} runs\",\n",
    "        f\"Fraction of selected best actions averaged over {nb_runs} runs\",\n",
    "        f\"Instantaneous regret averaged over {nb_runs} runs\",\n",
    "        f\"Total regret averaged over {nb_runs} runs\",\n",
    "    ],\n",
    "    algs_info=[\n",
    "        (rf\"$\\alpha = {alphas[0]}$, $C = {c}$\", \"tab:blue\", 0),\n",
    "        (rf\"$\\alpha = {alphas[1]}$, $C = {c}$\", \"tab:green\", 0),\n",
    "        (rf\"$\\alpha = {alphas[2]}$, $C = {c}$\", \"tab:orange\", 0),\n",
    "        (rf\"Empirical mean, $C = {c}$\", \"tab:red\", 0),\n",
    "    ],\n",
    "    size=(10, 20),\n",
    "    range_y=[\n",
    "        (0, 1),\n",
    "        (0, 1),\n",
    "        (0, np.max(np.mean(data_Q7[2, :, :, :], axis=2) * 1.1)),\n",
    "        (0, np.max(np.mean(data_Q7[3, :, :, :], axis=2) * 1.1)),\n",
    "    ],\n",
    "    fill_std=[0, None, None, None],\n",
    "    filename=\"q7.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCUxWkQcO_ob"
   },
   "source": [
    "### 7.4. Plotting the results (without std error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eA_zS2OCO_ob",
    "outputId": "849865d6-69c0-496b-c610-8aab39bad44a"
   },
   "outputs": [],
   "source": [
    "# plot(\n",
    "#     data=thomson_data_Q7,\n",
    "#     title='Question 6\\nThomson Sampling bandit',\n",
    "#     main_labels=['Time steps', 'Average reward', 'Fraction of correct actions', 'Instant regret', 'Total regret'],\n",
    "#     ax_titles=['Average reward', 'Fraction of correct actions', 'Instant regret', 'Total regret'],\n",
    "#     algs_info=[('Alpha = 0.1, C = 2', 'tab:blue', 0), ('Alpha = 0.01, C = 2', 'tab:green', 0), ('Alpha = 0.001, C = 2', 'tab:orange', 0), ('Alpha = None, C = 2', 'tab:red', 0)],\n",
    "#     size=(10, 20),\n",
    "#     fill_std=False,\n",
    "#     filename='Files/q7_no_std_errors.png'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHagYJTLO_ob"
   },
   "source": [
    "## Question 8: Algorithmic comparison\n",
    "\n",
    "For each of the algorithms, pick the best hyper-parameter combination you have observed (explain how you decided what ”best” means). Plot together the curves for this setting. Comment on the relative behavior of the different algorithms."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "v7hsT6bNO_ob"
   },
   "source": [
    "For the purposes of this question, the \"best\" hyperparameter(s) are the ones that end up with the best mean regret after 1000 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAA0FXBXO_oc"
   },
   "source": [
    "### 8.1. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "n-0LbK5mO_oc"
   },
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "nb_runs = 100\n",
    "nb_steps = 1000\n",
    "\n",
    "data_Q8 = np.zeros((4, 3, nb_steps, nb_runs))\n",
    "\n",
    "# Run the algorithms\n",
    "for run_ix in range(nb_runs):\n",
    "    best_eg_epsilon = 1 / 8\n",
    "    best_eg_alpha = None\n",
    "    best_ucb_alpha = 0.01\n",
    "    best_ucb_c = 2\n",
    "    best_thomson_alpha = 0.01\n",
    "\n",
    "    mab_eg = EpsilonGreedy_MAB_Q4(q_star, epsilon=best_eg_epsilon, alpha=best_eg_alpha)\n",
    "    mab_ucb = UCB_MAB_Q6(q_star, alpha=best_ucb_alpha, c=best_ucb_c)\n",
    "    for action_ix in range(len(q_star)):\n",
    "        mab_ucb.update(action_ix)\n",
    "    mab_thomson = ThomsonSampling_MAB_Q7(q_star, alpha=best_thomson_alpha)\n",
    "\n",
    "    for step_ix in range(nb_steps):\n",
    "        mab_eg.draw_sample()\n",
    "        if step_ix >= 3:\n",
    "            mab_ucb.draw_sample()\n",
    "        mab_thomson.draw_sample()\n",
    "\n",
    "    data_Q8[0, 0, :, run_ix] = mab_eg.mean_reward_history\n",
    "    data_Q8[1, 0, :, run_ix] = mab_eg.fraction_correct_history\n",
    "    data_Q8[2, 0, :, run_ix] = mab_eg.instant_regret_history\n",
    "    data_Q8[3, 0, :, run_ix] = mab_eg.total_regret_history\n",
    "\n",
    "    data_Q8[0, 1, :, run_ix] = mab_ucb.mean_reward_history\n",
    "    data_Q8[1, 1, :, run_ix] = mab_ucb.fraction_correct_history\n",
    "    data_Q8[2, 1, :, run_ix] = mab_ucb.instant_regret_history\n",
    "    data_Q8[3, 1, :, run_ix] = mab_ucb.total_regret_history\n",
    "\n",
    "    data_Q8[0, 2, :, run_ix] = mab_thomson.mean_reward_history\n",
    "    data_Q8[1, 2, :, run_ix] = mab_thomson.fraction_correct_history\n",
    "    data_Q8[2, 2, :, run_ix] = mab_thomson.instant_regret_history\n",
    "    data_Q8[3, 2, :, run_ix] = mab_thomson.total_regret_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ncwLQacxO_od"
   },
   "source": [
    "### 8.2. Plotting the results (with std error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "NAxQOmb4O_oe",
    "outputId": "847e450a-d21e-4df7-cda9-c62070f8980a"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q8,\n",
    "    title=\"Question 8\\nEpsilon Greedy, UCB and Thomson Sampling bandits\",\n",
    "    main_labels=[\n",
    "        \"Time steps\",\n",
    "        \"Mean reward\",\n",
    "        \"% optimal actions\",\n",
    "        \"Instantaneous regret (averaged)\",\n",
    "        \"Total regret (averaged)\",\n",
    "    ],\n",
    "    ax_titles=[\n",
    "        f\"Mean reward and SE averaged over {nb_runs} runs\",\n",
    "        f\"Fraction of selected best actions averaged over {nb_runs} runs\",\n",
    "        f\"Instantaneous regret averaged over {nb_runs} runs\",\n",
    "        f\"Total regret averaged over {nb_runs} runs\",\n",
    "    ],\n",
    "    algs_info=[\n",
    "        (r\"$\\epsilon$-greedy: $\\epsilon = 1/8$, Empirical mean\", \"tab:blue\", 0),\n",
    "        (r\"UCB, $\\alpha = 0.01$\", \"tab:green\", 0),\n",
    "        (r\"Thomson Sampling, $\\alpha = 0.01$\", \"tab:orange\", 0),\n",
    "    ],\n",
    "    size=(10, 20),\n",
    "    range_y=[\n",
    "        (0, 1),\n",
    "        (0, 1),\n",
    "        (0, np.max(np.mean(data_Q8[2, :, :, :], axis=2) * 1.1)),\n",
    "        (0, np.max(np.mean(data_Q8[3, :, :, :], axis=2) * 1.1)),\n",
    "    ],\n",
    "    fill_std=[0, None, None, None],\n",
    "    filename=\"q8.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlSLvcs1O_oe"
   },
   "source": [
    "### 8.2. Plotting the results (without std error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "EKrK6x57O_of",
    "outputId": "3a134ef7-7dc1-4f72-fdc3-7a7a43815293"
   },
   "outputs": [],
   "source": [
    "# plot(\n",
    "#     data=Q8_data,\n",
    "#     title='Question 8\\nEpsilon Greedy, UCB and Thomson Sampling bandits',\n",
    "#     main_labels=['Time steps', 'Average reward', 'Fraction of correct actions', 'Instant regret', 'Total regret'],\n",
    "#     ax_titles=['Average reward', 'Fraction of correct actions', 'Instant regret', 'Total regret'],\n",
    "#     algs_info=[('Epsilon Greedy, epsilon = 1/8', 'tab:blue', 0), ('UCB, alpha = None', 'tab:green', 0), ('Thomson Sampling, alpha = None', 'tab:orange', 0)],\n",
    "#     size=(10, 20),\n",
    "#     fill_std=False,\n",
    "#     filename='Files/q8_no_std_errors.png'\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-O6jKsqO_of"
   },
   "source": [
    "## Question 9: Non-stationary problem\n",
    "\n",
    "Let us now consider a non-stationary problem. Let $\\delta = 0.1$ and imagine that after 500 time steps, the parameter of actions 2 and 3 become $0.5+\\delta$ and $0.5+2\\delta$ respectively. Run $\\epsilon$-greedy and UCB for a fixed value of $\\alpha = 0.1$ and the averaging value estimation. For $\\epsilon$ use values 1/4 and 1/8. Plot only the reward graph as above (you should have 2 lines for each of the $\\epsilon$ values, two lines for UCB and one for Thompson sampling). Explain what you see in the graph. Based on\n",
    "these results, which algorithm is best suited to cope with non-stationarity?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1. Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "tWANJ9wdO_og"
   },
   "outputs": [],
   "source": [
    "delta = 0.1\n",
    "\n",
    "nb_runs = 100\n",
    "nb_steps = 1000\n",
    "\n",
    "data_Q9 = np.zeros((4, 8, nb_steps, nb_runs))\n",
    "\n",
    "epsilons = [1 / 4, 1 / 8]\n",
    "alphas = [None, 0.1]\n",
    "c = 2\n",
    "\n",
    "# Run the algorithms\n",
    "for run_ix in range(nb_runs):\n",
    "    for epsilon_ix, epsilon in enumerate(epsilons):\n",
    "        for alpha_ix, alpha in enumerate(alphas):\n",
    "            q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "            mab_eg = EpsilonGreedy_MAB_Q4(q_star, epsilon=epsilon, alpha=alpha)\n",
    "            for step_ix in range(nb_steps):\n",
    "                if step_ix == 500:\n",
    "                    mab_eg.q_star = [0.5, 0.5 + delta, 0.5 + 2 * delta]\n",
    "                mab_eg.draw_sample()\n",
    "            data_Q9[0, epsilon_ix * 2 + alpha_ix, :, run_ix] = mab_eg.mean_reward_history\n",
    "            data_Q9[1, epsilon_ix * 2 + alpha_ix, :, run_ix] = mab_eg.fraction_correct_history\n",
    "            data_Q9[2, epsilon_ix * 2 + alpha_ix, :, run_ix] = mab_eg.instant_regret_history\n",
    "            data_Q9[3, epsilon_ix * 2 + alpha_ix, :, run_ix] = mab_eg.total_regret_history\n",
    "    for alpha_ix, alpha in enumerate(alphas):\n",
    "        q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "        mab_ucb = UCB_MAB_Q6(q_star, alpha=alpha, c=c)\n",
    "        for action_ix in range(len(q_star)):\n",
    "            mab_ucb.update(action_ix)\n",
    "        for step_ix in range(nb_steps - len(q_star)):\n",
    "            if step_ix == 500:\n",
    "                mab_ucb.q_star = [0.5, 0.5 + delta, 0.5 + 2 * delta]\n",
    "            mab_ucb.draw_sample()\n",
    "        data_Q9[0, 4 + alpha_ix, :, run_ix] = mab_ucb.mean_reward_history\n",
    "        data_Q9[1, 4 + alpha_ix, :, run_ix] = mab_ucb.fraction_correct_history\n",
    "        data_Q9[2, 4 + alpha_ix, :, run_ix] = mab_ucb.instant_regret_history\n",
    "        data_Q9[3, 4 + alpha_ix, :, run_ix] = mab_ucb.total_regret_history\n",
    "    for alpha_ix, alpha in enumerate(alphas):\n",
    "        q_star = [0.5, 0.5 - delta, 0.5 - 2 * delta]\n",
    "        mab_thomson = ThomsonSampling_MAB_Q7(q_star, alpha=alpha)\n",
    "        for step_ix in range(nb_steps):\n",
    "            if step_ix == 500:\n",
    "                mab_thomson.q_star = [0.5, 0.5 + delta, 0.5 + 2 * delta]\n",
    "            mab_thomson.draw_sample()\n",
    "        data_Q9[0, 6 + alpha_ix, :, run_ix] = mab_thomson.mean_reward_history\n",
    "        data_Q9[1, 6 + alpha_ix, :, run_ix] = mab_thomson.fraction_correct_history\n",
    "        data_Q9[2, 6 + alpha_ix, :, run_ix] = mab_thomson.instant_regret_history\n",
    "        data_Q9[3, 6 + alpha_ix, :, run_ix] = mab_thomson.total_regret_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFuN-0gcO_oh"
   },
   "outputs": [],
   "source": [
    "plot(\n",
    "    data=data_Q9,\n",
    "    title=\"Question 9\\nEpsilon Greedy, UCB and Thomson Sampling bandits\",\n",
    "    main_labels=[\n",
    "        \"Time steps\",\n",
    "        \"Mean reward\",\n",
    "        \"% optimal actions\",\n",
    "        \"Instantaneous regret (averaged)\",\n",
    "        \"Total regret (averaged)\",\n",
    "    ],\n",
    "    ax_titles=[\n",
    "        f\"Mean reward and SE averaged over {nb_runs} runs\",\n",
    "        f\"Fraction of selected best actions averaged over {nb_runs} runs\",\n",
    "        f\"Instantaneous regret averaged over {nb_runs} runs\",\n",
    "        f\"Total regret averaged over {nb_runs} runs\",\n",
    "    ],\n",
    "    algs_info=[\n",
    "        (rf\"Epsilon Greedy, $\\epsilon = {epsilons[0]}$, Empirical mean\", \"tab:blue\", 0),\n",
    "        (rf\"Epsilon Greedy, $\\epsilon = {epsilons[0]}, \\alpha = {alphas[1]}$\", \"b\", 0),\n",
    "        (rf\"Epsilon Greedy, $\\epsilon = {epsilons[1]}$, Empirical mean\", \"tab:green\", 0),\n",
    "        (rf\"Epsilon Greedy, $\\epsilon = {epsilons[1]}, \\alpha = {alphas[1]}$\", \"g\", 0),\n",
    "        (rf\"UCB, Empirical mean, $C = {c}$\", \"tab:orange\", 0),\n",
    "        (rf\"UCB, $\\alpha = {alphas[1]}, C = {c}$\", \"orange\", 0),\n",
    "        (r\"Thomson Sampling, Empirical mean\", \"tab:red\", 0),\n",
    "        (rf\"Thomson Sampling, $\\alpha = {alphas[1]}$\", \"r\", 0),\n",
    "    ],\n",
    "    size=(10, 20),\n",
    "    range_y=[\n",
    "        (0, 1),\n",
    "        (0, 1),\n",
    "        (0, np.max(np.mean(data_Q9[2, :, :, :], axis=2) * 1.1)),\n",
    "        (0, np.max(np.mean(data_Q9[3, :, :, :], axis=2) * 1.1)),\n",
    "    ],\n",
    "    fill_std=[None, None, None, None],\n",
    "    filename=\"q9.png\" if SAVE_FILES else None,\n",
    "    show=SHOW_GRAPHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.transforms import Bbox\n",
    "# import numpy as np\n",
    "\n",
    "# # Generate random data\n",
    "# x = np.random.rand(10)\n",
    "# y = np.random.rand(10)\n",
    "\n",
    "# # Create a figure and axes\n",
    "# fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "# # Plot the data\n",
    "# ax.scatter(x, y, label='Data')\n",
    "\n",
    "# # Add a legend to the side of the plot\n",
    "# ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# # Show the plot\n",
    "# plt.savefig('your_file_name.png', bbox_inches=Bbox([[0, 0], [10, 10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
