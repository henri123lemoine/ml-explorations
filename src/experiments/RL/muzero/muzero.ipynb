{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPPiXuqI3de1",
        "outputId": "ae03e696-de25-4f0d-f34b-c083cd2d1ae4"
      },
      "outputs": [],
      "source": [
        "# %apt-get install -y python3-dev swig\n",
        "# %pip install box2d-py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HlYmsL-UpFRp"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUWIgBJwnBnN",
        "outputId": "503155f4-fd95-4e9b-f467-4a376830ef20"
      },
      "outputs": [],
      "source": [
        "# Envs\n",
        "from collections import deque\n",
        "import cv2\n",
        "import imageio\n",
        "\n",
        "## LunarLander\n",
        "import gymnasium\n",
        "import math\n",
        "\n",
        "## CarRacing\n",
        "\n",
        "# Network\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# ReplayBuffer\n",
        "import numpy as np\n",
        "\n",
        "# MuZero\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "61wHNq-jE0P5"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W_34rA5y2KL_"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jffc2Nc-2IqO"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # Save graphs/anims\n",
        "    GRAPH_DIR = \"data/graphs/\"\n",
        "    ANIM_DIR = \"data/anims/\"\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    ### Game\n",
        "    nb_episodes = 1000\n",
        "    nb_test_chunk = 3\n",
        "    nb_ep_bw_test = 20\n",
        "    update_graph_interval = 5\n",
        "\n",
        "    action_space = list(range(4))\n",
        "    resolution = (96, 96)\n",
        "    nb_channels = 1  # Number of channels in the observations, 1 for grayscale, 3 RGB\n",
        "    stacked_observations = (\n",
        "        4  # Number of previous observations and previous actions to add to the current observation\n",
        "    )\n",
        "    interval = 5  # Time-jumps between frames\n",
        "\n",
        "    ### Self-Play\n",
        "    max_moves = 400\n",
        "    num_simulations = 30  # Number of future moves self-simulated\n",
        "    discount = 0.999  # Chronological discount of the reward\n",
        "    value_loss_weight = 0.25  # paper recommends 0.25\n",
        "\n",
        "    # Root prior exploration noise\n",
        "    root_dirichlet_alpha = 0.25\n",
        "    root_exploration_fraction = 0.25\n",
        "\n",
        "    # UCB formula\n",
        "    pb_c_base = 19652\n",
        "    pb_c_init = 1.25\n",
        "\n",
        "    ### Network\n",
        "    support_size = 20\n",
        "\n",
        "    # Fully Connected Network\n",
        "    encoding_size = 10\n",
        "    fc_representation_layers = []\n",
        "    fc_dynamics_layers = [64]\n",
        "    fc_reward_layers = [64]\n",
        "    fc_value_layers = [64]\n",
        "    fc_policy_layers = [64]\n",
        "\n",
        "    ### Training\n",
        "    # Exponential learning rate schedule\n",
        "    lr_init = 0.005\n",
        "    lr_decay_rate = 1\n",
        "\n",
        "    training_steps = 20000\n",
        "    batch_size = 64\n",
        "\n",
        "    ### Replay Buffer\n",
        "    replay_buffer_size = 128  # Number of self-play games to keep in the replay buffer\n",
        "    num_unroll_steps = 10  # Number of game moves to keep for every batch element\n",
        "    td_steps = (\n",
        "        15  # Number of steps in the future to take into account for calculating the target value\n",
        "    )\n",
        "\n",
        "    visit_softmax_temperature = 0.35"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j7t149uS84Sz"
      },
      "source": [
        "## Game"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1oc0JZ621UT4"
      },
      "source": [
        "#### Lunar Lander"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3sOE-fwn1J1X"
      },
      "outputs": [],
      "source": [
        "class LunarLander:\n",
        "    def __init__(self, normalize=True, record=False):\n",
        "        self.config = Config\n",
        "        self.normalize = normalize\n",
        "        self.record = record\n",
        "\n",
        "        self.env = gymnasium.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
        "        self.action_space = self.env.action_space\n",
        "\n",
        "        self.frames = []\n",
        "        self.observations = deque(maxlen=self.config.stacked_observations)\n",
        "\n",
        "        self.step_count = 0\n",
        "        self.total_reward = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        observation, reward, terminated, truncated, _ = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        frame = np.array(self.render())\n",
        "        if self.record:\n",
        "            self.frames.append(frame)\n",
        "\n",
        "        observation = self._process_frame(frame)\n",
        "        self.observations.append(observation)\n",
        "\n",
        "        step_obs = torch.cat(list(self.observations), dim=0).unsqueeze(0)\n",
        "\n",
        "        self.step_count += 1\n",
        "        self.total_reward += reward\n",
        "        if self.step_count >= self.config.max_moves:\n",
        "            done = True\n",
        "\n",
        "        return step_obs, reward, done\n",
        "\n",
        "    def _process_frame(self, observation):\n",
        "        target_shape = (\n",
        "            self.config.resolution[0],\n",
        "            self.config.resolution[1],\n",
        "            self.config.nb_channels,\n",
        "        )\n",
        "        observation = cv2.resize(observation, target_shape[:2], interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # observation = np.transpose(observation, (2, 0, 1))\n",
        "\n",
        "        if self.config.nb_channels == 1:\n",
        "            observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
        "            # observation = cv2.cvtColor(observation.transpose(1, 2, 0), cv2.COLOR_RGB2GRAY)\n",
        "            observation = np.expand_dims(observation, axis=0)\n",
        "\n",
        "        if self.normalize:\n",
        "            observation = observation / 255.0\n",
        "\n",
        "        return torch.from_numpy(observation).to(torch.float32)\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "    def reset(self):\n",
        "        self.frames = []\n",
        "        self.observations = deque(maxlen=self.config.stacked_observations)\n",
        "\n",
        "        self.step_count = 0\n",
        "        self.total_reward = 0\n",
        "\n",
        "        self.env.reset()\n",
        "        observation = np.array(self.env.render())\n",
        "\n",
        "        processed_obs = self._process_frame(observation)\n",
        "        for _ in range(self.config.stacked_observations):\n",
        "            self.observations.append(processed_obs)\n",
        "\n",
        "        step_obs = torch.cat(list(self.observations), dim=0).unsqueeze(0)\n",
        "\n",
        "        return step_obs\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "    def save_video(self, VID_DIR, episode, fps=60, test_ep=None, prnt=True):\n",
        "        # Save the frames as a video\n",
        "        if test_ep is None:\n",
        "            video_filename = f\"{VID_DIR}/ep_{episode}.mp4\"\n",
        "        else:\n",
        "            video_filename = f\"{VID_DIR}/ep_{episode}_test_{test_ep}.mp4\"\n",
        "        if prnt:\n",
        "            print(f\"Saving video to {video_filename}\")\n",
        "        imageio.mimsave(video_filename, self.frames, fps=fps)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng-fNWc51Z4G"
      },
      "source": [
        "#### Car Racing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "33gbuCfC8-kD"
      },
      "outputs": [],
      "source": [
        "class CarRacing:\n",
        "    def __init__(self, normalize=True, record=False):\n",
        "        self.config = Config\n",
        "        self.normalize = normalize\n",
        "        self.record = record\n",
        "\n",
        "        self.env = gymnasium.make(\n",
        "            \"CarRacing-v2\", render_mode=\"rgb_array\", domain_randomize=False, continuous=False\n",
        "        )\n",
        "        self.action_space = self.env.action_space\n",
        "\n",
        "        self.frames = []\n",
        "        self.observations = deque(maxlen=self.config.stacked_observations)\n",
        "\n",
        "        self.step_count = 0\n",
        "        self.total_reward = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        observation, reward, terminated, truncated, _ = self.env.step(action)\n",
        "        if self.record:\n",
        "            self.frames.append(np.array(self.render()))\n",
        "\n",
        "        observation = self._process_frame(observation)\n",
        "        self.observations.append(observation)\n",
        "\n",
        "        step_obs = torch.cat(list(self.observations), dim=0).unsqueeze(0)\n",
        "\n",
        "        done = terminated or truncated\n",
        "        if self.step_count >= self.config.max_moves:\n",
        "            done = True\n",
        "\n",
        "        self.step_count += 1\n",
        "        self.total_reward += reward\n",
        "\n",
        "        return step_obs, reward, done\n",
        "\n",
        "    def _process_frame(self, observation):\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "\n",
        "        if self.config.nb_channels == 1:\n",
        "            observation = cv2.cvtColor(observation.transpose(1, 2, 0), cv2.COLOR_RGB2GRAY)\n",
        "            observation = np.expand_dims(observation, axis=0)\n",
        "\n",
        "        if self.normalize:\n",
        "            observation = observation / 255.0\n",
        "\n",
        "        return torch.from_numpy(observation).to(torch.float32)\n",
        "\n",
        "    def render(self):\n",
        "        return self.env.render()\n",
        "\n",
        "    def reset(self):\n",
        "        self.frames = []\n",
        "        self.observations = deque(maxlen=self.config.stacked_observations)\n",
        "\n",
        "        self.step_count = 0\n",
        "        self.total_reward = 0\n",
        "\n",
        "        observation, _ = self.env.reset()\n",
        "        if self.record:\n",
        "            self.frames.append(np.array(self.render()))\n",
        "\n",
        "        observation = self._process_frame(observation)\n",
        "        for _ in range(self.config.stacked_observations):\n",
        "            self.observations.append(observation)\n",
        "\n",
        "        step_obs = torch.cat(list(self.observations), dim=0).unsqueeze(0)\n",
        "\n",
        "        return step_obs\n",
        "\n",
        "    def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "    def save_video(self, VID_DIR, episode, fps=60, test_ep=None, prnt=True):\n",
        "        # Save the frames as a video\n",
        "        if test_ep is None:\n",
        "            video_filename = f\"{VID_DIR}/ep_{episode}.mp4\"\n",
        "        else:\n",
        "            video_filename = f\"{VID_DIR}/ep_{episode}_test_{test_ep}.mp4\"\n",
        "        if prnt:\n",
        "            print(f\"Saving video to {video_filename}\")\n",
        "        imageio.mimsave(video_filename, self.frames, fps=fps)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EqZP8ilRoCuz"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GkC6lEjvoERS"
      },
      "outputs": [],
      "source": [
        "class MuZeroNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MuZeroNetwork, self).__init__()\n",
        "\n",
        "        self.config = Config\n",
        "        self.device = self.config.device\n",
        "        self.to(self.device)\n",
        "\n",
        "        self.fc_reward_layers = Config.fc_reward_layers\n",
        "        self.fc_value_layers = Config.fc_value_layers\n",
        "        self.fc_policy_layers = Config.fc_policy_layers\n",
        "        self.fc_representation_layers = Config.fc_representation_layers\n",
        "        self.fc_dynamics_layers = Config.fc_dynamics_layers\n",
        "        self.full_support_size = 2 * self.config.support_size + 1\n",
        "        self.support_values = (\n",
        "            torch.arange(\n",
        "                -self.config.support_size, self.config.support_size + 1, dtype=torch.float32\n",
        "            )\n",
        "            .unsqueeze(0)\n",
        "            .to(self.device)\n",
        "        )\n",
        "\n",
        "        self.repr_conv = nn.Sequential(\n",
        "            nn.Conv2d(self.config.nb_channels * self.config.stacked_observations, 32, 8, stride=4),\n",
        "            nn.Conv2d(32, 64, 4, stride=2),\n",
        "            nn.Conv2d(64, 64, 3, stride=1),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        num_features = len(\n",
        "            self.repr_conv(\n",
        "                torch.zeros(\n",
        "                    (\n",
        "                        1,\n",
        "                        self.config.nb_channels * self.config.stacked_observations,\n",
        "                        self.config.resolution[0],\n",
        "                        self.config.resolution[1],\n",
        "                    )\n",
        "                )\n",
        "            )[0, :]\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(num_features, 128),\n",
        "            nn.Linear(128, self.config.encoding_size),\n",
        "        )\n",
        "\n",
        "        self.representation_network = nn.Sequential(\n",
        "            *self.repr_conv,\n",
        "            *self.fc,\n",
        "        )\n",
        "\n",
        "        self.dynamics_encoded_state_network = network_builder(\n",
        "            self.config.encoding_size + len(self.config.action_space),\n",
        "            self.fc_dynamics_layers,\n",
        "            self.config.encoding_size,\n",
        "        )\n",
        "        self.dynamics_reward_network = network_builder(\n",
        "            self.config.encoding_size, self.config.fc_reward_layers, self.full_support_size\n",
        "        )\n",
        "        self.prediction_policy_network = network_builder(\n",
        "            self.config.encoding_size, self.fc_policy_layers, len(self.config.action_space)\n",
        "        )\n",
        "        self.prediction_value_network = network_builder(\n",
        "            self.config.encoding_size, self.fc_value_layers, self.full_support_size\n",
        "        )\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.parameters(),\n",
        "            lr=Config.lr_init,\n",
        "            weight_decay=Config.lr_decay_rate,\n",
        "        )\n",
        "\n",
        "    def representation_function(self, observation):\n",
        "        encoded_state = self.representation_network(observation)\n",
        "\n",
        "        # Scale encoded state between [0, 1]\n",
        "        min_encoded_state = encoded_state.min(1, keepdim=True)[0]\n",
        "        max_encoded_state = encoded_state.max(1, keepdim=True)[0]\n",
        "        scale_encoded_state = max_encoded_state - min_encoded_state\n",
        "        scale_encoded_state[scale_encoded_state < 1e-5] += 1e-5\n",
        "        encoded_state_normalized = (encoded_state - min_encoded_state) / scale_encoded_state\n",
        "        return encoded_state_normalized\n",
        "\n",
        "    def prediction_function(self, encoded_state):\n",
        "        policy_logits = self.prediction_policy_network(encoded_state)\n",
        "        value = self.prediction_value_network(encoded_state)\n",
        "        return policy_logits, value\n",
        "\n",
        "    def dynamics_function(self, encoded_state, action):\n",
        "        # Stack encoded_state with a game specific one hot encoded action\n",
        "        action_one_hot = torch.zeros(\n",
        "            (action.shape[0], len(self.config.action_space)), device=self.device\n",
        "        ).float()\n",
        "        action_one_hot.scatter_(1, action.long(), 1.0)\n",
        "\n",
        "        x = torch.cat((encoded_state, action_one_hot), dim=1)\n",
        "\n",
        "        next_encoded_state = self.dynamics_encoded_state_network(x)\n",
        "\n",
        "        reward = self.dynamics_reward_network(next_encoded_state)\n",
        "\n",
        "        # Scale encoded state between [0, 1]\n",
        "        min_next_encoded_state = next_encoded_state.min(1, keepdim=True)[0]\n",
        "        max_next_encoded_state = next_encoded_state.max(1, keepdim=True)[0]\n",
        "        scale_next_encoded_state = max_next_encoded_state - min_next_encoded_state\n",
        "        scale_next_encoded_state[scale_next_encoded_state < 1e-5] += 1e-5\n",
        "        next_encoded_state_normalized = (\n",
        "            next_encoded_state - min_next_encoded_state\n",
        "        ) / scale_next_encoded_state\n",
        "\n",
        "        return next_encoded_state_normalized, reward\n",
        "\n",
        "    def initial_inference(self, observation):\n",
        "        observation = observation.to(self.device)\n",
        "        encoded_state = self.representation_function(observation)\n",
        "        policy_logits, value = self.prediction_function(encoded_state)\n",
        "        # reward equal to 0 for consistency\n",
        "        reward = torch.log(\n",
        "            (\n",
        "                torch.zeros(1, self.full_support_size)\n",
        "                .scatter(1, torch.tensor([[self.full_support_size // 2]]).long(), 1.0)\n",
        "                .repeat(len(observation), 1)\n",
        "                .to(self.device)\n",
        "            )\n",
        "        )\n",
        "        return value, reward, policy_logits, encoded_state\n",
        "\n",
        "    def recurrent_inference(self, encoded_state, action):\n",
        "        encoded_state = encoded_state.to(self.config.device)\n",
        "        action = action.to(self.config.device)\n",
        "        next_encoded_state, reward = self.dynamics_function(encoded_state, action)\n",
        "        policy_logits, value = self.prediction_function(next_encoded_state)\n",
        "        return value, reward, policy_logits, next_encoded_state\n",
        "\n",
        "\n",
        "def network_builder(\n",
        "    input_size,\n",
        "    layer_sizes,\n",
        "    output_size,\n",
        "    output_activation=torch.nn.Identity,\n",
        "    activation=torch.nn.ELU,\n",
        "    input_layers=[],\n",
        "):\n",
        "    sizes = [input_size] + layer_sizes + [output_size]\n",
        "    layers = []\n",
        "    layers.extend(input_layers)\n",
        "    for i in range(len(sizes) - 1):\n",
        "        act = activation if i < len(sizes) - 2 else output_activation\n",
        "        layers += [torch.nn.Linear(sizes[i], sizes[i + 1]), act()]\n",
        "    return torch.nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def support_to_scalar(logits, support_size):\n",
        "    \"\"\"\n",
        "    Transform a categorical representation to a scalar\n",
        "    \"\"\"\n",
        "    # Decode to a scalar\n",
        "    probabilities = torch.softmax(logits, dim=1)\n",
        "    support = (\n",
        "        torch.tensor([x for x in range(-support_size, support_size + 1)], device=logits.device)\n",
        "        .expand(probabilities.shape)\n",
        "        .float()\n",
        "    )\n",
        "\n",
        "    x = torch.sum(support * probabilities, dim=1, keepdim=True)\n",
        "\n",
        "    return torch.sign(x) * (\n",
        "        ((torch.sqrt(1 + 4 * 0.001 * (torch.abs(x) + 1 + 0.001)) - 1) / (2 * 0.001)) ** 2 - 1\n",
        "    )\n",
        "\n",
        "\n",
        "def scalar_to_support(x, support_size):\n",
        "    \"\"\"\n",
        "    Transform a scalar to a categorical representation with (2 * support_size + 1) categories\n",
        "    \"\"\"\n",
        "    # Reduce the scale (defined in https://arxiv.org/abs/1805.11593)\n",
        "    x = torch.sign(x) * (torch.sqrt(torch.abs(x) + 1) - 1) + 0.001 * x\n",
        "\n",
        "    # Encode on a vector\n",
        "    x = torch.clamp(x, -support_size, support_size)\n",
        "    floor = x.floor()\n",
        "    prob = x - floor\n",
        "    logits = torch.zeros(x.shape[0], x.shape[1], 2 * support_size + 1, device=x.device)\n",
        "    logits.scatter_(2, (floor + support_size).long().unsqueeze(-1), (1 - prob).unsqueeze(-1))\n",
        "    indexes = floor + support_size + 1\n",
        "    prob = prob.masked_fill_(2 * support_size < indexes, 0.0)\n",
        "    indexes = indexes.masked_fill_(2 * support_size < indexes, 0.0)\n",
        "    logits.scatter_(2, indexes.long().unsqueeze(-1), prob.unsqueeze(-1))\n",
        "    return logits"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nKyyJU_porAh"
      },
      "source": [
        "## Replay buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0Wx89hRCoyjh"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the replay buffer with a given size.\n",
        "        \"\"\"\n",
        "        self.config = Config\n",
        "        self.device = self.config.device\n",
        "\n",
        "        self.buffer = deque(maxlen=self.config.replay_buffer_size)\n",
        "\n",
        "        self.num_played_steps = 0\n",
        "        self.num_played_games = 0\n",
        "        self.total_samples = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def save_game(self, game_history):\n",
        "        self.buffer.append(game_history)\n",
        "        self.num_played_games += 1\n",
        "        self.num_played_steps += len(game_history.root_values)\n",
        "        self.total_samples = sum(len(game.root_values) for game in self.buffer)\n",
        "\n",
        "    def compute_target_value(self, game_history, index):\n",
        "        # The value target is the discounted root value of the search tree td_steps into the future, plus the discounted sum of all rewards until then.\n",
        "        bootstrap_index = index + self.config.td_steps\n",
        "        if bootstrap_index < len(game_history.root_values):\n",
        "            root_values = game_history.root_values\n",
        "            last_step_value = root_values[bootstrap_index]\n",
        "\n",
        "            value = last_step_value * self.config.discount**self.config.td_steps\n",
        "        else:\n",
        "            value = 0\n",
        "\n",
        "        for i, reward in enumerate(game_history.reward_history[index + 1 : bootstrap_index + 1]):\n",
        "            value += reward * self.config.discount**i\n",
        "\n",
        "        return value\n",
        "\n",
        "    def get_batch(self):\n",
        "        index_batch = []\n",
        "        observation_batch = []\n",
        "        action_batch = []\n",
        "        value_batch = []\n",
        "        reward_batch = []\n",
        "        policy_batch = []\n",
        "        gradient_scale_batch = []\n",
        "\n",
        "        for game_id, game_history in self.sample_game(self.config.batch_size):\n",
        "            game_pos = self.sample_position(game_history)\n",
        "\n",
        "            values, rewards, policies, actions = self.make_target(game_history, game_pos)\n",
        "\n",
        "            observation_batch.append(game_history.observation_history[game_pos])\n",
        "\n",
        "            index_batch.append([game_id, game_pos])\n",
        "            action_batch.append(actions)\n",
        "            value_batch.append(values)\n",
        "            reward_batch.append(rewards)\n",
        "            policy_batch.append(policies)\n",
        "            gradient_scale_batch.append(\n",
        "                [min(self.config.num_unroll_steps, len(game_history.action_history) - game_pos)]\n",
        "                * len(actions)\n",
        "            )\n",
        "\n",
        "        # Move all tensors in observation_batch to the same device\n",
        "        observation_batch = [obs.to(self.device) for obs in observation_batch]\n",
        "\n",
        "        # Now stack the tensors\n",
        "        observation_batch = torch.stack([obs.squeeze(0) for obs in observation_batch], axis=0)\n",
        "\n",
        "        return (\n",
        "            index_batch,\n",
        "            (\n",
        "                observation_batch,\n",
        "                action_batch,\n",
        "                value_batch,\n",
        "                reward_batch,\n",
        "                policy_batch,\n",
        "                gradient_scale_batch,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def _generate_single_batch(self, game_history, game_id, game_pos):\n",
        "        values, rewards, policies, actions, observations = self.make_target(game_history, game_pos)\n",
        "\n",
        "        single_index = [game_id, game_pos]\n",
        "        single_observation = observations\n",
        "        single_action = actions\n",
        "        single_value = values\n",
        "        single_reward = rewards\n",
        "        single_policy = policies\n",
        "        single_gradient_scale = [\n",
        "            min(self.config.num_unroll_steps, len(game_history.action_history) - game_pos)\n",
        "        ] * len(actions)\n",
        "\n",
        "        return (\n",
        "            single_index,\n",
        "            single_observation,\n",
        "            single_action,\n",
        "            single_value,\n",
        "            single_reward,\n",
        "            single_policy,\n",
        "            single_gradient_scale,\n",
        "        )\n",
        "\n",
        "    def sample_game(self, n_games=1):\n",
        "        \"\"\"\n",
        "        Sample n_games from the buffer uniformly.\n",
        "        See paper appendix Training.\n",
        "        \"\"\"\n",
        "        game_indices = np.random.choice(len(self.buffer), size=n_games, replace=True)\n",
        "        game_ids = [\n",
        "            self.num_played_games - len(self.buffer) + game_index for game_index in game_indices\n",
        "        ]\n",
        "\n",
        "        return [(game_index, self.buffer[game_index]) for game_index in game_indices]\n",
        "\n",
        "    def sample_position(self, game_history):\n",
        "        \"\"\"\n",
        "        Sample position from game uniformly.\n",
        "        See paper appendix Training.\n",
        "        \"\"\"\n",
        "        return np.random.choice(len(game_history.root_values))\n",
        "\n",
        "    def make_target(self, game_history, state_index):\n",
        "        \"\"\"\n",
        "        Generate targets for every unroll steps.\n",
        "        \"\"\"\n",
        "        target_values = []\n",
        "        target_rewards = []\n",
        "        target_policies = []\n",
        "        actions = []\n",
        "\n",
        "        for current_index in range(state_index, state_index + self.config.num_unroll_steps + 1):\n",
        "            value = self.compute_target_value(game_history, current_index)\n",
        "\n",
        "            if current_index < len(game_history.root_values):\n",
        "                target_values.append(value)\n",
        "                target_rewards.append(game_history.reward_history[current_index])\n",
        "                target_policies.append(game_history.child_visits[current_index])\n",
        "                actions.append(game_history.action_history[current_index])\n",
        "            elif current_index == len(game_history.root_values):\n",
        "                target_values.append(0)\n",
        "                target_rewards.append(game_history.reward_history[current_index])\n",
        "                # Uniform policy\n",
        "                target_policies.append(\n",
        "                    [\n",
        "                        1 / len(game_history.child_visits[0])\n",
        "                        for _ in range(len(game_history.child_visits[0]))\n",
        "                    ]\n",
        "                )\n",
        "                actions.append(game_history.action_history[current_index])\n",
        "            else:\n",
        "                # States past the end of games are treated as absorbing states\n",
        "                target_values.append(0)\n",
        "                target_rewards.append(0)\n",
        "                # Uniform policy\n",
        "                target_policies.append(\n",
        "                    [\n",
        "                        1 / len(game_history.child_visits[0])\n",
        "                        for _ in range(len(game_history.child_visits[0]))\n",
        "                    ]\n",
        "                )\n",
        "                actions.append(np.random.choice(self.config.action_space))\n",
        "\n",
        "        return target_values, target_rewards, target_policies, actions"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aDebWMeB4_NV"
      },
      "source": [
        "## GameHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zB0OXSeg5A0e"
      },
      "outputs": [],
      "source": [
        "class GameHistory:\n",
        "    \"\"\"\n",
        "    Store only usefull information of a self-play game.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.observation_history = []\n",
        "        self.action_history = []\n",
        "        self.reward_history = []\n",
        "        self.child_visits = []\n",
        "        self.root_values = []\n",
        "\n",
        "    def store_search_statistics(self, root, action_space):\n",
        "        # Turn visit count from root into a policy\n",
        "        if root is not None:\n",
        "            sum_visits = sum(child.visit_count for child in root.children.values())\n",
        "            self.child_visits.append(\n",
        "                [\n",
        "                    root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "                    for a in action_space\n",
        "                ]\n",
        "            )\n",
        "\n",
        "            self.root_values.append(root.value())\n",
        "        else:\n",
        "            self.root_values.append(None)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YLnIUlrg5hKD"
      },
      "source": [
        "## MCTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fyW9RmbT5geL"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "    def __init__(self, prior):\n",
        "        self.device = Config.device\n",
        "\n",
        "        self.visit_count = 0\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.hidden_state = None\n",
        "        self.reward = 0\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "    def expand(self, actions, reward, policy_logits, hidden_state):\n",
        "        \"\"\"\n",
        "        # FROM PAPER CODE GITHUB\n",
        "        We expand a node using the value, reward and policy prediction obtained from the\n",
        "        neural network.\n",
        "        \"\"\"\n",
        "        self.reward = reward\n",
        "        self.hidden_state = hidden_state\n",
        "\n",
        "        # policy_values = torch.softmax(torch.tensor([policy_logits[0][a] for a in actions]), dim=0).tolist()\n",
        "        policy_values = torch.softmax(\n",
        "            torch.tensor([policy_logits[0][a] for a in actions]).to(self.device), dim=0\n",
        "        ).tolist()\n",
        "        policy = {a: policy_values[i] for i, a in enumerate(actions)}\n",
        "\n",
        "        for action, p in policy.items():\n",
        "            self.children[action] = Node(p)\n",
        "\n",
        "    def add_exploration_noise(self, dirichlet_alpha, exploration_fraction):\n",
        "        \"\"\"\n",
        "        At the start of each search, we add dirichlet noise to the prior of the root to\n",
        "        encourage the search to explore new actions.\n",
        "        \"\"\"\n",
        "        actions = list(self.children.keys())\n",
        "        noise = np.random.dirichlet([dirichlet_alpha] * len(actions))\n",
        "        frac = exploration_fraction\n",
        "        for a, n in zip(actions, noise):\n",
        "            self.children[a].prior = self.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "class MinMaxStats:\n",
        "    \"\"\"\n",
        "    COPIED\n",
        "    A class that holds the min-max values of the tree.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.maximum = -float(\"inf\")\n",
        "        self.minimum = float(\"inf\")\n",
        "\n",
        "    def update(self, value):\n",
        "        self.maximum = max(self.maximum, value)\n",
        "        self.minimum = min(self.minimum, value)\n",
        "\n",
        "    def normalize(self, value):\n",
        "        if self.maximum > self.minimum:\n",
        "            # We normalize only when we have set the maximum and minimum values\n",
        "            return (value - self.minimum) / (self.maximum - self.minimum)\n",
        "        return value\n",
        "\n",
        "\n",
        "def select_action(node, best=False):\n",
        "    \"\"\"\n",
        "    Select action according to the visit count distribution and the temperature.\n",
        "    The temperature is changed dynamically with the visit_softmax_temperature function\n",
        "    in the config.\n",
        "    \"\"\"\n",
        "\n",
        "    visit_counts = np.array([child.visit_count for child in node.children.values()], dtype=\"int32\")\n",
        "    actions = [action for action in node.children.keys()]\n",
        "\n",
        "    visit_count_distribution = visit_counts ** (1 / Config.visit_softmax_temperature)\n",
        "    visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
        "\n",
        "    if best:\n",
        "        action = actions[np.argmax(visit_count_distribution)]\n",
        "    else:\n",
        "        action = np.random.choice(actions, p=visit_count_distribution)\n",
        "\n",
        "    # if np.random.random() < 0.01:\n",
        "    #     print(visit_count_distribution)\n",
        "    #     print(action)\n",
        "\n",
        "    return action\n",
        "\n",
        "\n",
        "class MCTS:\n",
        "    def __init__(self) -> None:\n",
        "        self.config = Config\n",
        "\n",
        "        self.discount = Config.discount\n",
        "\n",
        "    def run(self, network: MuZeroNetwork, observation, add_exploration_noise=True):\n",
        "        root = Node(0)\n",
        "\n",
        "        root_predicted_value, reward, policy_logits, hidden_state = network.initial_inference(\n",
        "            observation\n",
        "        )\n",
        "\n",
        "        root_predicted_value = support_to_scalar(\n",
        "            root_predicted_value, self.config.support_size\n",
        "        ).item()\n",
        "        reward = support_to_scalar(reward, self.config.support_size).item()\n",
        "\n",
        "        root.expand(\n",
        "            self.config.action_space,\n",
        "            reward,\n",
        "            policy_logits,\n",
        "            hidden_state,\n",
        "        )\n",
        "\n",
        "        if add_exploration_noise:\n",
        "            root.add_exploration_noise(\n",
        "                dirichlet_alpha=self.config.root_dirichlet_alpha,\n",
        "                exploration_fraction=self.config.root_exploration_fraction,\n",
        "            )\n",
        "\n",
        "        min_max_stats = MinMaxStats()\n",
        "\n",
        "        max_tree_depth = 0\n",
        "        for _ in range(self.config.num_simulations):\n",
        "            node = root\n",
        "            search_path = [node]\n",
        "            current_tree_depth = 0\n",
        "\n",
        "            while node.expanded():\n",
        "                current_tree_depth += 1\n",
        "                action, node = self.select_child(node, min_max_stats)\n",
        "                search_path.append(node)\n",
        "\n",
        "            parent = search_path[-2]\n",
        "            value, reward, policy_logits, hidden_state = network.recurrent_inference(\n",
        "                parent.hidden_state, torch.tensor([[action]])\n",
        "            )\n",
        "            value = support_to_scalar(value, self.config.support_size).item()\n",
        "            reward = support_to_scalar(reward, self.config.support_size).item()\n",
        "            node.expand(\n",
        "                self.config.action_space,\n",
        "                reward,\n",
        "                policy_logits,\n",
        "                hidden_state,\n",
        "            )\n",
        "\n",
        "            self.backpropagate(search_path, value, min_max_stats)\n",
        "\n",
        "            max_tree_depth = max(max_tree_depth, current_tree_depth)\n",
        "\n",
        "        return root\n",
        "\n",
        "    def select_child(self, node, min_max_stats):\n",
        "        \"\"\"\n",
        "        COPIED FROM GITHUB\n",
        "        Select the child with the highest UCB score.\n",
        "        \"\"\"\n",
        "        max_ucb = max(\n",
        "            self.ucb_score(node, child, min_max_stats) for action, child in node.children.items()\n",
        "        )\n",
        "        action = np.random.choice(\n",
        "            [\n",
        "                action\n",
        "                for action, child in node.children.items()\n",
        "                if self.ucb_score(node, child, min_max_stats) == max_ucb\n",
        "            ]\n",
        "        )\n",
        "        return action, node.children[action]\n",
        "\n",
        "    def ucb_score(self, parent, child, min_max_stats):\n",
        "        \"\"\"\n",
        "        COPIED FROM GITHUB\n",
        "        The score for a node is based on its value, plus an exploration bonus based on the prior.\n",
        "        \"\"\"\n",
        "        pb_c = (\n",
        "            math.log((parent.visit_count + self.config.pb_c_base + 1) / self.config.pb_c_base)\n",
        "            + self.config.pb_c_init\n",
        "        )\n",
        "        pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "        prior_score = pb_c * child.prior\n",
        "\n",
        "        if child.visit_count > 0:\n",
        "            # Mean value Q\n",
        "            value_score = min_max_stats.normalize(child.reward + self.discount * child.value())\n",
        "        else:\n",
        "            value_score = 0\n",
        "\n",
        "        return prior_score + value_score\n",
        "\n",
        "    def backpropagate(self, search_path, value, min_max_stats):\n",
        "        \"\"\"\n",
        "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
        "        to the root.\n",
        "        \"\"\"\n",
        "        for node in reversed(search_path):\n",
        "            node.value_sum += value\n",
        "            node.visit_count += 1\n",
        "            min_max_stats.update(node.reward + self.discount * node.value())\n",
        "\n",
        "            value = node.reward + self.discount * value"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8FFb9Kdpoeax"
      },
      "source": [
        "## MuZero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "p4mu7y8tm-UW"
      },
      "outputs": [],
      "source": [
        "class MuZero:\n",
        "    def __init__(self, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "\n",
        "        self.config = Config\n",
        "        self.device = self.config.device\n",
        "\n",
        "        self.network = MuZeroNetwork()\n",
        "        self.network.to(self.device)\n",
        "\n",
        "        # self.env = CarRacing()\n",
        "        self.env = LunarLander()\n",
        "        self.buffer = ReplayBuffer()\n",
        "\n",
        "        self.VID_DIR = f'{Config.ANIM_DIR}/{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
        "        self.GRAPH_DIR = f'{Config.GRAPH_DIR}/{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
        "\n",
        "        if not os.path.exists(f\"{self.VID_DIR}/test\"):\n",
        "            os.makedirs(f\"{self.VID_DIR}/test\")\n",
        "        if not os.path.exists(f\"{self.GRAPH_DIR}\"):\n",
        "            os.makedirs(f\"{self.GRAPH_DIR}\")\n",
        "\n",
        "        self.training_step = 0\n",
        "        self.training_history = {\n",
        "            \"loss\": [],\n",
        "            \"value_loss\": [],\n",
        "            \"reward_loss\": [],\n",
        "            \"policy_loss\": [],\n",
        "            \"reward\": [],\n",
        "        }\n",
        "\n",
        "    def train(self):\n",
        "        for episode in range(self.config.nb_episodes):\n",
        "            # Test the MuZero network\n",
        "            if episode % self.config.nb_ep_bw_test == 0:\n",
        "                self.test_play(episode)\n",
        "\n",
        "            # Run self-play simulation\n",
        "            game_history = self.play(episode)\n",
        "            self.training_history[\"reward\"].append(self.env.total_reward)\n",
        "\n",
        "            # Add trajectory data to replay buffer\n",
        "            self.buffer.save_game(game_history)\n",
        "\n",
        "            # Train the MuZero network\n",
        "            if self.training_step < self.config.training_steps:\n",
        "                batch_id, batch = self.buffer.get_batch()\n",
        "\n",
        "                (\n",
        "                    total_loss,\n",
        "                    value_loss,\n",
        "                    reward_loss,\n",
        "                    policy_loss,\n",
        "                ) = self.update_weights(batch)\n",
        "\n",
        "                self.training_history[\"loss\"].append(total_loss)\n",
        "                self.training_history[\"value_loss\"].append(value_loss)\n",
        "                self.training_history[\"reward_loss\"].append(reward_loss)\n",
        "                self.training_history[\"policy_loss\"].append(policy_loss)\n",
        "\n",
        "                print(\n",
        "                    f\"\\nLoss: {total_loss:.4f} | Value loss: {value_loss:.4f} | Reward loss: {reward_loss:.4f} | Policy loss: {policy_loss:.4f}\"\n",
        "                )\n",
        "\n",
        "            # Update graphs every few episodes\n",
        "            if episode % self.config.update_graph_interval == 0:\n",
        "                self.update_graphs()\n",
        "\n",
        "        print(\"End of training. Last tests...\\n\")\n",
        "\n",
        "        self.test_play(episode)\n",
        "        self.env.close()\n",
        "\n",
        "    def play(self, episode: int):\n",
        "        start = time.time()\n",
        "\n",
        "        print(f\"\\n\\nTraining episode {episode}...\")\n",
        "\n",
        "        # Train the MuZero network\n",
        "        observation = self.env.reset()\n",
        "        observation = observation.to(self.device)\n",
        "\n",
        "        done = False\n",
        "\n",
        "        game_history = GameHistory()\n",
        "        game_history.action_history.append(0)\n",
        "        game_history.observation_history.append(observation)\n",
        "        game_history.reward_history.append(0)\n",
        "\n",
        "        # Interaction loop\n",
        "        while not done:\n",
        "            # Plan and act using Monte-Carlo Tree Search (MCTS)\n",
        "            root = MCTS().run(self.network, observation)\n",
        "            action = select_action(root)\n",
        "\n",
        "            # Interact with the environment\n",
        "            observation, reward, done = self.env.step(action)\n",
        "\n",
        "            game_history.store_search_statistics(root, self.config.action_space)\n",
        "            game_history.action_history.append(action)\n",
        "            game_history.observation_history.append(observation)\n",
        "            game_history.reward_history.append(reward)\n",
        "\n",
        "        print(\n",
        "            f\"Total reward: {self.env.total_reward:.2f} | Steps: {self.env.step_count} | Time: {time.time() - start:.2f}s\"\n",
        "        )\n",
        "\n",
        "        return game_history\n",
        "\n",
        "    def test_play(self, episode: int):\n",
        "        print(f\"\\n\\nTesting episode {episode}...\")\n",
        "\n",
        "        self.env.record = True\n",
        "\n",
        "        # Test the MuZero network\n",
        "        for test_ep in range(self.config.nb_test_chunk):\n",
        "            # Reset the environment\n",
        "            test_observation = self.env.reset()\n",
        "            test_observation.to(self.device)\n",
        "            test_done = False\n",
        "\n",
        "            # Interaction loop for the test episode\n",
        "            while not test_done:\n",
        "                # Plan and act using Monte-Carlo Tree Search (MCTS)\n",
        "                test_root = MCTS().run(self.network, test_observation, add_exploration_noise=False)\n",
        "                test_action = select_action(test_root)  # , best=True)\n",
        "\n",
        "                test_observation, test_reward, test_done = self.env.step(test_action)\n",
        "\n",
        "            self.env.save_video(self.VID_DIR, episode, test_ep=test_ep)\n",
        "            print(f\"Total reward: {self.env.total_reward:.2f}\")\n",
        "\n",
        "        self.env.record = False\n",
        "\n",
        "    def update_graphs(self):\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(self.training_history[\"loss\"], label=\"Total loss\")\n",
        "        plt.plot(self.training_history[\"value_loss\"], label=\"Value loss\")\n",
        "        plt.plot(self.training_history[\"reward_loss\"], label=\"Reward loss\")\n",
        "        plt.plot(self.training_history[\"policy_loss\"], label=\"Policy loss\")\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"{self.GRAPH_DIR}/loss.png\")\n",
        "        plt.close()\n",
        "\n",
        "        smoothed_rewards = [self.training_history[\"reward\"][0]]\n",
        "\n",
        "        for i in range(1, len(self.training_history[\"reward\"])):\n",
        "            start = max(0, i - 6)\n",
        "            mean_reward = sum(self.training_history[\"reward\"][start : i + 1]) / (i - start + 1)\n",
        "            smoothed_rewards.append(mean_reward)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        # plt.plot(self.training_history[\"reward\"], label=\"Reward\")\n",
        "        plt.plot(smoothed_rewards, label=\"Smoothed reward\")\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"{self.GRAPH_DIR}/reward.png\")\n",
        "        plt.close()\n",
        "\n",
        "    def update_weights(self, batch):\n",
        "        \"\"\"\n",
        "        Perform one training step.\n",
        "        \"\"\"\n",
        "        (\n",
        "            observation_batch,\n",
        "            action_batch,\n",
        "            target_value,\n",
        "            target_reward,\n",
        "            target_policy,\n",
        "            gradient_scale_batch,\n",
        "        ) = batch\n",
        "\n",
        "        observation_batch = observation_batch.clone().detach()\n",
        "        action_batch = torch.tensor(action_batch).to(self.device).long().unsqueeze(-1)\n",
        "        target_value = torch.tensor(target_value).to(self.device).float()\n",
        "        target_reward = torch.tensor(target_reward).to(self.device).float()\n",
        "        target_policy = torch.tensor(target_policy).to(self.device).float()\n",
        "        gradient_scale_batch = torch.tensor(gradient_scale_batch).to(self.device).float()\n",
        "\n",
        "        target_value = scalar_to_support(target_value, self.config.support_size)\n",
        "        target_reward = scalar_to_support(target_reward, self.config.support_size)\n",
        "\n",
        "        ## Generate predictions\n",
        "        value, reward, policy_logits, hidden_state = self.network.initial_inference(\n",
        "            observation_batch\n",
        "        )\n",
        "\n",
        "        predictions = [(value, reward, policy_logits)]\n",
        "        for i in range(1, action_batch.shape[1]):\n",
        "            value, reward, policy_logits, hidden_state = self.network.recurrent_inference(\n",
        "                hidden_state, action_batch[:, i]\n",
        "            )\n",
        "\n",
        "            # Scale the gradient at the start of the dynamics function (See paper appendix Training)\n",
        "\n",
        "            # value = support_to_scalar(value, self.config.support_size).item()\n",
        "            # reward = support_to_scalar(reward, self.config.support_size).item()\n",
        "\n",
        "            hidden_state.register_hook(lambda grad: grad * 0.5)\n",
        "            predictions.append((value, reward, policy_logits))\n",
        "\n",
        "        ## Compute losses\n",
        "        value_loss, reward_loss, policy_loss = (0, 0, 0)\n",
        "        value, reward, policy_logits = predictions[0]\n",
        "        # Ignore reward loss for the first batch step\n",
        "        current_value_loss, _, current_policy_loss = self.loss_function(\n",
        "            value.squeeze(-1),\n",
        "            reward.squeeze(-1),\n",
        "            policy_logits,\n",
        "            target_value[:, 0],\n",
        "            target_reward[:, 0],\n",
        "            target_policy[:, 0],\n",
        "        )\n",
        "        value_loss += current_value_loss\n",
        "        policy_loss += current_policy_loss\n",
        "\n",
        "        for i in range(1, len(predictions)):\n",
        "            value, reward, policy_logits = predictions[i]\n",
        "            # print(f\"Value: {value.squeeze(-1).shape}\")\n",
        "            # print(f\"Reward: {reward.squeeze(-1).shape}\")\n",
        "            # print(f\"Policy logits: {policy_logits.shape}\")\n",
        "            # print(f\"Target value: {target_value[:, i].shape}\")\n",
        "            # print(f\"Target reward: {target_reward[:, i].shape}\")\n",
        "            # print(f\"Target policy: {target_policy[:, i].shape}\")\n",
        "\n",
        "            # print(support_to_scalar(value.squeeze(-1)[0], 41))\n",
        "            # print(support_to_scalar(target_value[:, i][0], 41))\n",
        "\n",
        "            # raise Exception\n",
        "\n",
        "            (\n",
        "                current_value_loss,\n",
        "                current_reward_loss,\n",
        "                current_policy_loss,\n",
        "            ) = self.loss_function(\n",
        "                value.squeeze(-1),\n",
        "                reward.squeeze(-1),\n",
        "                policy_logits,\n",
        "                target_value[:, i],\n",
        "                target_reward[:, i],\n",
        "                target_policy[:, i],\n",
        "            )\n",
        "\n",
        "            # Scale gradient by the number of unroll steps (See paper appendix Training)\n",
        "            current_value_loss.register_hook(lambda grad: grad / gradient_scale_batch[:, i])\n",
        "            current_reward_loss.register_hook(lambda grad: grad / gradient_scale_batch[:, i])\n",
        "            current_policy_loss.register_hook(lambda grad: grad / gradient_scale_batch[:, i])\n",
        "\n",
        "            value_loss += current_value_loss\n",
        "            reward_loss += current_reward_loss\n",
        "            policy_loss += current_policy_loss\n",
        "\n",
        "        # value_loss *= 0\n",
        "        # reward_loss *= 0\n",
        "        # policy_loss *= 1\n",
        "\n",
        "        loss = value_loss * self.config.value_loss_weight + reward_loss + policy_loss\n",
        "\n",
        "        # Mean over batch dimension (pseudocode do a sum)\n",
        "        loss = loss.mean()\n",
        "\n",
        "        # Optimize\n",
        "        self.network.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.network.optimizer.step()\n",
        "        self.training_step += 1\n",
        "\n",
        "        return (\n",
        "            # For log purpose\n",
        "            loss.item(),\n",
        "            value_loss.mean().item(),\n",
        "            reward_loss.mean().item(),\n",
        "            policy_loss.mean().item(),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def loss_function(\n",
        "        value,\n",
        "        reward,\n",
        "        policy_logits,\n",
        "        target_value,\n",
        "        target_reward,\n",
        "        target_policy,\n",
        "    ):\n",
        "        value_loss = torch.sum(-target_value * torch.nn.LogSoftmax(dim=1)(value), dim=1)\n",
        "        reward_loss = torch.sum(-target_reward * torch.nn.LogSoftmax(dim=1)(reward), dim=1)\n",
        "        policy_loss = torch.sum(-target_policy * torch.nn.LogSoftmax(dim=1)(policy_logits), dim=1)\n",
        "\n",
        "        return value_loss, reward_loss, policy_loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BrqV4UEfE3y3"
      },
      "source": [
        "# Part 2"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlj9gRvopWVq"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VtWoYLNOpS7p",
        "outputId": "a078bb58-5a89-4fc1-fe7c-0ad77a317f7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Testing episode 0...\n",
            "Saving video to data/anims//2024-10-18_15-47-50/ep_0_test_0.mp4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward: -237.97\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_0_test_1.mp4\n",
            "Total reward: -88.16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_0_test_2.mp4\n",
            "Total reward: -175.52\n",
            "\n",
            "\n",
            "Training episode 0...\n",
            "Total reward: -173.39 | Steps: 100 | Time: 1.63s\n",
            "\n",
            "Loss: 62.6021 | Value loss: 41.9713 | Reward loss: 37.0372 | Policy loss: 15.0721\n",
            "\n",
            "\n",
            "Training episode 1...\n",
            "Total reward: -120.59 | Steps: 69 | Time: 1.17s\n",
            "\n",
            "Loss: 61.7332 | Value loss: 41.2988 | Reward loss: 36.3982 | Policy loss: 15.0102\n",
            "\n",
            "\n",
            "Training episode 2...\n",
            "Total reward: -70.28 | Steps: 77 | Time: 1.29s\n",
            "\n",
            "Loss: 61.1594 | Value loss: 41.0437 | Reward loss: 35.8716 | Policy loss: 15.0269\n",
            "\n",
            "\n",
            "Training episode 3...\n",
            "Total reward: -100.46 | Steps: 111 | Time: 1.78s\n",
            "\n",
            "Loss: 60.8251 | Value loss: 40.7979 | Reward loss: 35.5829 | Policy loss: 15.0427\n",
            "\n",
            "\n",
            "Training episode 4...\n",
            "Total reward: -271.31 | Steps: 102 | Time: 1.80s\n",
            "\n",
            "Loss: 60.0985 | Value loss: 40.6188 | Reward loss: 34.8952 | Policy loss: 15.0487\n",
            "\n",
            "\n",
            "Training episode 5...\n",
            "Total reward: -348.86 | Steps: 127 | Time: 2.07s\n",
            "\n",
            "Loss: 59.8299 | Value loss: 40.6390 | Reward loss: 34.6206 | Policy loss: 15.0496\n",
            "\n",
            "\n",
            "Training episode 6...\n",
            "Total reward: -238.64 | Steps: 79 | Time: 1.22s\n",
            "\n",
            "Loss: 59.4727 | Value loss: 40.4536 | Reward loss: 34.2871 | Policy loss: 15.0722\n",
            "\n",
            "\n",
            "Training episode 7...\n",
            "Total reward: -100.31 | Steps: 118 | Time: 1.85s\n",
            "\n",
            "Loss: 59.1561 | Value loss: 40.3609 | Reward loss: 33.9858 | Policy loss: 15.0801\n",
            "\n",
            "\n",
            "Training episode 8...\n",
            "Total reward: -99.48 | Steps: 87 | Time: 1.33s\n",
            "\n",
            "Loss: 59.0098 | Value loss: 40.2397 | Reward loss: 33.8481 | Policy loss: 15.1018\n",
            "\n",
            "\n",
            "Training episode 9...\n",
            "Total reward: -253.58 | Steps: 109 | Time: 1.67s\n",
            "\n",
            "Loss: 58.8033 | Value loss: 40.0479 | Reward loss: 33.6907 | Policy loss: 15.1006\n",
            "\n",
            "\n",
            "Training episode 10...\n",
            "Total reward: -103.09 | Steps: 63 | Time: 0.95s\n",
            "\n",
            "Loss: 58.5540 | Value loss: 40.0307 | Reward loss: 33.4075 | Policy loss: 15.1389\n",
            "\n",
            "\n",
            "Training episode 11...\n",
            "Total reward: -148.94 | Steps: 72 | Time: 1.11s\n",
            "\n",
            "Loss: 58.3223 | Value loss: 39.9535 | Reward loss: 33.2082 | Policy loss: 15.1257\n",
            "\n",
            "\n",
            "Training episode 12...\n",
            "Total reward: -92.65 | Steps: 126 | Time: 1.93s\n",
            "\n",
            "Loss: 58.1722 | Value loss: 39.9550 | Reward loss: 33.0321 | Policy loss: 15.1513\n",
            "\n",
            "\n",
            "Training episode 13...\n",
            "Total reward: -172.71 | Steps: 75 | Time: 1.19s\n",
            "\n",
            "Loss: 58.0844 | Value loss: 39.8643 | Reward loss: 32.9540 | Policy loss: 15.1644\n",
            "\n",
            "\n",
            "Training episode 14...\n",
            "Total reward: -86.10 | Steps: 64 | Time: 1.19s\n",
            "\n",
            "Loss: 57.9915 | Value loss: 39.8801 | Reward loss: 32.8393 | Policy loss: 15.1821\n",
            "\n",
            "\n",
            "Training episode 15...\n",
            "Total reward: -75.59 | Steps: 79 | Time: 1.62s\n",
            "\n",
            "Loss: 57.9036 | Value loss: 39.7047 | Reward loss: 32.7863 | Policy loss: 15.1912\n",
            "\n",
            "\n",
            "Training episode 16...\n",
            "Total reward: -289.26 | Steps: 125 | Time: 2.03s\n",
            "\n",
            "Loss: 57.8378 | Value loss: 39.6390 | Reward loss: 32.7347 | Policy loss: 15.1934\n",
            "\n",
            "\n",
            "Training episode 17...\n",
            "Total reward: -109.31 | Steps: 86 | Time: 1.39s\n",
            "\n",
            "Loss: 57.7776 | Value loss: 39.5285 | Reward loss: 32.6812 | Policy loss: 15.2143\n",
            "\n",
            "\n",
            "Training episode 18...\n",
            "Total reward: -52.13 | Steps: 128 | Time: 1.97s\n",
            "\n",
            "Loss: 57.6916 | Value loss: 39.6334 | Reward loss: 32.5685 | Policy loss: 15.2148\n",
            "\n",
            "\n",
            "Training episode 19...\n",
            "Total reward: -83.39 | Steps: 73 | Time: 1.17s\n",
            "\n",
            "Loss: 57.6561 | Value loss: 39.5518 | Reward loss: 32.5599 | Policy loss: 15.2083\n",
            "\n",
            "\n",
            "Testing episode 20...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_20_test_0.mp4\n",
            "Total reward: -118.85\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_20_test_1.mp4\n",
            "Total reward: -117.07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_20_test_2.mp4\n",
            "Total reward: -128.16\n",
            "\n",
            "\n",
            "Training episode 20...\n",
            "Total reward: -109.31 | Steps: 72 | Time: 1.46s\n",
            "\n",
            "Loss: 57.6775 | Value loss: 39.5537 | Reward loss: 32.5753 | Policy loss: 15.2138\n",
            "\n",
            "\n",
            "Training episode 21...\n",
            "Total reward: -129.51 | Steps: 98 | Time: 1.87s\n",
            "\n",
            "Loss: 57.5589 | Value loss: 39.4046 | Reward loss: 32.4831 | Policy loss: 15.2246\n",
            "\n",
            "\n",
            "Training episode 22...\n",
            "Total reward: -126.87 | Steps: 94 | Time: 1.69s\n",
            "\n",
            "Loss: 57.6260 | Value loss: 39.4672 | Reward loss: 32.5347 | Policy loss: 15.2245\n",
            "\n",
            "\n",
            "Training episode 23...\n",
            "Total reward: -239.86 | Steps: 111 | Time: 1.74s\n",
            "\n",
            "Loss: 57.5945 | Value loss: 39.4773 | Reward loss: 32.5172 | Policy loss: 15.2080\n",
            "\n",
            "\n",
            "Training episode 24...\n",
            "Total reward: -88.71 | Steps: 123 | Time: 1.90s\n",
            "\n",
            "Loss: 57.6023 | Value loss: 39.5801 | Reward loss: 32.4879 | Policy loss: 15.2193\n",
            "\n",
            "\n",
            "Training episode 25...\n",
            "Total reward: -335.42 | Steps: 113 | Time: 1.92s\n",
            "\n",
            "Loss: 57.4886 | Value loss: 39.4039 | Reward loss: 32.4130 | Policy loss: 15.2246\n",
            "\n",
            "\n",
            "Training episode 26...\n",
            "Total reward: -125.38 | Steps: 119 | Time: 1.84s\n",
            "\n",
            "Loss: 57.5575 | Value loss: 39.5494 | Reward loss: 32.4526 | Policy loss: 15.2176\n",
            "\n",
            "\n",
            "Training episode 27...\n",
            "Total reward: -458.07 | Steps: 105 | Time: 1.65s\n",
            "\n",
            "Loss: 57.5682 | Value loss: 39.5919 | Reward loss: 32.4469 | Policy loss: 15.2233\n",
            "\n",
            "\n",
            "Training episode 28...\n",
            "Total reward: -250.53 | Steps: 107 | Time: 1.68s\n",
            "\n",
            "Loss: 57.5427 | Value loss: 39.5340 | Reward loss: 32.4448 | Policy loss: 15.2144\n",
            "\n",
            "\n",
            "Training episode 29...\n",
            "Total reward: -95.53 | Steps: 67 | Time: 1.02s\n",
            "\n",
            "Loss: 57.7064 | Value loss: 39.4708 | Reward loss: 32.6095 | Policy loss: 15.2291\n",
            "\n",
            "\n",
            "Training episode 30...\n",
            "Total reward: -110.82 | Steps: 81 | Time: 1.23s\n",
            "\n",
            "Loss: 57.7701 | Value loss: 39.6725 | Reward loss: 32.6250 | Policy loss: 15.2269\n",
            "\n",
            "\n",
            "Training episode 31...\n",
            "Total reward: -239.83 | Steps: 95 | Time: 1.45s\n",
            "\n",
            "Loss: 57.9636 | Value loss: 39.6163 | Reward loss: 32.8275 | Policy loss: 15.2320\n",
            "\n",
            "\n",
            "Training episode 32...\n",
            "Total reward: -2.36 | Steps: 64 | Time: 0.98s\n",
            "\n",
            "Loss: 58.1189 | Value loss: 39.6134 | Reward loss: 32.9880 | Policy loss: 15.2275\n",
            "\n",
            "\n",
            "Training episode 33...\n",
            "Total reward: -97.51 | Steps: 70 | Time: 1.05s\n",
            "\n",
            "Loss: 57.8826 | Value loss: 39.5305 | Reward loss: 32.7683 | Policy loss: 15.2317\n",
            "\n",
            "\n",
            "Training episode 34...\n",
            "Total reward: -108.83 | Steps: 68 | Time: 1.05s\n",
            "\n",
            "Loss: 58.2443 | Value loss: 39.7854 | Reward loss: 33.0622 | Policy loss: 15.2357\n",
            "\n",
            "\n",
            "Training episode 35...\n",
            "Total reward: -0.09 | Steps: 79 | Time: 1.23s\n",
            "\n",
            "Loss: 58.2090 | Value loss: 39.7603 | Reward loss: 33.0363 | Policy loss: 15.2326\n",
            "\n",
            "\n",
            "Training episode 36...\n",
            "Total reward: -430.02 | Steps: 128 | Time: 2.17s\n",
            "\n",
            "Loss: 58.6120 | Value loss: 39.9995 | Reward loss: 33.3819 | Policy loss: 15.2302\n",
            "\n",
            "\n",
            "Training episode 37...\n",
            "Total reward: -423.89 | Steps: 89 | Time: 1.41s\n",
            "\n",
            "Loss: 58.5127 | Value loss: 39.9231 | Reward loss: 33.2987 | Policy loss: 15.2333\n",
            "\n",
            "\n",
            "Training episode 38...\n",
            "Total reward: -100.97 | Steps: 64 | Time: 0.97s\n",
            "\n",
            "Loss: 58.5819 | Value loss: 39.7204 | Reward loss: 33.4144 | Policy loss: 15.2373\n",
            "\n",
            "\n",
            "Training episode 39...\n",
            "Total reward: -188.80 | Steps: 107 | Time: 1.62s\n",
            "\n",
            "Loss: 58.6537 | Value loss: 39.8310 | Reward loss: 33.4603 | Policy loss: 15.2356\n",
            "\n",
            "\n",
            "Testing episode 40...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_40_test_0.mp4\n",
            "Total reward: -88.62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_40_test_1.mp4\n",
            "Total reward: -98.83\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_40_test_2.mp4\n",
            "Total reward: -69.99\n",
            "\n",
            "\n",
            "Training episode 40...\n",
            "Total reward: -79.51 | Steps: 118 | Time: 1.91s\n",
            "\n",
            "Loss: 58.7808 | Value loss: 39.9245 | Reward loss: 33.5639 | Policy loss: 15.2358\n",
            "\n",
            "\n",
            "Training episode 41...\n",
            "Total reward: -98.70 | Steps: 82 | Time: 1.23s\n",
            "\n",
            "Loss: 58.7421 | Value loss: 39.8721 | Reward loss: 33.5333 | Policy loss: 15.2408\n",
            "\n",
            "\n",
            "Training episode 42...\n",
            "Total reward: -163.96 | Steps: 101 | Time: 1.53s\n",
            "\n",
            "Loss: 58.9585 | Value loss: 40.0320 | Reward loss: 33.7130 | Policy loss: 15.2375\n",
            "\n",
            "\n",
            "Training episode 43...\n",
            "Total reward: -196.02 | Steps: 112 | Time: 1.72s\n",
            "\n",
            "Loss: 58.9019 | Value loss: 39.8366 | Reward loss: 33.7081 | Policy loss: 15.2346\n",
            "\n",
            "\n",
            "Training episode 44...\n",
            "Total reward: -127.52 | Steps: 107 | Time: 1.61s\n",
            "\n",
            "Loss: 58.9680 | Value loss: 39.9619 | Reward loss: 33.7362 | Policy loss: 15.2413\n",
            "\n",
            "\n",
            "Training episode 45...\n",
            "Total reward: -66.62 | Steps: 101 | Time: 1.53s\n",
            "\n",
            "Loss: 59.1329 | Value loss: 40.0311 | Reward loss: 33.8829 | Policy loss: 15.2423\n",
            "\n",
            "\n",
            "Training episode 46...\n",
            "Total reward: -124.81 | Steps: 112 | Time: 1.68s\n",
            "\n",
            "Loss: 59.1923 | Value loss: 40.1153 | Reward loss: 33.9253 | Policy loss: 15.2382\n",
            "\n",
            "\n",
            "Training episode 47...\n",
            "Total reward: -95.55 | Steps: 66 | Time: 1.00s\n",
            "\n",
            "Loss: 59.2689 | Value loss: 40.1047 | Reward loss: 34.0061 | Policy loss: 15.2366\n",
            "\n",
            "\n",
            "Training episode 48...\n",
            "Total reward: -151.55 | Steps: 126 | Time: 1.89s\n",
            "\n",
            "Loss: 59.2961 | Value loss: 40.1263 | Reward loss: 34.0244 | Policy loss: 15.2401\n",
            "\n",
            "\n",
            "Training episode 49...\n",
            "Total reward: -295.23 | Steps: 74 | Time: 1.11s\n",
            "\n",
            "Loss: 59.3026 | Value loss: 40.1330 | Reward loss: 34.0291 | Policy loss: 15.2402\n",
            "\n",
            "\n",
            "Training episode 50...\n",
            "Total reward: -83.42 | Steps: 57 | Time: 0.87s\n",
            "\n",
            "Loss: 59.4345 | Value loss: 40.1500 | Reward loss: 34.1552 | Policy loss: 15.2418\n",
            "\n",
            "\n",
            "Training episode 51...\n",
            "Total reward: -297.87 | Steps: 96 | Time: 1.44s\n",
            "\n",
            "Loss: 59.5606 | Value loss: 40.2575 | Reward loss: 34.2566 | Policy loss: 15.2395\n",
            "\n",
            "\n",
            "Training episode 52...\n",
            "Total reward: -101.35 | Steps: 73 | Time: 1.14s\n",
            "\n",
            "Loss: 59.5447 | Value loss: 40.3082 | Reward loss: 34.2263 | Policy loss: 15.2414\n",
            "\n",
            "\n",
            "Training episode 53...\n",
            "Total reward: -200.81 | Steps: 97 | Time: 1.48s\n",
            "\n",
            "Loss: 59.4330 | Value loss: 40.1487 | Reward loss: 34.1615 | Policy loss: 15.2344\n",
            "\n",
            "\n",
            "Training episode 54...\n",
            "Total reward: -394.33 | Steps: 83 | Time: 1.26s\n",
            "\n",
            "Loss: 59.5415 | Value loss: 40.2413 | Reward loss: 34.2384 | Policy loss: 15.2427\n",
            "\n",
            "\n",
            "Training episode 55...\n",
            "Total reward: -168.83 | Steps: 123 | Time: 1.88s\n",
            "\n",
            "Loss: 59.7118 | Value loss: 40.3497 | Reward loss: 34.3822 | Policy loss: 15.2421\n",
            "\n",
            "\n",
            "Training episode 56...\n",
            "Total reward: -99.77 | Steps: 98 | Time: 1.52s\n",
            "\n",
            "Loss: 59.7966 | Value loss: 40.3951 | Reward loss: 34.4526 | Policy loss: 15.2452\n",
            "\n",
            "\n",
            "Training episode 57...\n",
            "Total reward: -305.65 | Steps: 109 | Time: 1.67s\n",
            "\n",
            "Loss: 59.6534 | Value loss: 40.3399 | Reward loss: 34.3259 | Policy loss: 15.2425\n",
            "\n",
            "\n",
            "Training episode 58...\n",
            "Total reward: -112.29 | Steps: 96 | Time: 1.45s\n",
            "\n",
            "Loss: 59.7056 | Value loss: 40.3595 | Reward loss: 34.3755 | Policy loss: 15.2403\n",
            "\n",
            "\n",
            "Training episode 59...\n",
            "Total reward: -121.87 | Steps: 75 | Time: 1.13s\n",
            "\n",
            "Loss: 59.6730 | Value loss: 40.3586 | Reward loss: 34.3373 | Policy loss: 15.2461\n",
            "\n",
            "\n",
            "Testing episode 60...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_60_test_0.mp4\n",
            "Total reward: -104.32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_60_test_1.mp4\n",
            "Total reward: -119.89\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_60_test_2.mp4\n",
            "Total reward: -109.21\n",
            "\n",
            "\n",
            "Training episode 60...\n",
            "Total reward: -155.75 | Steps: 125 | Time: 2.10s\n",
            "\n",
            "Loss: 59.7457 | Value loss: 40.3251 | Reward loss: 34.4222 | Policy loss: 15.2422\n",
            "\n",
            "\n",
            "Training episode 61...\n",
            "Total reward: -81.95 | Steps: 83 | Time: 1.32s\n",
            "\n",
            "Loss: 59.7001 | Value loss: 40.3372 | Reward loss: 34.3699 | Policy loss: 15.2459\n",
            "\n",
            "\n",
            "Training episode 62...\n",
            "Total reward: -173.16 | Steps: 101 | Time: 1.54s\n",
            "\n",
            "Loss: 59.8099 | Value loss: 40.3944 | Reward loss: 34.4682 | Policy loss: 15.2431\n",
            "\n",
            "\n",
            "Training episode 63...\n",
            "Total reward: -85.35 | Steps: 67 | Time: 1.05s\n",
            "\n",
            "Loss: 59.9399 | Value loss: 40.4183 | Reward loss: 34.5918 | Policy loss: 15.2436\n",
            "\n",
            "\n",
            "Training episode 64...\n",
            "Total reward: -202.29 | Steps: 115 | Time: 1.76s\n",
            "\n",
            "Loss: 59.7089 | Value loss: 40.3275 | Reward loss: 34.3832 | Policy loss: 15.2438\n",
            "\n",
            "\n",
            "Training episode 65...\n",
            "Total reward: -83.78 | Steps: 71 | Time: 1.10s\n",
            "\n",
            "Loss: 59.7955 | Value loss: 40.3534 | Reward loss: 34.4638 | Policy loss: 15.2434\n",
            "\n",
            "\n",
            "Training episode 66...\n",
            "Total reward: -204.56 | Steps: 73 | Time: 1.22s\n",
            "\n",
            "Loss: 59.8506 | Value loss: 40.3927 | Reward loss: 34.5084 | Policy loss: 15.2441\n",
            "\n",
            "\n",
            "Training episode 67...\n",
            "Total reward: -334.55 | Steps: 94 | Time: 1.43s\n",
            "\n",
            "Loss: 59.9403 | Value loss: 40.4662 | Reward loss: 34.5780 | Policy loss: 15.2457\n",
            "\n",
            "\n",
            "Training episode 68...\n",
            "Total reward: -522.52 | Steps: 116 | Time: 1.79s\n",
            "\n",
            "Loss: 59.8222 | Value loss: 40.4091 | Reward loss: 34.4776 | Policy loss: 15.2423\n",
            "\n",
            "\n",
            "Training episode 69...\n",
            "Total reward: -97.11 | Steps: 83 | Time: 1.36s\n",
            "\n",
            "Loss: 59.7583 | Value loss: 40.4104 | Reward loss: 34.4115 | Policy loss: 15.2442\n",
            "\n",
            "\n",
            "Training episode 70...\n",
            "Total reward: -110.16 | Steps: 71 | Time: 1.09s\n",
            "\n",
            "Loss: 59.7875 | Value loss: 40.4027 | Reward loss: 34.4395 | Policy loss: 15.2474\n",
            "\n",
            "\n",
            "Training episode 71...\n",
            "Total reward: -57.44 | Steps: 58 | Time: 0.93s\n",
            "\n",
            "Loss: 59.8374 | Value loss: 40.4660 | Reward loss: 34.4767 | Policy loss: 15.2442\n",
            "\n",
            "\n",
            "Training episode 72...\n",
            "Total reward: -62.86 | Steps: 122 | Time: 1.89s\n",
            "\n",
            "Loss: 59.9368 | Value loss: 40.4983 | Reward loss: 34.5662 | Policy loss: 15.2460\n",
            "\n",
            "\n",
            "Training episode 73...\n",
            "Total reward: -182.70 | Steps: 107 | Time: 1.73s\n",
            "\n",
            "Loss: 59.8784 | Value loss: 40.4410 | Reward loss: 34.5221 | Policy loss: 15.2461\n",
            "\n",
            "\n",
            "Training episode 74...\n",
            "Total reward: -118.14 | Steps: 95 | Time: 1.46s\n",
            "\n",
            "Loss: 59.8709 | Value loss: 40.4537 | Reward loss: 34.5110 | Policy loss: 15.2465\n",
            "\n",
            "\n",
            "Training episode 75...\n",
            "Total reward: -125.64 | Steps: 63 | Time: 0.95s\n",
            "\n",
            "Loss: 59.8124 | Value loss: 40.4977 | Reward loss: 34.4428 | Policy loss: 15.2452\n",
            "\n",
            "\n",
            "Training episode 76...\n",
            "Total reward: -208.14 | Steps: 95 | Time: 1.44s\n",
            "\n",
            "Loss: 59.6981 | Value loss: 40.3559 | Reward loss: 34.3638 | Policy loss: 15.2453\n",
            "\n",
            "\n",
            "Training episode 77...\n",
            "Total reward: -91.20 | Steps: 97 | Time: 1.50s\n",
            "\n",
            "Loss: 59.8126 | Value loss: 40.4479 | Reward loss: 34.4557 | Policy loss: 15.2449\n",
            "\n",
            "\n",
            "Training episode 78...\n",
            "Total reward: -176.60 | Steps: 62 | Time: 0.94s\n",
            "\n",
            "Loss: 59.7224 | Value loss: 40.4655 | Reward loss: 34.3604 | Policy loss: 15.2457\n",
            "\n",
            "\n",
            "Training episode 79...\n",
            "Total reward: -410.00 | Steps: 108 | Time: 1.62s\n",
            "\n",
            "Loss: 59.6879 | Value loss: 40.4140 | Reward loss: 34.3403 | Policy loss: 15.2440\n",
            "\n",
            "\n",
            "Testing episode 80...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_80_test_0.mp4\n",
            "Total reward: -118.99\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_80_test_1.mp4\n",
            "Total reward: -257.45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_80_test_2.mp4\n",
            "Total reward: -121.00\n",
            "\n",
            "\n",
            "Training episode 80...\n",
            "Total reward: -99.02 | Steps: 81 | Time: 1.25s\n",
            "\n",
            "Loss: 59.8704 | Value loss: 40.4750 | Reward loss: 34.5056 | Policy loss: 15.2461\n",
            "\n",
            "\n",
            "Training episode 81...\n",
            "Total reward: -126.63 | Steps: 60 | Time: 0.97s\n",
            "\n",
            "Loss: 59.6951 | Value loss: 40.4500 | Reward loss: 34.3361 | Policy loss: 15.2464\n",
            "\n",
            "\n",
            "Training episode 82...\n",
            "Total reward: -358.91 | Steps: 101 | Time: 1.54s\n",
            "\n",
            "Loss: 59.7570 | Value loss: 40.4317 | Reward loss: 34.4044 | Policy loss: 15.2447\n",
            "\n",
            "\n",
            "Training episode 83...\n",
            "Total reward: -142.85 | Steps: 84 | Time: 1.27s\n",
            "\n",
            "Loss: 59.6721 | Value loss: 40.4722 | Reward loss: 34.3086 | Policy loss: 15.2454\n",
            "\n",
            "\n",
            "Training episode 84...\n",
            "Total reward: -115.01 | Steps: 79 | Time: 1.20s\n",
            "\n",
            "Loss: 59.7683 | Value loss: 40.4598 | Reward loss: 34.4083 | Policy loss: 15.2450\n",
            "\n",
            "\n",
            "Training episode 85...\n",
            "Total reward: -119.67 | Steps: 65 | Time: 0.98s\n",
            "\n",
            "Loss: 59.6795 | Value loss: 40.4884 | Reward loss: 34.3153 | Policy loss: 15.2421\n",
            "\n",
            "\n",
            "Training episode 86...\n",
            "Total reward: -324.95 | Steps: 85 | Time: 1.32s\n",
            "\n",
            "Loss: 59.6621 | Value loss: 40.3968 | Reward loss: 34.3199 | Policy loss: 15.2431\n",
            "\n",
            "\n",
            "Training episode 87...\n",
            "Total reward: -172.30 | Steps: 109 | Time: 1.65s\n",
            "\n",
            "Loss: 59.7831 | Value loss: 40.4982 | Reward loss: 34.4117 | Policy loss: 15.2468\n",
            "\n",
            "\n",
            "Training episode 88...\n",
            "Total reward: -175.17 | Steps: 103 | Time: 1.55s\n",
            "\n",
            "Loss: 59.6901 | Value loss: 40.4564 | Reward loss: 34.3310 | Policy loss: 15.2451\n",
            "\n",
            "\n",
            "Training episode 89...\n",
            "Total reward: -200.12 | Steps: 121 | Time: 1.82s\n",
            "\n",
            "Loss: 59.6237 | Value loss: 40.4918 | Reward loss: 34.2552 | Policy loss: 15.2456\n",
            "\n",
            "\n",
            "Training episode 90...\n",
            "Total reward: -134.60 | Steps: 102 | Time: 1.57s\n",
            "\n",
            "Loss: 59.6647 | Value loss: 40.4670 | Reward loss: 34.3035 | Policy loss: 15.2445\n",
            "\n",
            "\n",
            "Training episode 91...\n",
            "Total reward: -125.59 | Steps: 127 | Time: 1.91s\n",
            "\n",
            "Loss: 59.6357 | Value loss: 40.4942 | Reward loss: 34.2678 | Policy loss: 15.2443\n",
            "\n",
            "\n",
            "Training episode 92...\n",
            "Total reward: -130.73 | Steps: 102 | Time: 1.54s\n",
            "\n",
            "Loss: 59.6071 | Value loss: 40.4611 | Reward loss: 34.2466 | Policy loss: 15.2452\n",
            "\n",
            "\n",
            "Training episode 93...\n",
            "Total reward: -88.97 | Steps: 127 | Time: 1.89s\n",
            "\n",
            "Loss: 59.6769 | Value loss: 40.5347 | Reward loss: 34.2970 | Policy loss: 15.2462\n",
            "\n",
            "\n",
            "Training episode 94...\n",
            "Total reward: -113.01 | Steps: 115 | Time: 1.73s\n",
            "\n",
            "Loss: 59.6189 | Value loss: 40.5256 | Reward loss: 34.2407 | Policy loss: 15.2468\n",
            "\n",
            "\n",
            "Training episode 95...\n",
            "Total reward: -123.88 | Steps: 118 | Time: 1.79s\n",
            "\n",
            "Loss: 59.6384 | Value loss: 40.5020 | Reward loss: 34.2663 | Policy loss: 15.2466\n",
            "\n",
            "\n",
            "Training episode 96...\n",
            "Total reward: -100.26 | Steps: 105 | Time: 1.59s\n",
            "\n",
            "Loss: 59.4978 | Value loss: 40.4629 | Reward loss: 34.1368 | Policy loss: 15.2452\n",
            "\n",
            "\n",
            "Training episode 97...\n",
            "Total reward: -76.59 | Steps: 57 | Time: 0.85s\n",
            "\n",
            "Loss: 59.6304 | Value loss: 40.5240 | Reward loss: 34.2530 | Policy loss: 15.2464\n",
            "\n",
            "\n",
            "Training episode 98...\n",
            "Total reward: -399.24 | Steps: 140 | Time: 2.15s\n",
            "\n",
            "Loss: 59.5929 | Value loss: 40.5202 | Reward loss: 34.2164 | Policy loss: 15.2464\n",
            "\n",
            "\n",
            "Training episode 99...\n",
            "Total reward: -224.58 | Steps: 90 | Time: 1.36s\n",
            "\n",
            "Loss: 59.5833 | Value loss: 40.4983 | Reward loss: 34.2112 | Policy loss: 15.2475\n",
            "\n",
            "\n",
            "Testing episode 100...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_100_test_0.mp4\n",
            "Total reward: -187.86\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_100_test_1.mp4\n",
            "Total reward: -140.97\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_100_test_2.mp4\n",
            "Total reward: -119.28\n",
            "\n",
            "\n",
            "Training episode 100...\n",
            "Total reward: -57.86 | Steps: 67 | Time: 1.06s\n",
            "\n",
            "Loss: 59.5965 | Value loss: 40.5066 | Reward loss: 34.2223 | Policy loss: 15.2476\n",
            "\n",
            "\n",
            "Training episode 101...\n",
            "Total reward: -211.61 | Steps: 84 | Time: 1.26s\n",
            "\n",
            "Loss: 59.6736 | Value loss: 40.4879 | Reward loss: 34.3043 | Policy loss: 15.2473\n",
            "\n",
            "\n",
            "Training episode 102...\n",
            "Total reward: -84.38 | Steps: 63 | Time: 0.97s\n",
            "\n",
            "Loss: 59.5948 | Value loss: 40.5212 | Reward loss: 34.2160 | Policy loss: 15.2485\n",
            "\n",
            "\n",
            "Training episode 103...\n",
            "Total reward: -140.29 | Steps: 64 | Time: 0.97s\n",
            "\n",
            "Loss: 59.5817 | Value loss: 40.4889 | Reward loss: 34.2118 | Policy loss: 15.2477\n",
            "\n",
            "\n",
            "Training episode 104...\n",
            "Total reward: -194.52 | Steps: 74 | Time: 1.16s\n",
            "\n",
            "Loss: 59.5234 | Value loss: 40.4665 | Reward loss: 34.1590 | Policy loss: 15.2478\n",
            "\n",
            "\n",
            "Training episode 105...\n",
            "Total reward: -137.84 | Steps: 72 | Time: 1.09s\n",
            "\n",
            "Loss: 59.5344 | Value loss: 40.5083 | Reward loss: 34.1593 | Policy loss: 15.2480\n",
            "\n",
            "\n",
            "Training episode 106...\n",
            "Total reward: -297.64 | Steps: 122 | Time: 1.85s\n",
            "\n",
            "Loss: 59.5308 | Value loss: 40.5031 | Reward loss: 34.1573 | Policy loss: 15.2478\n",
            "\n",
            "\n",
            "Training episode 107...\n",
            "Total reward: -234.55 | Steps: 63 | Time: 0.95s\n",
            "\n",
            "Loss: 59.5742 | Value loss: 40.4993 | Reward loss: 34.2019 | Policy loss: 15.2474\n",
            "\n",
            "\n",
            "Training episode 108...\n",
            "Total reward: -227.89 | Steps: 85 | Time: 1.27s\n",
            "\n",
            "Loss: 59.5538 | Value loss: 40.5069 | Reward loss: 34.1798 | Policy loss: 15.2472\n",
            "\n",
            "\n",
            "Training episode 109...\n",
            "Total reward: -198.37 | Steps: 123 | Time: 1.86s\n",
            "\n",
            "Loss: 59.4824 | Value loss: 40.4954 | Reward loss: 34.1114 | Policy loss: 15.2472\n",
            "\n",
            "\n",
            "Training episode 110...\n",
            "Total reward: -334.96 | Steps: 101 | Time: 1.51s\n",
            "\n",
            "Loss: 59.4438 | Value loss: 40.4711 | Reward loss: 34.0788 | Policy loss: 15.2472\n",
            "\n",
            "\n",
            "Training episode 111...\n",
            "Total reward: -98.70 | Steps: 67 | Time: 1.01s\n",
            "\n",
            "Loss: 59.5471 | Value loss: 40.5138 | Reward loss: 34.1712 | Policy loss: 15.2475\n",
            "\n",
            "\n",
            "Training episode 112...\n",
            "Total reward: -126.06 | Steps: 84 | Time: 1.27s\n",
            "\n",
            "Loss: 59.5309 | Value loss: 40.5573 | Reward loss: 34.1446 | Policy loss: 15.2470\n",
            "\n",
            "\n",
            "Training episode 113...\n",
            "Total reward: -71.29 | Steps: 107 | Time: 1.60s\n",
            "\n",
            "Loss: 59.4634 | Value loss: 40.4808 | Reward loss: 34.0962 | Policy loss: 15.2470\n",
            "\n",
            "\n",
            "Training episode 114...\n",
            "Total reward: -250.69 | Steps: 102 | Time: 1.54s\n",
            "\n",
            "Loss: 59.3814 | Value loss: 40.4874 | Reward loss: 34.0117 | Policy loss: 15.2478\n",
            "\n",
            "\n",
            "Training episode 115...\n",
            "Total reward: -82.02 | Steps: 63 | Time: 0.95s\n",
            "\n",
            "Loss: 59.5696 | Value loss: 40.5363 | Reward loss: 34.1880 | Policy loss: 15.2475\n",
            "\n",
            "\n",
            "Training episode 116...\n",
            "Total reward: -151.61 | Steps: 76 | Time: 1.14s\n",
            "\n",
            "Loss: 59.4404 | Value loss: 40.5178 | Reward loss: 34.0647 | Policy loss: 15.2462\n",
            "\n",
            "\n",
            "Training episode 117...\n",
            "Total reward: -123.07 | Steps: 74 | Time: 1.12s\n",
            "\n",
            "Loss: 59.4604 | Value loss: 40.5008 | Reward loss: 34.0882 | Policy loss: 15.2470\n",
            "\n",
            "\n",
            "Training episode 118...\n",
            "Total reward: -71.32 | Steps: 69 | Time: 1.03s\n",
            "\n",
            "Loss: 59.5131 | Value loss: 40.5262 | Reward loss: 34.1338 | Policy loss: 15.2477\n",
            "\n",
            "\n",
            "Training episode 119...\n",
            "Total reward: -82.69 | Steps: 64 | Time: 0.98s\n",
            "\n",
            "Loss: 59.6116 | Value loss: 40.5782 | Reward loss: 34.2185 | Policy loss: 15.2486\n",
            "\n",
            "\n",
            "Testing episode 120...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_120_test_0.mp4\n",
            "Total reward: -142.51\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_120_test_1.mp4\n",
            "Total reward: -110.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_120_test_2.mp4\n",
            "Total reward: -430.11\n",
            "\n",
            "\n",
            "Training episode 120...\n",
            "Total reward: -96.03 | Steps: 97 | Time: 1.52s\n",
            "\n",
            "Loss: 59.3902 | Value loss: 40.4909 | Reward loss: 34.0199 | Policy loss: 15.2476\n",
            "\n",
            "\n",
            "Training episode 121...\n",
            "Total reward: -452.26 | Steps: 92 | Time: 1.42s\n",
            "\n",
            "Loss: 59.4492 | Value loss: 40.4925 | Reward loss: 34.0786 | Policy loss: 15.2475\n",
            "\n",
            "\n",
            "Training episode 122...\n",
            "Total reward: -103.11 | Steps: 115 | Time: 1.75s\n",
            "\n",
            "Loss: 59.4082 | Value loss: 40.5235 | Reward loss: 34.0298 | Policy loss: 15.2475\n",
            "\n",
            "\n",
            "Training episode 123...\n",
            "Total reward: -110.66 | Steps: 67 | Time: 1.01s\n",
            "\n",
            "Loss: 59.5048 | Value loss: 40.5329 | Reward loss: 34.1227 | Policy loss: 15.2489\n",
            "\n",
            "\n",
            "Training episode 124...\n",
            "Total reward: -143.66 | Steps: 114 | Time: 1.72s\n",
            "\n",
            "Loss: 59.4207 | Value loss: 40.4892 | Reward loss: 34.0500 | Policy loss: 15.2485\n",
            "\n",
            "\n",
            "Training episode 125...\n",
            "Total reward: -7.98 | Steps: 92 | Time: 1.38s\n",
            "\n",
            "Loss: 59.4201 | Value loss: 40.5499 | Reward loss: 34.0342 | Policy loss: 15.2485\n",
            "\n",
            "\n",
            "Training episode 126...\n",
            "Total reward: -479.86 | Steps: 86 | Time: 1.30s\n",
            "\n",
            "Loss: 59.3922 | Value loss: 40.5083 | Reward loss: 34.0170 | Policy loss: 15.2481\n",
            "\n",
            "\n",
            "Training episode 127...\n",
            "Total reward: -212.60 | Steps: 66 | Time: 0.99s\n",
            "\n",
            "Loss: 59.5859 | Value loss: 40.5373 | Reward loss: 34.2031 | Policy loss: 15.2486\n",
            "\n",
            "\n",
            "Training episode 128...\n",
            "Total reward: -90.99 | Steps: 65 | Time: 0.99s\n",
            "\n",
            "Loss: 59.4100 | Value loss: 40.5235 | Reward loss: 34.0308 | Policy loss: 15.2483\n",
            "\n",
            "\n",
            "Training episode 129...\n",
            "Total reward: -412.52 | Steps: 111 | Time: 1.73s\n",
            "\n",
            "Loss: 59.4102 | Value loss: 40.5132 | Reward loss: 34.0336 | Policy loss: 15.2484\n",
            "\n",
            "\n",
            "Training episode 130...\n",
            "Total reward: -5.10 | Steps: 69 | Time: 1.05s\n",
            "\n",
            "Loss: 59.5328 | Value loss: 40.5130 | Reward loss: 34.1562 | Policy loss: 15.2484\n",
            "\n",
            "\n",
            "Training episode 131...\n",
            "Total reward: -40.62 | Steps: 87 | Time: 1.33s\n",
            "\n",
            "Loss: 59.3206 | Value loss: 40.4865 | Reward loss: 33.9506 | Policy loss: 15.2483\n",
            "\n",
            "\n",
            "Training episode 132...\n",
            "Total reward: -165.51 | Steps: 104 | Time: 1.59s\n",
            "\n",
            "Loss: 59.3613 | Value loss: 40.5206 | Reward loss: 33.9825 | Policy loss: 15.2486\n",
            "\n",
            "\n",
            "Training episode 133...\n",
            "Total reward: -334.58 | Steps: 101 | Time: 1.58s\n",
            "\n",
            "Loss: 59.3242 | Value loss: 40.4919 | Reward loss: 33.9524 | Policy loss: 15.2488\n",
            "\n",
            "\n",
            "Training episode 134...\n",
            "Total reward: -175.31 | Steps: 83 | Time: 1.28s\n",
            "\n",
            "Loss: 59.5394 | Value loss: 40.5556 | Reward loss: 34.1517 | Policy loss: 15.2488\n",
            "\n",
            "\n",
            "Training episode 135...\n",
            "Total reward: -118.88 | Steps: 92 | Time: 1.40s\n",
            "\n",
            "Loss: 59.3688 | Value loss: 40.5114 | Reward loss: 33.9926 | Policy loss: 15.2484\n",
            "\n",
            "\n",
            "Training episode 136...\n",
            "Total reward: -129.62 | Steps: 65 | Time: 1.00s\n",
            "\n",
            "Loss: 59.4407 | Value loss: 40.5782 | Reward loss: 34.0476 | Policy loss: 15.2486\n",
            "\n",
            "\n",
            "Training episode 137...\n",
            "Total reward: -103.63 | Steps: 89 | Time: 1.36s\n",
            "\n",
            "Loss: 59.4318 | Value loss: 40.5182 | Reward loss: 34.0536 | Policy loss: 15.2486\n",
            "\n",
            "\n",
            "Training episode 138...\n",
            "Total reward: -173.12 | Steps: 89 | Time: 1.40s\n",
            "\n",
            "Loss: 59.3181 | Value loss: 40.4929 | Reward loss: 33.9461 | Policy loss: 15.2488\n",
            "\n",
            "\n",
            "Training episode 139...\n",
            "Total reward: -133.39 | Steps: 102 | Time: 1.56s\n",
            "\n",
            "Loss: 59.2649 | Value loss: 40.5291 | Reward loss: 33.8837 | Policy loss: 15.2489\n",
            "\n",
            "\n",
            "Testing episode 140...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_140_test_0.mp4\n",
            "Total reward: -158.83\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_140_test_1.mp4\n",
            "Total reward: -124.36\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_140_test_2.mp4\n",
            "Total reward: 10.90\n",
            "\n",
            "\n",
            "Training episode 140...\n",
            "Total reward: -71.92 | Steps: 67 | Time: 1.03s\n",
            "\n",
            "Loss: 59.3927 | Value loss: 40.4725 | Reward loss: 34.0258 | Policy loss: 15.2488\n",
            "\n",
            "\n",
            "Training episode 141...\n",
            "Total reward: -157.33 | Steps: 111 | Time: 1.69s\n",
            "\n",
            "Loss: 59.3778 | Value loss: 40.5448 | Reward loss: 33.9928 | Policy loss: 15.2489\n",
            "\n",
            "\n",
            "Training episode 142...\n",
            "Total reward: -462.85 | Steps: 120 | Time: 1.87s\n",
            "\n",
            "Loss: 59.3287 | Value loss: 40.5309 | Reward loss: 33.9469 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 143...\n",
            "Total reward: -426.63 | Steps: 94 | Time: 1.47s\n",
            "\n",
            "Loss: 59.3082 | Value loss: 40.4791 | Reward loss: 33.9393 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 144...\n",
            "Total reward: -95.07 | Steps: 80 | Time: 1.22s\n",
            "\n",
            "Loss: 59.3268 | Value loss: 40.4844 | Reward loss: 33.9567 | Policy loss: 15.2490\n",
            "\n",
            "\n",
            "Training episode 145...\n",
            "Total reward: -310.91 | Steps: 91 | Time: 1.38s\n",
            "\n",
            "Loss: 59.4201 | Value loss: 40.5094 | Reward loss: 34.0437 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 146...\n",
            "Total reward: -196.31 | Steps: 81 | Time: 1.24s\n",
            "\n",
            "Loss: 59.4219 | Value loss: 40.5227 | Reward loss: 34.0421 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 147...\n",
            "Total reward: -133.55 | Steps: 103 | Time: 1.57s\n",
            "\n",
            "Loss: 59.2708 | Value loss: 40.5100 | Reward loss: 33.8941 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 148...\n",
            "Total reward: -155.54 | Steps: 82 | Time: 1.25s\n",
            "\n",
            "Loss: 59.2131 | Value loss: 40.5139 | Reward loss: 33.8354 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 149...\n",
            "Total reward: -121.25 | Steps: 126 | Time: 1.91s\n",
            "\n",
            "Loss: 59.2091 | Value loss: 40.4720 | Reward loss: 33.8419 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 150...\n",
            "Total reward: -115.00 | Steps: 94 | Time: 1.43s\n",
            "\n",
            "Loss: 59.3352 | Value loss: 40.5025 | Reward loss: 33.9605 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 151...\n",
            "Total reward: -106.29 | Steps: 76 | Time: 1.16s\n",
            "\n",
            "Loss: 59.2604 | Value loss: 40.5169 | Reward loss: 33.8821 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 152...\n",
            "Total reward: -207.80 | Steps: 109 | Time: 1.70s\n",
            "\n",
            "Loss: 59.2466 | Value loss: 40.5173 | Reward loss: 33.8682 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 153...\n",
            "Total reward: -95.45 | Steps: 87 | Time: 1.32s\n",
            "\n",
            "Loss: 59.3226 | Value loss: 40.4882 | Reward loss: 33.9517 | Policy loss: 15.2489\n",
            "\n",
            "\n",
            "Training episode 154...\n",
            "Total reward: -116.02 | Steps: 84 | Time: 1.28s\n",
            "\n",
            "Loss: 59.2726 | Value loss: 40.5623 | Reward loss: 33.8829 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 155...\n",
            "Total reward: -433.98 | Steps: 118 | Time: 1.79s\n",
            "\n",
            "Loss: 59.2185 | Value loss: 40.4962 | Reward loss: 33.8455 | Policy loss: 15.2489\n",
            "\n",
            "\n",
            "Training episode 156...\n",
            "Total reward: -400.00 | Steps: 117 | Time: 1.79s\n",
            "\n",
            "Loss: 59.1998 | Value loss: 40.4879 | Reward loss: 33.8285 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 157...\n",
            "Total reward: -391.19 | Steps: 85 | Time: 1.34s\n",
            "\n",
            "Loss: 59.2739 | Value loss: 40.5132 | Reward loss: 33.8966 | Policy loss: 15.2490\n",
            "\n",
            "\n",
            "Training episode 158...\n",
            "Total reward: -154.04 | Steps: 104 | Time: 1.63s\n",
            "\n",
            "Loss: 59.1196 | Value loss: 40.4883 | Reward loss: 33.7487 | Policy loss: 15.2489\n",
            "\n",
            "\n",
            "Training episode 159...\n",
            "Total reward: -111.59 | Steps: 66 | Time: 1.00s\n",
            "\n",
            "Loss: 59.1795 | Value loss: 40.5115 | Reward loss: 33.8030 | Policy loss: 15.2487\n",
            "\n",
            "\n",
            "Testing episode 160...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_160_test_0.mp4\n",
            "Total reward: -191.59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_160_test_1.mp4\n",
            "Total reward: -139.20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_160_test_2.mp4\n",
            "Total reward: -397.65\n",
            "\n",
            "\n",
            "Training episode 160...\n",
            "Total reward: -170.85 | Steps: 102 | Time: 1.63s\n",
            "\n",
            "Loss: 59.3388 | Value loss: 40.5445 | Reward loss: 33.9537 | Policy loss: 15.2490\n",
            "\n",
            "\n",
            "Training episode 161...\n",
            "Total reward: -107.20 | Steps: 84 | Time: 1.29s\n",
            "\n",
            "Loss: 59.0744 | Value loss: 40.4957 | Reward loss: 33.7013 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 162...\n",
            "Total reward: -149.27 | Steps: 80 | Time: 1.22s\n",
            "\n",
            "Loss: 59.3374 | Value loss: 40.5575 | Reward loss: 33.9490 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 163...\n",
            "Total reward: -314.48 | Steps: 100 | Time: 1.52s\n",
            "\n",
            "Loss: 59.3543 | Value loss: 40.5334 | Reward loss: 33.9717 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 164...\n",
            "Total reward: -115.79 | Steps: 104 | Time: 1.59s\n",
            "\n",
            "Loss: 59.2348 | Value loss: 40.5149 | Reward loss: 33.8567 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 165...\n",
            "Total reward: -432.60 | Steps: 112 | Time: 1.75s\n",
            "\n",
            "Loss: 59.2485 | Value loss: 40.5759 | Reward loss: 33.8554 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 166...\n",
            "Total reward: -97.47 | Steps: 73 | Time: 1.11s\n",
            "\n",
            "Loss: 59.2922 | Value loss: 40.5403 | Reward loss: 33.9081 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 167...\n",
            "Total reward: -236.74 | Steps: 98 | Time: 1.48s\n",
            "\n",
            "Loss: 59.1786 | Value loss: 40.5328 | Reward loss: 33.7963 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 168...\n",
            "Total reward: -157.19 | Steps: 69 | Time: 1.05s\n",
            "\n",
            "Loss: 59.2891 | Value loss: 40.5147 | Reward loss: 33.9113 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 169...\n",
            "Total reward: -355.86 | Steps: 100 | Time: 1.53s\n",
            "\n",
            "Loss: 59.2886 | Value loss: 40.5249 | Reward loss: 33.9082 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 170...\n",
            "Total reward: -107.63 | Steps: 98 | Time: 1.49s\n",
            "\n",
            "Loss: 59.3277 | Value loss: 40.5480 | Reward loss: 33.9414 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 171...\n",
            "Total reward: -227.89 | Steps: 65 | Time: 0.99s\n",
            "\n",
            "Loss: 59.1925 | Value loss: 40.5680 | Reward loss: 33.8014 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 172...\n",
            "Total reward: -158.99 | Steps: 83 | Time: 1.28s\n",
            "\n",
            "Loss: 59.0998 | Value loss: 40.4801 | Reward loss: 33.7305 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 173...\n",
            "Total reward: -190.31 | Steps: 80 | Time: 1.21s\n",
            "\n",
            "Loss: 59.2001 | Value loss: 40.5679 | Reward loss: 33.8089 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 174...\n",
            "Total reward: -105.44 | Steps: 75 | Time: 1.14s\n",
            "\n",
            "Loss: 59.1107 | Value loss: 40.5203 | Reward loss: 33.7314 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 175...\n",
            "Total reward: -392.34 | Steps: 101 | Time: 1.62s\n",
            "\n",
            "Loss: 59.1323 | Value loss: 40.5205 | Reward loss: 33.7530 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 176...\n",
            "Total reward: -153.01 | Steps: 93 | Time: 1.71s\n",
            "\n",
            "Loss: 59.1860 | Value loss: 40.5639 | Reward loss: 33.7959 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 177...\n",
            "Total reward: -133.84 | Steps: 109 | Time: 1.78s\n",
            "\n",
            "Loss: 59.1853 | Value loss: 40.5222 | Reward loss: 33.8056 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 178...\n",
            "Total reward: -108.91 | Steps: 68 | Time: 1.07s\n",
            "\n",
            "Loss: 59.1600 | Value loss: 40.5444 | Reward loss: 33.7747 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 179...\n",
            "Total reward: -311.17 | Steps: 93 | Time: 1.48s\n",
            "\n",
            "Loss: 59.1949 | Value loss: 40.5130 | Reward loss: 33.8176 | Policy loss: 15.2490\n",
            "\n",
            "\n",
            "Testing episode 180...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_180_test_0.mp4\n",
            "Total reward: -207.62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_180_test_1.mp4\n",
            "Total reward: -111.42\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_180_test_2.mp4\n",
            "Total reward: -114.87\n",
            "\n",
            "\n",
            "Training episode 180...\n",
            "Total reward: -126.67 | Steps: 75 | Time: 1.15s\n",
            "\n",
            "Loss: 59.1231 | Value loss: 40.5094 | Reward loss: 33.7467 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 181...\n",
            "Total reward: -266.07 | Steps: 83 | Time: 1.26s\n",
            "\n",
            "Loss: 59.1848 | Value loss: 40.5168 | Reward loss: 33.8064 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 182...\n",
            "Total reward: -117.26 | Steps: 98 | Time: 1.50s\n",
            "\n",
            "Loss: 59.2135 | Value loss: 40.4992 | Reward loss: 33.8395 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 183...\n",
            "Total reward: -145.21 | Steps: 127 | Time: 1.93s\n",
            "\n",
            "Loss: 59.1813 | Value loss: 40.4796 | Reward loss: 33.8123 | Policy loss: 15.2490\n",
            "\n",
            "\n",
            "Training episode 184...\n",
            "Total reward: -73.22 | Steps: 130 | Time: 1.98s\n",
            "\n",
            "Loss: 59.0986 | Value loss: 40.5087 | Reward loss: 33.7223 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 185...\n",
            "Total reward: -83.22 | Steps: 92 | Time: 1.45s\n",
            "\n",
            "Loss: 59.2319 | Value loss: 40.5197 | Reward loss: 33.8526 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 186...\n",
            "Total reward: -163.77 | Steps: 90 | Time: 1.37s\n",
            "\n",
            "Loss: 59.0877 | Value loss: 40.4811 | Reward loss: 33.7182 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 187...\n",
            "Total reward: -92.97 | Steps: 59 | Time: 0.90s\n",
            "\n",
            "Loss: 59.2315 | Value loss: 40.5194 | Reward loss: 33.8524 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 188...\n",
            "Total reward: -262.81 | Steps: 99 | Time: 1.50s\n",
            "\n",
            "Loss: 59.1550 | Value loss: 40.4939 | Reward loss: 33.7824 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 189...\n",
            "Total reward: -122.88 | Steps: 63 | Time: 1.02s\n",
            "\n",
            "Loss: 59.1568 | Value loss: 40.5142 | Reward loss: 33.7791 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 190...\n",
            "Total reward: -254.48 | Steps: 90 | Time: 1.37s\n",
            "\n",
            "Loss: 58.9936 | Value loss: 40.5052 | Reward loss: 33.6181 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 191...\n",
            "Total reward: -198.30 | Steps: 111 | Time: 1.71s\n",
            "\n",
            "Loss: 59.2840 | Value loss: 40.5374 | Reward loss: 33.9005 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 192...\n",
            "Total reward: -223.03 | Steps: 114 | Time: 1.74s\n",
            "\n",
            "Loss: 59.1319 | Value loss: 40.5249 | Reward loss: 33.7515 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 193...\n",
            "Total reward: -98.40 | Steps: 61 | Time: 0.93s\n",
            "\n",
            "Loss: 59.0851 | Value loss: 40.4998 | Reward loss: 33.7110 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 194...\n",
            "Total reward: -113.77 | Steps: 75 | Time: 1.15s\n",
            "\n",
            "Loss: 59.1734 | Value loss: 40.4989 | Reward loss: 33.7995 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 195...\n",
            "Total reward: -87.18 | Steps: 80 | Time: 1.21s\n",
            "\n",
            "Loss: 59.0734 | Value loss: 40.4934 | Reward loss: 33.7009 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 196...\n",
            "Total reward: -121.38 | Steps: 88 | Time: 1.49s\n",
            "\n",
            "Loss: 59.0418 | Value loss: 40.4841 | Reward loss: 33.6715 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 197...\n",
            "Total reward: -277.23 | Steps: 94 | Time: 1.44s\n",
            "\n",
            "Loss: 59.1113 | Value loss: 40.5214 | Reward loss: 33.7317 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 198...\n",
            "Total reward: -75.97 | Steps: 83 | Time: 1.33s\n",
            "\n",
            "Loss: 59.0688 | Value loss: 40.5311 | Reward loss: 33.6868 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 199...\n",
            "Total reward: -136.07 | Steps: 77 | Time: 1.18s\n",
            "\n",
            "Loss: 59.0435 | Value loss: 40.4752 | Reward loss: 33.6755 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Testing episode 200...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_200_test_0.mp4\n",
            "Total reward: -136.85\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_200_test_1.mp4\n",
            "Total reward: -98.08\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_200_test_2.mp4\n",
            "Total reward: -93.11\n",
            "\n",
            "\n",
            "Training episode 200...\n",
            "Total reward: -40.69 | Steps: 91 | Time: 1.44s\n",
            "\n",
            "Loss: 59.0636 | Value loss: 40.4802 | Reward loss: 33.6943 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 201...\n",
            "Total reward: -67.25 | Steps: 58 | Time: 0.90s\n",
            "\n",
            "Loss: 59.0602 | Value loss: 40.4817 | Reward loss: 33.6905 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 202...\n",
            "Total reward: -94.19 | Steps: 94 | Time: 1.48s\n",
            "\n",
            "Loss: 59.2909 | Value loss: 40.4937 | Reward loss: 33.9183 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 203...\n",
            "Total reward: -103.84 | Steps: 78 | Time: 1.23s\n",
            "\n",
            "Loss: 59.1326 | Value loss: 40.5173 | Reward loss: 33.7540 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 204...\n",
            "Total reward: -91.55 | Steps: 66 | Time: 1.02s\n",
            "\n",
            "Loss: 58.9521 | Value loss: 40.4874 | Reward loss: 33.5811 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 205...\n",
            "Total reward: -151.78 | Steps: 92 | Time: 1.46s\n",
            "\n",
            "Loss: 59.0544 | Value loss: 40.5012 | Reward loss: 33.6799 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 206...\n",
            "Total reward: -297.69 | Steps: 87 | Time: 1.33s\n",
            "\n",
            "Loss: 59.0392 | Value loss: 40.4851 | Reward loss: 33.6688 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 207...\n",
            "Total reward: -222.08 | Steps: 96 | Time: 1.46s\n",
            "\n",
            "Loss: 59.0668 | Value loss: 40.4868 | Reward loss: 33.6960 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 208...\n",
            "Total reward: -83.84 | Steps: 69 | Time: 1.05s\n",
            "\n",
            "Loss: 58.9852 | Value loss: 40.4969 | Reward loss: 33.6119 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 209...\n",
            "Total reward: -110.72 | Steps: 130 | Time: 1.98s\n",
            "\n",
            "Loss: 58.9971 | Value loss: 40.5376 | Reward loss: 33.6135 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 210...\n",
            "Total reward: -58.14 | Steps: 65 | Time: 1.02s\n",
            "\n",
            "Loss: 58.9810 | Value loss: 40.4657 | Reward loss: 33.6154 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 211...\n",
            "Total reward: -9.90 | Steps: 70 | Time: 1.07s\n",
            "\n",
            "Loss: 59.1777 | Value loss: 40.4976 | Reward loss: 33.8041 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 212...\n",
            "Total reward: -86.83 | Steps: 68 | Time: 1.05s\n",
            "\n",
            "Loss: 59.0526 | Value loss: 40.4816 | Reward loss: 33.6829 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 213...\n",
            "Total reward: -180.98 | Steps: 90 | Time: 1.36s\n",
            "\n",
            "Loss: 58.9356 | Value loss: 40.4481 | Reward loss: 33.5744 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 214...\n",
            "Total reward: -87.24 | Steps: 81 | Time: 1.24s\n",
            "\n",
            "Loss: 59.0651 | Value loss: 40.4879 | Reward loss: 33.6939 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 215...\n",
            "Total reward: -171.79 | Steps: 77 | Time: 1.20s\n",
            "\n",
            "Loss: 58.9476 | Value loss: 40.4721 | Reward loss: 33.5803 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 216...\n",
            "Total reward: -115.77 | Steps: 85 | Time: 1.31s\n",
            "\n",
            "Loss: 58.9913 | Value loss: 40.5044 | Reward loss: 33.6160 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 217...\n",
            "Total reward: -92.29 | Steps: 89 | Time: 1.37s\n",
            "\n",
            "Loss: 58.9423 | Value loss: 40.4717 | Reward loss: 33.5752 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 218...\n",
            "Total reward: -214.61 | Steps: 71 | Time: 1.08s\n",
            "\n",
            "Loss: 58.9524 | Value loss: 40.4554 | Reward loss: 33.5893 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 219...\n",
            "Total reward: -212.89 | Steps: 130 | Time: 1.96s\n",
            "\n",
            "Loss: 58.9578 | Value loss: 40.4585 | Reward loss: 33.5939 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Testing episode 220...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_220_test_0.mp4\n",
            "Total reward: -67.23\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_220_test_1.mp4\n",
            "Total reward: -291.58\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_220_test_2.mp4\n",
            "Total reward: -300.97\n",
            "\n",
            "\n",
            "Training episode 220...\n",
            "Total reward: -312.75 | Steps: 116 | Time: 1.81s\n",
            "\n",
            "Loss: 59.1079 | Value loss: 40.5052 | Reward loss: 33.7324 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 221...\n",
            "Total reward: -75.58 | Steps: 80 | Time: 1.22s\n",
            "\n",
            "Loss: 58.9691 | Value loss: 40.4516 | Reward loss: 33.6070 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 222...\n",
            "Total reward: -108.27 | Steps: 58 | Time: 0.89s\n",
            "\n",
            "Loss: 58.9033 | Value loss: 40.4414 | Reward loss: 33.5438 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 223...\n",
            "Total reward: -116.28 | Steps: 125 | Time: 1.91s\n",
            "\n",
            "Loss: 59.2256 | Value loss: 40.5241 | Reward loss: 33.8454 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 224...\n",
            "Total reward: -49.84 | Steps: 103 | Time: 1.57s\n",
            "\n",
            "Loss: 59.0320 | Value loss: 40.5191 | Reward loss: 33.6530 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 225...\n",
            "Total reward: -405.71 | Steps: 127 | Time: 1.94s\n",
            "\n",
            "Loss: 59.0697 | Value loss: 40.5389 | Reward loss: 33.6857 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 226...\n",
            "Total reward: -131.51 | Steps: 63 | Time: 0.96s\n",
            "\n",
            "Loss: 59.0256 | Value loss: 40.5128 | Reward loss: 33.6480 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 227...\n",
            "Total reward: -190.00 | Steps: 90 | Time: 1.38s\n",
            "\n",
            "Loss: 58.9984 | Value loss: 40.4537 | Reward loss: 33.6357 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 228...\n",
            "Total reward: -153.16 | Steps: 64 | Time: 0.98s\n",
            "\n",
            "Loss: 59.1325 | Value loss: 40.5084 | Reward loss: 33.7563 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 229...\n",
            "Total reward: -82.94 | Steps: 79 | Time: 1.24s\n",
            "\n",
            "Loss: 58.9890 | Value loss: 40.5008 | Reward loss: 33.6146 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 230...\n",
            "Total reward: -186.44 | Steps: 89 | Time: 1.35s\n",
            "\n",
            "Loss: 59.0254 | Value loss: 40.5213 | Reward loss: 33.6458 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 231...\n",
            "Total reward: -283.58 | Steps: 97 | Time: 1.47s\n",
            "\n",
            "Loss: 59.0521 | Value loss: 40.5186 | Reward loss: 33.6732 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 232...\n",
            "Total reward: -80.14 | Steps: 87 | Time: 1.33s\n",
            "\n",
            "Loss: 59.1601 | Value loss: 40.5170 | Reward loss: 33.7815 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 233...\n",
            "Total reward: -90.77 | Steps: 91 | Time: 1.39s\n",
            "\n",
            "Loss: 59.0211 | Value loss: 40.5397 | Reward loss: 33.6370 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 234...\n",
            "Total reward: -120.61 | Steps: 129 | Time: 2.02s\n",
            "\n",
            "Loss: 59.1594 | Value loss: 40.5070 | Reward loss: 33.7834 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 235...\n",
            "Total reward: -97.46 | Steps: 91 | Time: 1.41s\n",
            "\n",
            "Loss: 59.1127 | Value loss: 40.5313 | Reward loss: 33.7307 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 236...\n",
            "Total reward: 13.19 | Steps: 160 | Time: 2.42s\n",
            "\n",
            "Loss: 59.0513 | Value loss: 40.5444 | Reward loss: 33.6660 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 237...\n",
            "Total reward: -52.17 | Steps: 126 | Time: 1.97s\n",
            "\n",
            "Loss: 59.2131 | Value loss: 40.5701 | Reward loss: 33.8212 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 238...\n",
            "Total reward: -429.75 | Steps: 110 | Time: 1.68s\n",
            "\n",
            "Loss: 59.0410 | Value loss: 40.5175 | Reward loss: 33.6623 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 239...\n",
            "Total reward: -112.30 | Steps: 74 | Time: 1.15s\n",
            "\n",
            "Loss: 58.9502 | Value loss: 40.5032 | Reward loss: 33.5752 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Testing episode 240...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_240_test_0.mp4\n",
            "Total reward: -236.21\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_240_test_1.mp4\n",
            "Total reward: -368.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_240_test_2.mp4\n",
            "Total reward: -362.04\n",
            "\n",
            "\n",
            "Training episode 240...\n",
            "Total reward: -79.38 | Steps: 94 | Time: 1.44s\n",
            "\n",
            "Loss: 59.0900 | Value loss: 40.4827 | Reward loss: 33.7201 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 241...\n",
            "Total reward: -227.47 | Steps: 113 | Time: 1.80s\n",
            "\n",
            "Loss: 59.1035 | Value loss: 40.5017 | Reward loss: 33.7288 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 242...\n",
            "Total reward: -134.91 | Steps: 105 | Time: 1.60s\n",
            "\n",
            "Loss: 58.8390 | Value loss: 40.5051 | Reward loss: 33.4635 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 243...\n",
            "Total reward: -114.70 | Steps: 94 | Time: 1.51s\n",
            "\n",
            "Loss: 59.1899 | Value loss: 40.5246 | Reward loss: 33.8095 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 244...\n",
            "Total reward: -336.69 | Steps: 89 | Time: 1.35s\n",
            "\n",
            "Loss: 58.9298 | Value loss: 40.5041 | Reward loss: 33.5545 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 245...\n",
            "Total reward: -21.06 | Steps: 111 | Time: 1.68s\n",
            "\n",
            "Loss: 59.0407 | Value loss: 40.5064 | Reward loss: 33.6648 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 246...\n",
            "Total reward: -321.67 | Steps: 106 | Time: 1.60s\n",
            "\n",
            "Loss: 59.1290 | Value loss: 40.5592 | Reward loss: 33.7399 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 247...\n",
            "Total reward: -97.25 | Steps: 74 | Time: 1.12s\n",
            "\n",
            "Loss: 58.9339 | Value loss: 40.5281 | Reward loss: 33.5527 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 248...\n",
            "Total reward: -76.05 | Steps: 99 | Time: 1.49s\n",
            "\n",
            "Loss: 59.0294 | Value loss: 40.5159 | Reward loss: 33.6511 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 249...\n",
            "Total reward: -196.77 | Steps: 65 | Time: 1.05s\n",
            "\n",
            "Loss: 58.9228 | Value loss: 40.4902 | Reward loss: 33.5511 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 250...\n",
            "Total reward: -161.77 | Steps: 63 | Time: 0.96s\n",
            "\n",
            "Loss: 59.3038 | Value loss: 40.5501 | Reward loss: 33.9171 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 251...\n",
            "Total reward: -125.48 | Steps: 109 | Time: 1.65s\n",
            "\n",
            "Loss: 58.9847 | Value loss: 40.5239 | Reward loss: 33.6045 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 252...\n",
            "Total reward: -84.55 | Steps: 125 | Time: 1.89s\n",
            "\n",
            "Loss: 59.0282 | Value loss: 40.5164 | Reward loss: 33.6499 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 253...\n",
            "Total reward: -155.47 | Steps: 93 | Time: 1.45s\n",
            "\n",
            "Loss: 59.1676 | Value loss: 40.5334 | Reward loss: 33.7851 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 254...\n",
            "Total reward: -346.91 | Steps: 111 | Time: 1.73s\n",
            "\n",
            "Loss: 58.8872 | Value loss: 40.4973 | Reward loss: 33.5135 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 255...\n",
            "Total reward: -120.14 | Steps: 102 | Time: 1.54s\n",
            "\n",
            "Loss: 58.9754 | Value loss: 40.5019 | Reward loss: 33.6007 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 256...\n",
            "Total reward: -40.34 | Steps: 65 | Time: 0.99s\n",
            "\n",
            "Loss: 58.9417 | Value loss: 40.4812 | Reward loss: 33.5721 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 257...\n",
            "Total reward: -283.47 | Steps: 85 | Time: 1.30s\n",
            "\n",
            "Loss: 58.9235 | Value loss: 40.4820 | Reward loss: 33.5537 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 258...\n",
            "Total reward: -101.90 | Steps: 73 | Time: 1.12s\n",
            "\n",
            "Loss: 59.0028 | Value loss: 40.5140 | Reward loss: 33.6250 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 259...\n",
            "Total reward: -154.72 | Steps: 92 | Time: 1.41s\n",
            "\n",
            "Loss: 58.9019 | Value loss: 40.4934 | Reward loss: 33.5293 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Testing episode 260...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_260_test_0.mp4\n",
            "Total reward: -174.64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_260_test_1.mp4\n",
            "Total reward: -22.34\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_260_test_2.mp4\n",
            "Total reward: -31.38\n",
            "\n",
            "\n",
            "Training episode 260...\n",
            "Total reward: -102.31 | Steps: 62 | Time: 0.96s\n",
            "\n",
            "Loss: 58.9472 | Value loss: 40.4837 | Reward loss: 33.5770 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 261...\n",
            "Total reward: -87.23 | Steps: 89 | Time: 1.36s\n",
            "\n",
            "Loss: 59.0086 | Value loss: 40.4974 | Reward loss: 33.6350 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 262...\n",
            "Total reward: -183.82 | Steps: 76 | Time: 1.16s\n",
            "\n",
            "Loss: 58.8716 | Value loss: 40.4717 | Reward loss: 33.5044 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 263...\n",
            "Total reward: -117.91 | Steps: 58 | Time: 0.90s\n",
            "\n",
            "Loss: 58.9338 | Value loss: 40.4996 | Reward loss: 33.5596 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 264...\n",
            "Total reward: -132.48 | Steps: 100 | Time: 1.58s\n",
            "\n",
            "Loss: 59.0807 | Value loss: 40.5230 | Reward loss: 33.7007 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 265...\n",
            "Total reward: -124.64 | Steps: 111 | Time: 1.70s\n",
            "\n",
            "Loss: 58.9848 | Value loss: 40.4724 | Reward loss: 33.6175 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 266...\n",
            "Total reward: -119.71 | Steps: 99 | Time: 1.51s\n",
            "\n",
            "Loss: 59.0142 | Value loss: 40.4799 | Reward loss: 33.6451 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 267...\n",
            "Total reward: -173.52 | Steps: 102 | Time: 1.55s\n",
            "\n",
            "Loss: 59.1318 | Value loss: 40.4906 | Reward loss: 33.7599 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 268...\n",
            "Total reward: -120.36 | Steps: 90 | Time: 1.37s\n",
            "\n",
            "Loss: 58.8873 | Value loss: 40.4892 | Reward loss: 33.5158 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 269...\n",
            "Total reward: -270.90 | Steps: 97 | Time: 1.51s\n",
            "\n",
            "Loss: 59.0028 | Value loss: 40.4786 | Reward loss: 33.6339 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 270...\n",
            "Total reward: -265.22 | Steps: 102 | Time: 1.58s\n",
            "\n",
            "Loss: 59.0291 | Value loss: 40.4670 | Reward loss: 33.6632 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 271...\n",
            "Total reward: -202.50 | Steps: 92 | Time: 1.41s\n",
            "\n",
            "Loss: 59.1561 | Value loss: 40.4809 | Reward loss: 33.7867 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 272...\n",
            "Total reward: -484.72 | Steps: 106 | Time: 1.65s\n",
            "\n",
            "Loss: 58.8966 | Value loss: 40.4489 | Reward loss: 33.5352 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 273...\n",
            "Total reward: -306.80 | Steps: 85 | Time: 1.30s\n",
            "\n",
            "Loss: 58.9722 | Value loss: 40.4934 | Reward loss: 33.5995 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 274...\n",
            "Total reward: -108.12 | Steps: 75 | Time: 1.15s\n",
            "\n",
            "Loss: 58.9234 | Value loss: 40.4819 | Reward loss: 33.5537 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 275...\n",
            "Total reward: -121.43 | Steps: 68 | Time: 1.04s\n",
            "\n",
            "Loss: 58.8764 | Value loss: 40.4661 | Reward loss: 33.5106 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 276...\n",
            "Total reward: -87.91 | Steps: 65 | Time: 1.00s\n",
            "\n",
            "Loss: 59.1465 | Value loss: 40.5081 | Reward loss: 33.7703 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 277...\n",
            "Total reward: -86.70 | Steps: 59 | Time: 0.91s\n",
            "\n",
            "Loss: 58.9664 | Value loss: 40.4572 | Reward loss: 33.6028 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 278...\n",
            "Total reward: -201.41 | Steps: 94 | Time: 1.44s\n",
            "\n",
            "Loss: 59.0294 | Value loss: 40.4709 | Reward loss: 33.6624 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 279...\n",
            "Total reward: -372.14 | Steps: 113 | Time: 1.72s\n",
            "\n",
            "Loss: 59.0893 | Value loss: 40.5106 | Reward loss: 33.7124 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Testing episode 280...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_280_test_0.mp4\n",
            "Total reward: -537.52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_280_test_1.mp4\n",
            "Total reward: -416.99\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_280_test_2.mp4\n",
            "Total reward: -255.74\n",
            "\n",
            "\n",
            "Training episode 280...\n",
            "Total reward: -213.79 | Steps: 106 | Time: 1.66s\n",
            "\n",
            "Loss: 58.8886 | Value loss: 40.4835 | Reward loss: 33.5185 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 281...\n",
            "Total reward: -113.62 | Steps: 64 | Time: 1.16s\n",
            "\n",
            "Loss: 59.0747 | Value loss: 40.5277 | Reward loss: 33.6936 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 282...\n",
            "Total reward: -138.23 | Steps: 119 | Time: 1.81s\n",
            "\n",
            "Loss: 58.9561 | Value loss: 40.4993 | Reward loss: 33.5820 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 283...\n",
            "Total reward: -196.57 | Steps: 82 | Time: 1.29s\n",
            "\n",
            "Loss: 59.1226 | Value loss: 40.5355 | Reward loss: 33.7395 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 284...\n",
            "Total reward: -112.45 | Steps: 62 | Time: 0.96s\n",
            "\n",
            "Loss: 58.9808 | Value loss: 40.5306 | Reward loss: 33.5989 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 285...\n",
            "Total reward: -153.70 | Steps: 87 | Time: 1.34s\n",
            "\n",
            "Loss: 59.0494 | Value loss: 40.4822 | Reward loss: 33.6796 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 286...\n",
            "Total reward: -89.55 | Steps: 79 | Time: 1.22s\n",
            "\n",
            "Loss: 58.9433 | Value loss: 40.4998 | Reward loss: 33.5691 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 287...\n",
            "Total reward: -213.05 | Steps: 75 | Time: 1.16s\n",
            "\n",
            "Loss: 58.8871 | Value loss: 40.4911 | Reward loss: 33.5151 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 288...\n",
            "Total reward: -82.44 | Steps: 69 | Time: 1.12s\n",
            "\n",
            "Loss: 59.0763 | Value loss: 40.4874 | Reward loss: 33.7052 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 289...\n",
            "Total reward: -175.10 | Steps: 71 | Time: 1.10s\n",
            "\n",
            "Loss: 58.8895 | Value loss: 40.4803 | Reward loss: 33.5202 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 290...\n",
            "Total reward: -241.04 | Steps: 91 | Time: 1.39s\n",
            "\n",
            "Loss: 59.0751 | Value loss: 40.5270 | Reward loss: 33.6941 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 291...\n",
            "Total reward: -245.20 | Steps: 93 | Time: 1.44s\n",
            "\n",
            "Loss: 58.9674 | Value loss: 40.5034 | Reward loss: 33.5923 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 292...\n",
            "Total reward: -127.59 | Steps: 86 | Time: 1.32s\n",
            "\n",
            "Loss: 58.9601 | Value loss: 40.4983 | Reward loss: 33.5863 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 293...\n",
            "Total reward: -106.60 | Steps: 65 | Time: 1.00s\n",
            "\n",
            "Loss: 58.8961 | Value loss: 40.4695 | Reward loss: 33.5295 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 294...\n",
            "Total reward: -126.73 | Steps: 88 | Time: 1.35s\n",
            "\n",
            "Loss: 58.8167 | Value loss: 40.4518 | Reward loss: 33.4545 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 295...\n",
            "Total reward: -332.79 | Steps: 89 | Time: 1.37s\n",
            "\n",
            "Loss: 58.9610 | Value loss: 40.4909 | Reward loss: 33.5890 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 296...\n",
            "Total reward: -167.43 | Steps: 109 | Time: 1.66s\n",
            "\n",
            "Loss: 59.0297 | Value loss: 40.5052 | Reward loss: 33.6541 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 297...\n",
            "Total reward: -84.45 | Steps: 59 | Time: 1.10s\n",
            "\n",
            "Loss: 58.9497 | Value loss: 40.5028 | Reward loss: 33.5748 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 298...\n",
            "Total reward: -143.47 | Steps: 114 | Time: 1.89s\n",
            "\n",
            "Loss: 58.9144 | Value loss: 40.4997 | Reward loss: 33.5402 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 299...\n",
            "Total reward: -143.14 | Steps: 74 | Time: 1.26s\n",
            "\n",
            "Loss: 59.0522 | Value loss: 40.5251 | Reward loss: 33.6717 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Testing episode 300...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_300_test_0.mp4\n",
            "Total reward: -60.33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_300_test_1.mp4\n",
            "Total reward: -175.52\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_300_test_2.mp4\n",
            "Total reward: -64.49\n",
            "\n",
            "\n",
            "Training episode 300...\n",
            "Total reward: -252.97 | Steps: 116 | Time: 1.78s\n",
            "\n",
            "Loss: 58.8672 | Value loss: 40.4710 | Reward loss: 33.5002 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 301...\n",
            "Total reward: -224.67 | Steps: 81 | Time: 1.23s\n",
            "\n",
            "Loss: 58.8288 | Value loss: 40.4318 | Reward loss: 33.4717 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 302...\n",
            "Total reward: -88.59 | Steps: 78 | Time: 1.23s\n",
            "\n",
            "Loss: 59.0735 | Value loss: 40.5176 | Reward loss: 33.6950 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 303...\n",
            "Total reward: -232.59 | Steps: 113 | Time: 1.72s\n",
            "\n",
            "Loss: 58.9194 | Value loss: 40.4727 | Reward loss: 33.5520 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 304...\n",
            "Total reward: -320.68 | Steps: 95 | Time: 1.45s\n",
            "\n",
            "Loss: 58.9913 | Value loss: 40.4756 | Reward loss: 33.6231 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 305...\n",
            "Total reward: 109.77 | Steps: 400 | Time: 6.06s\n",
            "\n",
            "Loss: 58.9183 | Value loss: 40.4472 | Reward loss: 33.5572 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 306...\n",
            "Total reward: -102.41 | Steps: 62 | Time: 0.97s\n",
            "\n",
            "Loss: 58.9760 | Value loss: 40.4646 | Reward loss: 33.6106 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 307...\n",
            "Total reward: -155.91 | Steps: 79 | Time: 1.21s\n",
            "\n",
            "Loss: 59.0468 | Value loss: 40.5182 | Reward loss: 33.6680 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 308...\n",
            "Total reward: -98.72 | Steps: 56 | Time: 0.91s\n",
            "\n",
            "Loss: 59.0567 | Value loss: 40.4925 | Reward loss: 33.6843 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 309...\n",
            "Total reward: -121.28 | Steps: 121 | Time: 1.86s\n",
            "\n",
            "Loss: 58.9875 | Value loss: 40.4938 | Reward loss: 33.6148 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 310...\n",
            "Total reward: -146.31 | Steps: 81 | Time: 1.24s\n",
            "\n",
            "Loss: 59.0365 | Value loss: 40.5219 | Reward loss: 33.6567 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 311...\n",
            "Total reward: -78.49 | Steps: 68 | Time: 1.04s\n",
            "\n",
            "Loss: 58.9768 | Value loss: 40.5069 | Reward loss: 33.6008 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 312...\n",
            "Total reward: -327.13 | Steps: 113 | Time: 1.77s\n",
            "\n",
            "Loss: 59.0083 | Value loss: 40.4973 | Reward loss: 33.6345 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 313...\n",
            "Total reward: -89.19 | Steps: 77 | Time: 1.18s\n",
            "\n",
            "Loss: 58.9338 | Value loss: 40.5234 | Reward loss: 33.5536 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 314...\n",
            "Total reward: -129.72 | Steps: 104 | Time: 1.60s\n",
            "\n",
            "Loss: 59.0599 | Value loss: 40.5148 | Reward loss: 33.6820 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 315...\n",
            "Total reward: -103.40 | Steps: 73 | Time: 1.11s\n",
            "\n",
            "Loss: 59.0839 | Value loss: 40.5267 | Reward loss: 33.7030 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 316...\n",
            "Total reward: -146.22 | Steps: 79 | Time: 1.20s\n",
            "\n",
            "Loss: 58.9235 | Value loss: 40.5384 | Reward loss: 33.5397 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 317...\n",
            "Total reward: -99.88 | Steps: 64 | Time: 1.06s\n",
            "\n",
            "Loss: 59.0230 | Value loss: 40.4900 | Reward loss: 33.6514 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 318...\n",
            "Total reward: -348.49 | Steps: 83 | Time: 1.27s\n",
            "\n",
            "Loss: 59.1800 | Value loss: 40.5586 | Reward loss: 33.7909 | Policy loss: 15.2495\n",
            "\n",
            "\n",
            "Training episode 319...\n",
            "Total reward: -298.59 | Steps: 104 | Time: 1.58s\n",
            "\n",
            "Loss: 59.1383 | Value loss: 40.5260 | Reward loss: 33.7575 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Testing episode 320...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_320_test_0.mp4\n",
            "Total reward: -363.01\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_320_test_1.mp4\n",
            "Total reward: -181.82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_320_test_2.mp4\n",
            "Total reward: -262.62\n",
            "\n",
            "\n",
            "Training episode 320...\n",
            "Total reward: -67.65 | Steps: 81 | Time: 1.31s\n",
            "\n",
            "Loss: 58.9488 | Value loss: 40.5099 | Reward loss: 33.5721 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 321...\n",
            "Total reward: -92.60 | Steps: 61 | Time: 0.97s\n",
            "\n",
            "Loss: 58.9384 | Value loss: 40.4949 | Reward loss: 33.5654 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 322...\n",
            "Total reward: -98.08 | Steps: 85 | Time: 1.34s\n",
            "\n",
            "Loss: 59.0166 | Value loss: 40.5108 | Reward loss: 33.6396 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 323...\n",
            "Total reward: -83.93 | Steps: 116 | Time: 1.83s\n",
            "\n",
            "Loss: 59.0276 | Value loss: 40.5362 | Reward loss: 33.6444 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 324...\n",
            "Total reward: -310.78 | Steps: 95 | Time: 1.55s\n",
            "\n",
            "Loss: 58.9314 | Value loss: 40.4764 | Reward loss: 33.5631 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 325...\n",
            "Total reward: -85.45 | Steps: 127 | Time: 2.80s\n",
            "\n",
            "Loss: 59.0438 | Value loss: 40.5426 | Reward loss: 33.6588 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 326...\n",
            "Total reward: -82.62 | Steps: 56 | Time: 0.87s\n",
            "\n",
            "Loss: 59.0333 | Value loss: 40.4805 | Reward loss: 33.6639 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 327...\n",
            "Total reward: -145.70 | Steps: 66 | Time: 1.06s\n",
            "\n",
            "Loss: 59.0314 | Value loss: 40.4827 | Reward loss: 33.6614 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 328...\n",
            "Total reward: -90.87 | Steps: 125 | Time: 1.96s\n",
            "\n",
            "Loss: 59.0762 | Value loss: 40.5347 | Reward loss: 33.6933 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 329...\n",
            "Total reward: -401.05 | Steps: 89 | Time: 1.41s\n",
            "\n",
            "Loss: 59.0193 | Value loss: 40.5074 | Reward loss: 33.6432 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 330...\n",
            "Total reward: -335.97 | Steps: 110 | Time: 1.75s\n",
            "\n",
            "Loss: 58.9464 | Value loss: 40.5152 | Reward loss: 33.5683 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 331...\n",
            "Total reward: -286.03 | Steps: 113 | Time: 1.78s\n",
            "\n",
            "Loss: 59.0656 | Value loss: 40.4956 | Reward loss: 33.6925 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 332...\n",
            "Total reward: -182.18 | Steps: 89 | Time: 1.38s\n",
            "\n",
            "Loss: 59.0416 | Value loss: 40.5365 | Reward loss: 33.6583 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 333...\n",
            "Total reward: -191.74 | Steps: 115 | Time: 1.82s\n",
            "\n",
            "Loss: 58.8933 | Value loss: 40.5360 | Reward loss: 33.5099 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 334...\n",
            "Total reward: -131.84 | Steps: 95 | Time: 2.02s\n",
            "\n",
            "Loss: 58.9841 | Value loss: 40.4855 | Reward loss: 33.6136 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 335...\n",
            "Total reward: -117.59 | Steps: 77 | Time: 1.36s\n",
            "\n",
            "Loss: 58.9252 | Value loss: 40.4685 | Reward loss: 33.5587 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 336...\n",
            "Total reward: -105.94 | Steps: 126 | Time: 1.93s\n",
            "\n",
            "Loss: 58.8540 | Value loss: 40.4344 | Reward loss: 33.4962 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 337...\n",
            "Total reward: -113.58 | Steps: 97 | Time: 1.58s\n",
            "\n",
            "Loss: 59.0307 | Value loss: 40.4909 | Reward loss: 33.6586 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 338...\n",
            "Total reward: -152.02 | Steps: 94 | Time: 1.52s\n",
            "\n",
            "Loss: 59.0322 | Value loss: 40.4770 | Reward loss: 33.6638 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 339...\n",
            "Total reward: -155.91 | Steps: 85 | Time: 1.30s\n",
            "\n",
            "Loss: 59.0076 | Value loss: 40.4817 | Reward loss: 33.6378 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Testing episode 340...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_340_test_0.mp4\n",
            "Total reward: -337.18\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_340_test_1.mp4\n",
            "Total reward: -47.79\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_340_test_2.mp4\n",
            "Total reward: -385.72\n",
            "\n",
            "\n",
            "Training episode 340...\n",
            "Total reward: -86.62 | Steps: 72 | Time: 1.14s\n",
            "\n",
            "Loss: 58.9369 | Value loss: 40.4386 | Reward loss: 33.5780 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 341...\n",
            "Total reward: -125.11 | Steps: 88 | Time: 1.36s\n",
            "\n",
            "Loss: 58.9368 | Value loss: 40.4692 | Reward loss: 33.5702 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 342...\n",
            "Total reward: -327.13 | Steps: 116 | Time: 1.84s\n",
            "\n",
            "Loss: 58.8965 | Value loss: 40.4679 | Reward loss: 33.5303 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 343...\n",
            "Total reward: -156.92 | Steps: 104 | Time: 1.67s\n",
            "\n",
            "Loss: 58.8862 | Value loss: 40.4679 | Reward loss: 33.5198 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 344...\n",
            "Total reward: -132.48 | Steps: 92 | Time: 1.90s\n",
            "\n",
            "Loss: 59.0802 | Value loss: 40.4577 | Reward loss: 33.7164 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 345...\n",
            "Total reward: -68.32 | Steps: 71 | Time: 1.27s\n",
            "\n",
            "Loss: 58.9342 | Value loss: 40.4422 | Reward loss: 33.5745 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 346...\n",
            "Total reward: -123.83 | Steps: 166 | Time: 2.57s\n",
            "\n",
            "Loss: 59.1269 | Value loss: 40.5398 | Reward loss: 33.7429 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 347...\n",
            "Total reward: -151.32 | Steps: 88 | Time: 1.35s\n",
            "\n",
            "Loss: 58.9022 | Value loss: 40.4487 | Reward loss: 33.5407 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 348...\n",
            "Total reward: -102.02 | Steps: 95 | Time: 1.49s\n",
            "\n",
            "Loss: 58.9135 | Value loss: 40.4298 | Reward loss: 33.5568 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 349...\n",
            "Total reward: -109.03 | Steps: 67 | Time: 1.06s\n",
            "\n",
            "Loss: 59.0755 | Value loss: 40.5046 | Reward loss: 33.7001 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 350...\n",
            "Total reward: -127.79 | Steps: 87 | Time: 1.54s\n",
            "\n",
            "Loss: 58.8279 | Value loss: 40.4274 | Reward loss: 33.4718 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 351...\n",
            "Total reward: -109.97 | Steps: 66 | Time: 1.01s\n",
            "\n",
            "Loss: 58.9420 | Value loss: 40.4446 | Reward loss: 33.5814 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 352...\n",
            "Total reward: -103.80 | Steps: 121 | Time: 1.84s\n",
            "\n",
            "Loss: 58.9727 | Value loss: 40.4389 | Reward loss: 33.6137 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 353...\n",
            "Total reward: -8.25 | Steps: 138 | Time: 2.10s\n",
            "\n",
            "Loss: 58.8884 | Value loss: 40.4553 | Reward loss: 33.5253 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 354...\n",
            "Total reward: -140.67 | Steps: 79 | Time: 1.20s\n",
            "\n",
            "Loss: 58.8925 | Value loss: 40.4446 | Reward loss: 33.5321 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 355...\n",
            "Total reward: -311.92 | Steps: 116 | Time: 1.76s\n",
            "\n",
            "Loss: 59.0101 | Value loss: 40.4590 | Reward loss: 33.6460 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 356...\n",
            "Total reward: -102.80 | Steps: 117 | Time: 1.77s\n",
            "\n",
            "Loss: 58.9746 | Value loss: 40.4863 | Reward loss: 33.6037 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 357...\n",
            "Total reward: -59.71 | Steps: 80 | Time: 1.27s\n",
            "\n",
            "Loss: 58.7999 | Value loss: 40.4732 | Reward loss: 33.4324 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 358...\n",
            "Total reward: -376.77 | Steps: 121 | Time: 1.82s\n",
            "\n",
            "Loss: 58.8322 | Value loss: 40.4759 | Reward loss: 33.4639 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 359...\n",
            "Total reward: -134.89 | Steps: 81 | Time: 1.24s\n",
            "\n",
            "Loss: 58.7549 | Value loss: 40.4206 | Reward loss: 33.4005 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Testing episode 360...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_360_test_0.mp4\n",
            "Total reward: -69.35\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_360_test_1.mp4\n",
            "Total reward: -461.84\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_360_test_2.mp4\n",
            "Total reward: -45.22\n",
            "\n",
            "\n",
            "Training episode 360...\n",
            "Total reward: -111.00 | Steps: 94 | Time: 1.44s\n",
            "\n",
            "Loss: 59.0295 | Value loss: 40.4935 | Reward loss: 33.6569 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 361...\n",
            "Total reward: -363.90 | Steps: 115 | Time: 1.74s\n",
            "\n",
            "Loss: 59.0768 | Value loss: 40.4966 | Reward loss: 33.7034 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 362...\n",
            "Total reward: -294.08 | Steps: 96 | Time: 1.52s\n",
            "\n",
            "Loss: 58.8976 | Value loss: 40.4856 | Reward loss: 33.5268 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 363...\n",
            "Total reward: -126.92 | Steps: 73 | Time: 1.12s\n",
            "\n",
            "Loss: 58.9950 | Value loss: 40.4894 | Reward loss: 33.6234 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 364...\n",
            "Total reward: -160.48 | Steps: 71 | Time: 1.10s\n",
            "\n",
            "Loss: 58.8237 | Value loss: 40.4581 | Reward loss: 33.4599 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 365...\n",
            "Total reward: -99.63 | Steps: 68 | Time: 1.04s\n",
            "\n",
            "Loss: 58.9896 | Value loss: 40.4681 | Reward loss: 33.6234 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 366...\n",
            "Total reward: -83.72 | Steps: 61 | Time: 0.95s\n",
            "\n",
            "Loss: 58.9213 | Value loss: 40.4674 | Reward loss: 33.5552 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 367...\n",
            "Total reward: -107.80 | Steps: 78 | Time: 1.18s\n",
            "\n",
            "Loss: 58.9659 | Value loss: 40.5022 | Reward loss: 33.5911 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 368...\n",
            "Total reward: -117.37 | Steps: 72 | Time: 1.09s\n",
            "\n",
            "Loss: 59.0029 | Value loss: 40.5394 | Reward loss: 33.6188 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 369...\n",
            "Total reward: -211.16 | Steps: 101 | Time: 1.54s\n",
            "\n",
            "Loss: 58.9555 | Value loss: 40.4949 | Reward loss: 33.5824 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 370...\n",
            "Total reward: -327.83 | Steps: 125 | Time: 1.89s\n",
            "\n",
            "Loss: 58.8881 | Value loss: 40.4745 | Reward loss: 33.5203 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 371...\n",
            "Total reward: -340.98 | Steps: 111 | Time: 1.73s\n",
            "\n",
            "Loss: 58.9016 | Value loss: 40.5000 | Reward loss: 33.5272 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 372...\n",
            "Total reward: -385.88 | Steps: 106 | Time: 1.61s\n",
            "\n",
            "Loss: 58.8966 | Value loss: 40.4607 | Reward loss: 33.5322 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 373...\n",
            "Total reward: -117.95 | Steps: 108 | Time: 1.63s\n",
            "\n",
            "Loss: 58.9012 | Value loss: 40.4789 | Reward loss: 33.5323 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 374...\n",
            "Total reward: -161.98 | Steps: 105 | Time: 1.63s\n",
            "\n",
            "Loss: 58.8680 | Value loss: 40.4674 | Reward loss: 33.5018 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 375...\n",
            "Total reward: -142.22 | Steps: 104 | Time: 1.57s\n",
            "\n",
            "Loss: 59.0243 | Value loss: 40.4740 | Reward loss: 33.6567 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 376...\n",
            "Total reward: -63.78 | Steps: 105 | Time: 1.60s\n",
            "\n",
            "Loss: 58.9227 | Value loss: 40.4763 | Reward loss: 33.5543 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 377...\n",
            "Total reward: -22.84 | Steps: 91 | Time: 1.39s\n",
            "\n",
            "Loss: 58.8638 | Value loss: 40.4919 | Reward loss: 33.4916 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 378...\n",
            "Total reward: -377.87 | Steps: 118 | Time: 1.83s\n",
            "\n",
            "Loss: 58.8393 | Value loss: 40.4525 | Reward loss: 33.4769 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 379...\n",
            "Total reward: -107.10 | Steps: 88 | Time: 1.34s\n",
            "\n",
            "Loss: 58.9114 | Value loss: 40.4804 | Reward loss: 33.5420 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Testing episode 380...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_380_test_0.mp4\n",
            "Total reward: -464.62\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_380_test_1.mp4\n",
            "Total reward: -307.77\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_380_test_2.mp4\n",
            "Total reward: -489.06\n",
            "\n",
            "\n",
            "Training episode 380...\n",
            "Total reward: -130.17 | Steps: 99 | Time: 1.52s\n",
            "\n",
            "Loss: 58.8854 | Value loss: 40.4969 | Reward loss: 33.5119 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 381...\n",
            "Total reward: -117.84 | Steps: 70 | Time: 1.07s\n",
            "\n",
            "Loss: 58.7895 | Value loss: 40.4347 | Reward loss: 33.4317 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 382...\n",
            "Total reward: 2.95 | Steps: 100 | Time: 1.61s\n",
            "\n",
            "Loss: 58.9760 | Value loss: 40.4669 | Reward loss: 33.6100 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 383...\n",
            "Total reward: -130.52 | Steps: 108 | Time: 1.68s\n",
            "\n",
            "Loss: 58.9336 | Value loss: 40.4728 | Reward loss: 33.5661 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 384...\n",
            "Total reward: -77.93 | Steps: 84 | Time: 1.33s\n",
            "\n",
            "Loss: 58.9328 | Value loss: 40.4343 | Reward loss: 33.5751 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 385...\n",
            "Total reward: -315.48 | Steps: 97 | Time: 1.80s\n",
            "\n",
            "Loss: 58.9499 | Value loss: 40.5080 | Reward loss: 33.5736 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 386...\n",
            "Total reward: -93.08 | Steps: 97 | Time: 1.53s\n",
            "\n",
            "Loss: 58.9665 | Value loss: 40.4420 | Reward loss: 33.6067 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 387...\n",
            "Total reward: -75.39 | Steps: 80 | Time: 1.26s\n",
            "\n",
            "Loss: 58.9574 | Value loss: 40.5212 | Reward loss: 33.5778 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 388...\n",
            "Total reward: -97.89 | Steps: 73 | Time: 1.16s\n",
            "\n",
            "Loss: 58.8657 | Value loss: 40.4792 | Reward loss: 33.4966 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 389...\n",
            "Total reward: -81.99 | Steps: 63 | Time: 0.98s\n",
            "\n",
            "Loss: 58.8820 | Value loss: 40.4731 | Reward loss: 33.5145 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 390...\n",
            "Total reward: -129.28 | Steps: 86 | Time: 1.31s\n",
            "\n",
            "Loss: 58.9465 | Value loss: 40.4759 | Reward loss: 33.5783 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 391...\n",
            "Total reward: -49.11 | Steps: 74 | Time: 1.13s\n",
            "\n",
            "Loss: 59.0162 | Value loss: 40.4875 | Reward loss: 33.6451 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 392...\n",
            "Total reward: -114.43 | Steps: 71 | Time: 1.08s\n",
            "\n",
            "Loss: 58.9714 | Value loss: 40.4557 | Reward loss: 33.6082 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 393...\n",
            "Total reward: -471.87 | Steps: 86 | Time: 1.37s\n",
            "\n",
            "Loss: 58.8455 | Value loss: 40.4749 | Reward loss: 33.4776 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 394...\n",
            "Total reward: -116.91 | Steps: 73 | Time: 1.12s\n",
            "\n",
            "Loss: 58.8032 | Value loss: 40.4249 | Reward loss: 33.4476 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 395...\n",
            "Total reward: -172.94 | Steps: 76 | Time: 1.17s\n",
            "\n",
            "Loss: 58.8377 | Value loss: 40.4893 | Reward loss: 33.4662 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 396...\n",
            "Total reward: -151.73 | Steps: 87 | Time: 1.33s\n",
            "\n",
            "Loss: 58.9693 | Value loss: 40.4586 | Reward loss: 33.6055 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 397...\n",
            "Total reward: -398.80 | Steps: 105 | Time: 1.59s\n",
            "\n",
            "Loss: 58.8224 | Value loss: 40.4616 | Reward loss: 33.4578 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 398...\n",
            "Total reward: -128.19 | Steps: 77 | Time: 1.17s\n",
            "\n",
            "Loss: 58.9241 | Value loss: 40.4480 | Reward loss: 33.5629 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 399...\n",
            "Total reward: -61.34 | Steps: 69 | Time: 1.05s\n",
            "\n",
            "Loss: 59.0661 | Value loss: 40.5137 | Reward loss: 33.6884 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Testing episode 400...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_400_test_0.mp4\n",
            "Total reward: -115.89\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_400_test_1.mp4\n",
            "Total reward: -363.32\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_400_test_2.mp4\n",
            "Total reward: -280.24\n",
            "\n",
            "\n",
            "Training episode 400...\n",
            "Total reward: -131.21 | Steps: 91 | Time: 1.47s\n",
            "\n",
            "Loss: 59.0354 | Value loss: 40.5429 | Reward loss: 33.6504 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 401...\n",
            "Total reward: -243.84 | Steps: 100 | Time: 1.53s\n",
            "\n",
            "Loss: 58.9456 | Value loss: 40.4861 | Reward loss: 33.5748 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 402...\n",
            "Total reward: -288.64 | Steps: 83 | Time: 1.28s\n",
            "\n",
            "Loss: 58.9502 | Value loss: 40.5228 | Reward loss: 33.5703 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 403...\n",
            "Total reward: -13.59 | Steps: 131 | Time: 2.04s\n",
            "\n",
            "Loss: 58.9700 | Value loss: 40.5231 | Reward loss: 33.5900 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 404...\n",
            "Total reward: -97.65 | Steps: 77 | Time: 1.20s\n",
            "\n",
            "Loss: 59.1201 | Value loss: 40.5553 | Reward loss: 33.7320 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 405...\n",
            "Total reward: -109.21 | Steps: 91 | Time: 1.38s\n",
            "\n",
            "Loss: 59.1106 | Value loss: 40.5279 | Reward loss: 33.7295 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 406...\n",
            "Total reward: -115.93 | Steps: 98 | Time: 1.49s\n",
            "\n",
            "Loss: 59.0377 | Value loss: 40.5366 | Reward loss: 33.6544 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 407...\n",
            "Total reward: -96.98 | Steps: 93 | Time: 1.41s\n",
            "\n",
            "Loss: 58.9964 | Value loss: 40.5506 | Reward loss: 33.6094 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 408...\n",
            "Total reward: -136.90 | Steps: 69 | Time: 1.13s\n",
            "\n",
            "Loss: 58.8894 | Value loss: 40.4943 | Reward loss: 33.5165 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 409...\n",
            "Total reward: -389.04 | Steps: 94 | Time: 1.43s\n",
            "\n",
            "Loss: 58.8862 | Value loss: 40.5038 | Reward loss: 33.5109 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 410...\n",
            "Total reward: -259.42 | Steps: 87 | Time: 1.32s\n",
            "\n",
            "Loss: 59.0415 | Value loss: 40.5220 | Reward loss: 33.6618 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 411...\n",
            "Total reward: -200.38 | Steps: 104 | Time: 1.59s\n",
            "\n",
            "Loss: 58.9421 | Value loss: 40.5115 | Reward loss: 33.5649 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 412...\n",
            "Total reward: -182.18 | Steps: 92 | Time: 1.39s\n",
            "\n",
            "Loss: 59.0075 | Value loss: 40.5209 | Reward loss: 33.6279 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 413...\n",
            "Total reward: -374.46 | Steps: 105 | Time: 1.72s\n",
            "\n",
            "Loss: 58.9840 | Value loss: 40.5166 | Reward loss: 33.6056 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 414...\n",
            "Total reward: -313.90 | Steps: 113 | Time: 1.71s\n",
            "\n",
            "Loss: 59.0575 | Value loss: 40.5349 | Reward loss: 33.6745 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 415...\n",
            "Total reward: -166.99 | Steps: 74 | Time: 1.13s\n",
            "\n",
            "Loss: 58.9297 | Value loss: 40.5343 | Reward loss: 33.5469 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 416...\n",
            "Total reward: -65.34 | Steps: 109 | Time: 1.71s\n",
            "\n",
            "Loss: 59.0340 | Value loss: 40.5313 | Reward loss: 33.6518 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 417...\n",
            "Total reward: -261.15 | Steps: 93 | Time: 1.42s\n",
            "\n",
            "Loss: 58.9861 | Value loss: 40.4787 | Reward loss: 33.6171 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 418...\n",
            "Total reward: -52.93 | Steps: 71 | Time: 1.08s\n",
            "\n",
            "Loss: 58.9716 | Value loss: 40.5374 | Reward loss: 33.5881 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 419...\n",
            "Total reward: -402.23 | Steps: 109 | Time: 1.64s\n",
            "\n",
            "Loss: 59.0310 | Value loss: 40.5391 | Reward loss: 33.6469 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Testing episode 420...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_420_test_0.mp4\n",
            "Total reward: -173.61\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_420_test_1.mp4\n",
            "Total reward: -365.93\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_420_test_2.mp4\n",
            "Total reward: -94.28\n",
            "\n",
            "\n",
            "Training episode 420...\n",
            "Total reward: -12.83 | Steps: 144 | Time: 2.20s\n",
            "\n",
            "Loss: 58.8047 | Value loss: 40.5193 | Reward loss: 33.4256 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 421...\n",
            "Total reward: -277.64 | Steps: 90 | Time: 1.40s\n",
            "\n",
            "Loss: 59.0278 | Value loss: 40.5466 | Reward loss: 33.6419 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 422...\n",
            "Total reward: -214.69 | Steps: 119 | Time: 1.86s\n",
            "\n",
            "Loss: 58.9264 | Value loss: 40.5069 | Reward loss: 33.5505 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 423...\n",
            "Total reward: -95.96 | Steps: 93 | Time: 1.42s\n",
            "\n",
            "Loss: 58.8788 | Value loss: 40.5280 | Reward loss: 33.4975 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 424...\n",
            "Total reward: -124.03 | Steps: 84 | Time: 1.28s\n",
            "\n",
            "Loss: 58.8262 | Value loss: 40.4505 | Reward loss: 33.4643 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 425...\n",
            "Total reward: -306.39 | Steps: 113 | Time: 1.74s\n",
            "\n",
            "Loss: 58.9464 | Value loss: 40.4989 | Reward loss: 33.5724 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 426...\n",
            "Total reward: -100.97 | Steps: 90 | Time: 1.44s\n",
            "\n",
            "Loss: 58.8658 | Value loss: 40.4954 | Reward loss: 33.4927 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 427...\n",
            "Total reward: -113.11 | Steps: 87 | Time: 1.36s\n",
            "\n",
            "Loss: 58.8191 | Value loss: 40.5143 | Reward loss: 33.4413 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 428...\n",
            "Total reward: -67.84 | Steps: 62 | Time: 0.97s\n",
            "\n",
            "Loss: 58.9712 | Value loss: 40.4851 | Reward loss: 33.6007 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 429...\n",
            "Total reward: -113.45 | Steps: 67 | Time: 1.02s\n",
            "\n",
            "Loss: 58.9196 | Value loss: 40.5228 | Reward loss: 33.5396 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 430...\n",
            "Total reward: -79.70 | Steps: 78 | Time: 1.23s\n",
            "\n",
            "Loss: 58.8710 | Value loss: 40.4785 | Reward loss: 33.5022 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 431...\n",
            "Total reward: -129.32 | Steps: 75 | Time: 1.15s\n",
            "\n",
            "Loss: 58.9896 | Value loss: 40.5081 | Reward loss: 33.6132 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 432...\n",
            "Total reward: -204.56 | Steps: 111 | Time: 1.68s\n",
            "\n",
            "Loss: 58.9358 | Value loss: 40.4863 | Reward loss: 33.5649 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 433...\n",
            "Total reward: -130.45 | Steps: 109 | Time: 1.83s\n",
            "\n",
            "Loss: 58.8632 | Value loss: 40.4837 | Reward loss: 33.4931 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 434...\n",
            "Total reward: -349.70 | Steps: 83 | Time: 1.39s\n",
            "\n",
            "Loss: 58.9424 | Value loss: 40.4962 | Reward loss: 33.5691 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 435...\n",
            "Total reward: -179.19 | Steps: 114 | Time: 1.88s\n",
            "\n",
            "Loss: 58.9663 | Value loss: 40.5044 | Reward loss: 33.5909 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 436...\n",
            "Total reward: -401.65 | Steps: 92 | Time: 1.54s\n",
            "\n",
            "Loss: 58.8574 | Value loss: 40.5161 | Reward loss: 33.4792 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 437...\n",
            "Total reward: -159.10 | Steps: 126 | Time: 1.95s\n",
            "\n",
            "Loss: 59.0319 | Value loss: 40.4799 | Reward loss: 33.6627 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 438...\n",
            "Total reward: -258.18 | Steps: 115 | Time: 1.82s\n",
            "\n",
            "Loss: 58.9364 | Value loss: 40.5351 | Reward loss: 33.5534 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 439...\n",
            "Total reward: -285.97 | Steps: 81 | Time: 1.25s\n",
            "\n",
            "Loss: 59.0577 | Value loss: 40.5106 | Reward loss: 33.6807 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Testing episode 440...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_440_test_0.mp4\n",
            "Total reward: -101.79\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_440_test_1.mp4\n",
            "Total reward: -178.77\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_440_test_2.mp4\n",
            "Total reward: -96.99\n",
            "\n",
            "\n",
            "Training episode 440...\n",
            "Total reward: -185.00 | Steps: 67 | Time: 1.06s\n",
            "\n",
            "Loss: 58.9239 | Value loss: 40.5195 | Reward loss: 33.5448 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 441...\n",
            "Total reward: -98.60 | Steps: 93 | Time: 1.45s\n",
            "\n",
            "Loss: 58.9568 | Value loss: 40.5003 | Reward loss: 33.5824 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 442...\n",
            "Total reward: -264.05 | Steps: 130 | Time: 2.05s\n",
            "\n",
            "Loss: 58.9481 | Value loss: 40.5214 | Reward loss: 33.5684 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 443...\n",
            "Total reward: -195.28 | Steps: 109 | Time: 1.71s\n",
            "\n",
            "Loss: 58.8811 | Value loss: 40.5205 | Reward loss: 33.5017 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 444...\n",
            "Total reward: -92.73 | Steps: 83 | Time: 1.27s\n",
            "\n",
            "Loss: 58.9625 | Value loss: 40.5078 | Reward loss: 33.5863 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 445...\n",
            "Total reward: -151.02 | Steps: 93 | Time: 1.41s\n",
            "\n",
            "Loss: 58.9750 | Value loss: 40.4941 | Reward loss: 33.6021 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 446...\n",
            "Total reward: -113.59 | Steps: 93 | Time: 1.42s\n",
            "\n",
            "Loss: 58.9675 | Value loss: 40.5235 | Reward loss: 33.5873 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 447...\n",
            "Total reward: -249.87 | Steps: 98 | Time: 1.49s\n",
            "\n",
            "Loss: 59.0049 | Value loss: 40.4867 | Reward loss: 33.6339 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 448...\n",
            "Total reward: -92.79 | Steps: 62 | Time: 0.97s\n",
            "\n",
            "Loss: 58.9683 | Value loss: 40.5207 | Reward loss: 33.5889 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 449...\n",
            "Total reward: -59.03 | Steps: 76 | Time: 1.18s\n",
            "\n",
            "Loss: 59.0126 | Value loss: 40.5198 | Reward loss: 33.6334 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 450...\n",
            "Total reward: -139.70 | Steps: 78 | Time: 1.19s\n",
            "\n",
            "Loss: 58.9719 | Value loss: 40.5335 | Reward loss: 33.5894 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 451...\n",
            "Total reward: -300.36 | Steps: 132 | Time: 2.06s\n",
            "\n",
            "Loss: 59.0639 | Value loss: 40.5180 | Reward loss: 33.6852 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 452...\n",
            "Total reward: -191.66 | Steps: 94 | Time: 1.43s\n",
            "\n",
            "Loss: 59.0053 | Value loss: 40.5273 | Reward loss: 33.6240 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 453...\n",
            "Total reward: -41.30 | Steps: 149 | Time: 2.25s\n",
            "\n",
            "Loss: 58.8370 | Value loss: 40.4841 | Reward loss: 33.4667 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 454...\n",
            "Total reward: -131.78 | Steps: 93 | Time: 1.42s\n",
            "\n",
            "Loss: 58.9582 | Value loss: 40.4797 | Reward loss: 33.5890 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 455...\n",
            "Total reward: -285.43 | Steps: 89 | Time: 1.36s\n",
            "\n",
            "Loss: 59.0370 | Value loss: 40.5177 | Reward loss: 33.6583 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 456...\n",
            "Total reward: -126.95 | Steps: 110 | Time: 1.67s\n",
            "\n",
            "Loss: 59.0470 | Value loss: 40.5219 | Reward loss: 33.6672 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 457...\n",
            "Total reward: -293.98 | Steps: 90 | Time: 1.37s\n",
            "\n",
            "Loss: 58.9968 | Value loss: 40.4454 | Reward loss: 33.6362 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 458...\n",
            "Total reward: -347.38 | Steps: 91 | Time: 1.39s\n",
            "\n",
            "Loss: 59.1550 | Value loss: 40.5489 | Reward loss: 33.7685 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 459...\n",
            "Total reward: -139.28 | Steps: 65 | Time: 1.00s\n",
            "\n",
            "Loss: 59.0693 | Value loss: 40.5158 | Reward loss: 33.6911 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Testing episode 460...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_460_test_0.mp4\n",
            "Total reward: -8.10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_460_test_1.mp4\n",
            "Total reward: -236.82\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_460_test_2.mp4\n",
            "Total reward: -348.48\n",
            "\n",
            "\n",
            "Training episode 460...\n",
            "Total reward: -118.92 | Steps: 97 | Time: 1.49s\n",
            "\n",
            "Loss: 59.1393 | Value loss: 40.5327 | Reward loss: 33.7569 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 461...\n",
            "Total reward: -226.63 | Steps: 126 | Time: 2.00s\n",
            "\n",
            "Loss: 58.9170 | Value loss: 40.4833 | Reward loss: 33.5469 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 462...\n",
            "Total reward: -121.08 | Steps: 68 | Time: 1.11s\n",
            "\n",
            "Loss: 58.9555 | Value loss: 40.5331 | Reward loss: 33.5729 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 463...\n",
            "Total reward: -127.89 | Steps: 73 | Time: 1.17s\n",
            "\n",
            "Loss: 58.9822 | Value loss: 40.4822 | Reward loss: 33.6124 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 464...\n",
            "Total reward: -440.72 | Steps: 88 | Time: 1.43s\n",
            "\n",
            "Loss: 59.0256 | Value loss: 40.5075 | Reward loss: 33.6496 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 465...\n",
            "Total reward: -116.40 | Steps: 75 | Time: 1.23s\n",
            "\n",
            "Loss: 58.9427 | Value loss: 40.4613 | Reward loss: 33.5781 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 466...\n",
            "Total reward: -170.47 | Steps: 89 | Time: 1.54s\n",
            "\n",
            "Loss: 59.0485 | Value loss: 40.5413 | Reward loss: 33.6640 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 467...\n",
            "Total reward: -371.49 | Steps: 101 | Time: 1.70s\n",
            "\n",
            "Loss: 58.8875 | Value loss: 40.4799 | Reward loss: 33.5183 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 468...\n",
            "Total reward: -235.76 | Steps: 74 | Time: 1.28s\n",
            "\n",
            "Loss: 59.0351 | Value loss: 40.4739 | Reward loss: 33.6675 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 469...\n",
            "Total reward: -460.32 | Steps: 96 | Time: 1.57s\n",
            "\n",
            "Loss: 58.9832 | Value loss: 40.5284 | Reward loss: 33.6018 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 470...\n",
            "Total reward: -104.04 | Steps: 85 | Time: 1.41s\n",
            "\n",
            "Loss: 59.0555 | Value loss: 40.4802 | Reward loss: 33.6861 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 471...\n",
            "Total reward: -34.78 | Steps: 69 | Time: 1.09s\n",
            "\n",
            "Loss: 59.0702 | Value loss: 40.4792 | Reward loss: 33.7013 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 472...\n",
            "Total reward: -138.04 | Steps: 78 | Time: 1.34s\n",
            "\n",
            "Loss: 59.0834 | Value loss: 40.4670 | Reward loss: 33.7174 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 473...\n",
            "Total reward: -234.90 | Steps: 112 | Time: 1.82s\n",
            "\n",
            "Loss: 59.0781 | Value loss: 40.5007 | Reward loss: 33.7038 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 474...\n",
            "Total reward: -128.46 | Steps: 118 | Time: 1.92s\n",
            "\n",
            "Loss: 58.9838 | Value loss: 40.5419 | Reward loss: 33.5990 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 475...\n",
            "Total reward: -100.63 | Steps: 91 | Time: 1.53s\n",
            "\n",
            "Loss: 59.0496 | Value loss: 40.5147 | Reward loss: 33.6718 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 476...\n",
            "Total reward: -126.78 | Steps: 61 | Time: 0.98s\n",
            "\n",
            "Loss: 59.1077 | Value loss: 40.5578 | Reward loss: 33.7190 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 477...\n",
            "Total reward: -264.39 | Steps: 115 | Time: 1.79s\n",
            "\n",
            "Loss: 59.0935 | Value loss: 40.4983 | Reward loss: 33.7197 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 478...\n",
            "Total reward: -121.83 | Steps: 70 | Time: 1.13s\n",
            "\n",
            "Loss: 59.0071 | Value loss: 40.5297 | Reward loss: 33.6254 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 479...\n",
            "Total reward: -73.22 | Steps: 117 | Time: 1.88s\n",
            "\n",
            "Loss: 59.1084 | Value loss: 40.5158 | Reward loss: 33.7301 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Testing episode 480...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_480_test_0.mp4\n",
            "Total reward: -557.33\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_480_test_1.mp4\n",
            "Total reward: -295.30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_480_test_2.mp4\n",
            "Total reward: -220.19\n",
            "\n",
            "\n",
            "Training episode 480...\n",
            "Total reward: -90.50 | Steps: 72 | Time: 1.17s\n",
            "\n",
            "Loss: 59.0031 | Value loss: 40.5166 | Reward loss: 33.6247 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 481...\n",
            "Total reward: -106.03 | Steps: 78 | Time: 1.27s\n",
            "\n",
            "Loss: 59.0291 | Value loss: 40.5443 | Reward loss: 33.6439 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 482...\n",
            "Total reward: -265.96 | Steps: 98 | Time: 1.54s\n",
            "\n",
            "Loss: 58.9614 | Value loss: 40.5157 | Reward loss: 33.5831 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 483...\n",
            "Total reward: -88.85 | Steps: 63 | Time: 0.98s\n",
            "\n",
            "Loss: 59.1285 | Value loss: 40.5699 | Reward loss: 33.7367 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 484...\n",
            "Total reward: -443.49 | Steps: 107 | Time: 1.75s\n",
            "\n",
            "Loss: 58.9769 | Value loss: 40.4751 | Reward loss: 33.6088 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 485...\n",
            "Total reward: -90.28 | Steps: 63 | Time: 0.99s\n",
            "\n",
            "Loss: 59.0192 | Value loss: 40.5275 | Reward loss: 33.6380 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 486...\n",
            "Total reward: -99.10 | Steps: 67 | Time: 1.11s\n",
            "\n",
            "Loss: 58.9024 | Value loss: 40.5062 | Reward loss: 33.5266 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 487...\n",
            "Total reward: -266.79 | Steps: 86 | Time: 1.41s\n",
            "\n",
            "Loss: 59.0434 | Value loss: 40.5260 | Reward loss: 33.6627 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 488...\n",
            "Total reward: -273.77 | Steps: 83 | Time: 1.35s\n",
            "\n",
            "Loss: 58.9899 | Value loss: 40.5337 | Reward loss: 33.6073 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 489...\n",
            "Total reward: -177.46 | Steps: 66 | Time: 1.04s\n",
            "\n",
            "Loss: 58.9941 | Value loss: 40.5424 | Reward loss: 33.6091 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 490...\n",
            "Total reward: -102.83 | Steps: 81 | Time: 1.27s\n",
            "\n",
            "Loss: 58.9949 | Value loss: 40.5120 | Reward loss: 33.6175 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 491...\n",
            "Total reward: -200.22 | Steps: 84 | Time: 1.34s\n",
            "\n",
            "Loss: 59.0454 | Value loss: 40.5273 | Reward loss: 33.6643 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 492...\n",
            "Total reward: -342.43 | Steps: 103 | Time: 1.73s\n",
            "\n",
            "Loss: 59.0072 | Value loss: 40.5319 | Reward loss: 33.6250 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 493...\n",
            "Total reward: -330.71 | Steps: 100 | Time: 1.65s\n",
            "\n",
            "Loss: 58.9273 | Value loss: 40.5086 | Reward loss: 33.5509 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 494...\n",
            "Total reward: -91.13 | Steps: 73 | Time: 1.24s\n",
            "\n",
            "Loss: 59.1079 | Value loss: 40.5470 | Reward loss: 33.7219 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 495...\n",
            "Total reward: -132.53 | Steps: 142 | Time: 2.45s\n",
            "\n",
            "Loss: 59.0265 | Value loss: 40.5378 | Reward loss: 33.6429 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 496...\n",
            "Total reward: -106.29 | Steps: 80 | Time: 1.51s\n",
            "\n",
            "Loss: 59.0197 | Value loss: 40.5244 | Reward loss: 33.6394 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 497...\n",
            "Total reward: -144.21 | Steps: 94 | Time: 1.54s\n",
            "\n",
            "Loss: 59.1326 | Value loss: 40.5450 | Reward loss: 33.7470 | Policy loss: 15.2494\n",
            "\n",
            "\n",
            "Training episode 498...\n",
            "Total reward: -112.64 | Steps: 78 | Time: 1.25s\n",
            "\n",
            "Loss: 59.0274 | Value loss: 40.5278 | Reward loss: 33.6464 | Policy loss: 15.2491\n",
            "\n",
            "\n",
            "Training episode 499...\n",
            "Total reward: -147.48 | Steps: 84 | Time: 1.36s\n",
            "\n",
            "Loss: 58.8487 | Value loss: 40.5053 | Reward loss: 33.4732 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Testing episode 500...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_500_test_0.mp4\n",
            "Total reward: -153.76\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_500_test_1.mp4\n",
            "Total reward: -143.65\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving video to data/anims//2024-10-18_15-47-50/ep_500_test_2.mp4\n",
            "Total reward: -302.95\n",
            "\n",
            "\n",
            "Training episode 500...\n",
            "Total reward: -147.98 | Steps: 115 | Time: 1.90s\n",
            "\n",
            "Loss: 58.9543 | Value loss: 40.5315 | Reward loss: 33.5721 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 501...\n",
            "Total reward: -102.02 | Steps: 76 | Time: 1.27s\n",
            "\n",
            "Loss: 58.9086 | Value loss: 40.5433 | Reward loss: 33.5235 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 502...\n",
            "Total reward: -113.02 | Steps: 85 | Time: 1.30s\n",
            "\n",
            "Loss: 58.9777 | Value loss: 40.5433 | Reward loss: 33.5927 | Policy loss: 15.2492\n",
            "\n",
            "\n",
            "Training episode 503...\n",
            "Total reward: -223.39 | Steps: 72 | Time: 1.16s\n",
            "\n",
            "Loss: 58.9792 | Value loss: 40.5307 | Reward loss: 33.5972 | Policy loss: 15.2493\n",
            "\n",
            "\n",
            "Training episode 504...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m muzero \u001b[38;5;241m=\u001b[39m MuZero()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmuzero\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[10], line 41\u001b[0m, in \u001b[0;36mMuZero.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_play(episode)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Run self-play simulation\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m game_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_history[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mtotal_reward)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Add trajectory data to replay buffer\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[10], line 95\u001b[0m, in \u001b[0;36mMuZero.play\u001b[0;34m(self, episode)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Interaction loop\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# Plan and act using Monte-Carlo Tree Search (MCTS)\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mMCTS\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     action \u001b[38;5;241m=\u001b[39m select_action(root)\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# Interact with the environment\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[9], line 145\u001b[0m, in \u001b[0;36mMCTS.run\u001b[0;34m(self, network, observation, add_exploration_noise)\u001b[0m\n\u001b[1;32m    141\u001b[0m value, reward, policy_logits, hidden_state \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mrecurrent_inference(\n\u001b[1;32m    142\u001b[0m     parent\u001b[38;5;241m.\u001b[39mhidden_state, torch\u001b[38;5;241m.\u001b[39mtensor([[action]])\n\u001b[1;32m    143\u001b[0m )\n\u001b[1;32m    144\u001b[0m value \u001b[38;5;241m=\u001b[39m support_to_scalar(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39msupport_size)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 145\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43msupport_to_scalar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    146\u001b[0m node\u001b[38;5;241m.\u001b[39mexpand(\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39maction_space,\n\u001b[1;32m    148\u001b[0m     reward,\n\u001b[1;32m    149\u001b[0m     policy_logits,\n\u001b[1;32m    150\u001b[0m     hidden_state,\n\u001b[1;32m    151\u001b[0m )\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackpropagate(search_path, value, min_max_stats)\n",
            "Cell \u001b[0;32mIn[6], line 167\u001b[0m, in \u001b[0;36msupport_to_scalar\u001b[0;34m(logits, support_size)\u001b[0m\n\u001b[1;32m    158\u001b[0m support \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    159\u001b[0m     torch\u001b[38;5;241m.\u001b[39mtensor([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m-\u001b[39msupport_size, support_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)], device\u001b[38;5;241m=\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;241m.\u001b[39mexpand(probabilities\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    162\u001b[0m )\n\u001b[1;32m    164\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(support \u001b[38;5;241m*\u001b[39m probabilities, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msign(x) \u001b[38;5;241m*\u001b[39m (\n\u001b[0;32m--> 167\u001b[0m     ((torch\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.001\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.001\u001b[39m)) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.001\u001b[39m)) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    168\u001b[0m )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "muzero = MuZero()\n",
        "\n",
        "muzero.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wgf6dqjLs3Y2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "1oc0JZ621UT4",
        "Ng-fNWc51Z4G",
        "EqZP8ilRoCuz",
        "nKyyJU_porAh",
        "aDebWMeB4_NV",
        "YLnIUlrg5hKD",
        "8FFb9Kdpoeax"
      ],
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
