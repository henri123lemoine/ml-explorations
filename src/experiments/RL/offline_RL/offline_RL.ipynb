{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KKkX86NX8d5"
      },
      "source": [
        "# Offline RL\n",
        "\n",
        "*By*\n",
        "\n",
        "*Henri Lemoine; 261056402; henri.lemoine@mail.mcgill.ca*\n",
        "\n",
        "*Frederic Baroz; 261118133; frederic.baroz@mail.mcgill.ca*\n",
        "\n",
        "***TO DISCUSS***\n",
        "- use discretized states for log reg\n",
        "- in fitted q, r is r from dataset right?\n",
        "- shuffle or not shuffle\n",
        "- what are the horizontal lines... from when we collect dataset\n",
        "- find a way to include fitted MLP QL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jy71h68YdNI"
      },
      "source": [
        "## 2.&nbsp;Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k5SNxsbcYngN"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzvcEodUYgV-"
      },
      "source": [
        "## 3.&nbsp;Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Ooa8s1tmYuov"
      },
      "outputs": [],
      "source": [
        "# AGENT HYPERPARAMETERS AND SETTINGS\n",
        "# Offline agent learning rates\n",
        "DEFAULT_GAMMA = 0.98  # for fitted q-lerning_agent\n",
        "DEFAULT_K = 10  # for fitted q-learning agent\n",
        "LEARNING_RATES = [0.1, 0.01]  # for both fitted q-learning agent and logistic regression agent\n",
        "LR_EPOCHS = 5000  # for logistic regression agent\n",
        "MLP_EPOCHS = 1  # for MLP agent\n",
        "EPISODE_CAP = 1000\n",
        "\n",
        "BATCH_SIZE = 0  # batch size for MLP agent (0=no batch is used)\n",
        "FC1_DIMS = 256  # number of dimensions for first fully connected layer of QNetwork\n",
        "FC2_DIMS = 256  # number of dimensions for second fully connected layer of QNetwork\n",
        "\n",
        "# Shuffling\n",
        "SHUFFLE_BEFORE_TRAINING = True  # for both fitted q-learning agent and logistic regression agent\n",
        "\n",
        "# Discretizing state\n",
        "USE_DISCRETIZED_STATES = True  # for both logistic regression agent and MLP agent\n",
        "STORE_STATES_AS_DISCRETIZED = True\n",
        "NB_BINS = 10\n",
        "\n",
        "# PLOTTING SETTINGS\n",
        "# Use standard deviation (True) or standard error (False)\n",
        "USE_STD: bool = True\n",
        "\n",
        "# KEYING CONSTANTS\n",
        "# Dataset sizes\n",
        "SIZE_100 = 0\n",
        "SIZE_250 = 1\n",
        "SIZE_500 = 2\n",
        "\n",
        "# Dataset conditions\n",
        "CONDITION_EXPERT = 0\n",
        "CONDITION_MIXT = 1\n",
        "CONDITION_RANDOM = 2\n",
        "\n",
        "# Agent types\n",
        "LR_AGENT = 0\n",
        "Q_AGENT = 1\n",
        "MLP_Q_AGENT = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYHwi7D0PGHu"
      },
      "source": [
        "## Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "p-oMP9E6PHwH"
      },
      "outputs": [],
      "source": [
        "# PLOTS\n",
        "def plot_simple_returns(returns, title):\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(returns)\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Cumulative undiscounted return\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_bars(results):\n",
        "    pass\n",
        "\n",
        "\n",
        "# DISCRETIZE\n",
        "def discretize_state(state, nb_bins, nb_states):\n",
        "    cart_pos, cart_vel, pole_angle, pole_vel = state\n",
        "    cart_pos = np.digitize(cart_pos, np.linspace(-2.4, 2.4, nb_bins - 1))\n",
        "    cart_vel = np.digitize(cart_vel, np.linspace(-3.5, 3.5, nb_bins - 1))\n",
        "    pole_angle = np.digitize(pole_angle, np.linspace(-0.4, 0.4, nb_bins - 1))\n",
        "    pole_vel = np.digitize(pole_vel, np.linspace(-3.5, 3.5, nb_bins - 1))\n",
        "    arr = np.zeros(nb_bins * nb_states)\n",
        "    arr[cart_pos + nb_bins * 0] = 1\n",
        "    arr[cart_vel + nb_bins * 1] = 1\n",
        "    arr[pole_angle + nb_bins * 2] = 1\n",
        "    arr[pole_vel + nb_bins * 3] = 1\n",
        "    return arr  # (40,)\n",
        "\n",
        "\n",
        "# BUILDING DATASETS\n",
        "def make_dataset(expert_data, random_data, nb_episodes, condition):\n",
        "    # TODO: should be the max of the episode column in datasets\n",
        "    if nb_episodes > min(expert_data.shape[0], random_data.shape[0]):\n",
        "        raise Exception(\n",
        "            \"Sorry, nb_episodes cannot be greater than the number of episodes in the source datasets (expert or random).\"\n",
        "        )\n",
        "\n",
        "    # Initializing dataset to return\n",
        "    dataset = np.zeros((0, expert_data.shape[1]))\n",
        "    # Loop through all needed episodes\n",
        "    for i in range(nb_episodes):\n",
        "        # Choose if data comes from expert or random agent\n",
        "        source_data = expert_data\n",
        "        if condition == CONDITION_RANDOM:\n",
        "            source_data = random_data\n",
        "        elif condition == CONDITION_MIXT:\n",
        "            if np.random.randint(2, size=1)[0] == 1:\n",
        "                source_data = random_data\n",
        "\n",
        "        # Selecting all samples for this episode index\n",
        "        episode_samples = source_data[source_data[:, 10] == i]\n",
        "\n",
        "        # Append all the samples from this episode to dataset\n",
        "        dataset = np.append(dataset, episode_samples, axis=0)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def make_all_datasets(expert_data, random_data):\n",
        "    all_datasets = {}\n",
        "\n",
        "    all_datasets[(SIZE_100, CONDITION_EXPERT)] = make_dataset(\n",
        "        expert_data, random_data, 100, CONDITION_EXPERT\n",
        "    )\n",
        "    all_datasets[(SIZE_250, CONDITION_EXPERT)] = make_dataset(\n",
        "        expert_data, random_data, 250, CONDITION_EXPERT\n",
        "    )\n",
        "    all_datasets[(SIZE_500, CONDITION_EXPERT)] = make_dataset(\n",
        "        expert_data, random_data, 500, CONDITION_EXPERT\n",
        "    )\n",
        "\n",
        "    all_datasets[(SIZE_100, CONDITION_MIXT)] = make_dataset(\n",
        "        expert_data, random_data, 100, CONDITION_MIXT\n",
        "    )\n",
        "    all_datasets[(SIZE_250, CONDITION_MIXT)] = make_dataset(\n",
        "        expert_data, random_data, 250, CONDITION_MIXT\n",
        "    )\n",
        "    all_datasets[(SIZE_500, CONDITION_MIXT)] = make_dataset(\n",
        "        expert_data, random_data, 500, CONDITION_MIXT\n",
        "    )\n",
        "\n",
        "    all_datasets[(SIZE_100, CONDITION_RANDOM)] = make_dataset(\n",
        "        expert_data, random_data, 100, CONDITION_RANDOM\n",
        "    )\n",
        "    all_datasets[(SIZE_250, CONDITION_RANDOM)] = make_dataset(\n",
        "        expert_data, random_data, 250, CONDITION_RANDOM\n",
        "    )\n",
        "    all_datasets[(SIZE_500, CONDITION_RANDOM)] = make_dataset(\n",
        "        expert_data, random_data, 500, CONDITION_RANDOM\n",
        "    )\n",
        "\n",
        "    return all_datasets\n",
        "\n",
        "\n",
        "# CALUCLATE STD\n",
        "def get_error(returns):\n",
        "    if USE_STD:\n",
        "        return np.std(returns)\n",
        "    else:\n",
        "        return np.std(returns) / np.sqrt(len(returns))\n",
        "\n",
        "\n",
        "# NAMING STUFF\n",
        "def uc_first(s):\n",
        "    return s[0].upper() + s[1:]\n",
        "\n",
        "\n",
        "def name_agent(agent_ix, ucfirst=False):\n",
        "    if agent_ix == LR_AGENT:\n",
        "        r = \"logistic regression agent\"\n",
        "    else:\n",
        "        r = \"fitted Q-learning agent\"\n",
        "\n",
        "    if ucfirst:\n",
        "        r = uc_first(r)\n",
        "    return r\n",
        "\n",
        "\n",
        "def name_size(size_ix, ucfirst=False):\n",
        "    if size_ix == SIZE_100:\n",
        "        r = \"small dataset\"\n",
        "    elif size_ix == SIZE_250:\n",
        "        r = \"medium dataset\"\n",
        "    else:\n",
        "        r = \"large dataset\"\n",
        "\n",
        "    if ucfirst:\n",
        "        r = uc_first(r)\n",
        "    return r\n",
        "\n",
        "\n",
        "def name_condition(condition_ix, ucfirst=False):\n",
        "    if condition_ix == CONDITION_EXPERT:\n",
        "        r = \"pure expert\"\n",
        "    elif condition_ix == CONDITION_MIXT:\n",
        "        r = \"mixt\"\n",
        "    else:\n",
        "        r = \"pure random\"\n",
        "\n",
        "    if ucfirst:\n",
        "        r = uc_first(r)\n",
        "    return r\n",
        "\n",
        "\n",
        "def name_algorithm(agent_ix, size_ix, condition_ix, ucfirst=False):\n",
        "    r = name_agent(agent_ix) + \" on \" + name_condition(condition_ix) + \" \" + name_size(size_ix)\n",
        "    if ucfirst:\n",
        "        r = uc_first(r)\n",
        "    return r\n",
        "\n",
        "\n",
        "def name_bar(agent_ix, condition_ix, ucfirst=False):\n",
        "    r = name_agent(agent_ix) + \" on \" + name_condition(condition_ix) + \" dataset\"\n",
        "\n",
        "    if ucfirst:\n",
        "        r = uc_first(r)\n",
        "    return r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9aL7W0RZK4x"
      },
      "source": [
        "## 4.&nbsp;Expert and random agents\n",
        "\n",
        "TODO: change replay_buffer into dataset (cause not really repla buffer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1VbhBhX4aMLA"
      },
      "outputs": [],
      "source": [
        "class RandomAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.nb_actions = self.env.action_space.n\n",
        "        self.replay_buffer = np.zeros((0, 11))\n",
        "\n",
        "    def run_episode(self, episode_ix, use_replay_buffer):\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        state = self.env.reset()[0]\n",
        "\n",
        "        while not done and total_reward <= EPISODE_CAP:  # ensure episode is finite\n",
        "            # Select uniformely random action\n",
        "            action = np.random.choice(self.nb_actions)\n",
        "            # Take action in environment\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            # Increment reward\n",
        "            total_reward += reward\n",
        "\n",
        "            # Store s, a, sprime, r and episode_index in replay buffer\n",
        "            if use_replay_buffer:\n",
        "                self.replay_buffer = np.append(\n",
        "                    self.replay_buffer,\n",
        "                    [\n",
        "                        [\n",
        "                            state[0],\n",
        "                            state[1],\n",
        "                            state[2],\n",
        "                            state[3],\n",
        "                            action,\n",
        "                            next_state[0],\n",
        "                            next_state[1],\n",
        "                            next_state[2],\n",
        "                            next_state[3],\n",
        "                            reward,\n",
        "                            episode_ix,\n",
        "                        ]\n",
        "                    ],\n",
        "                    axis=0,\n",
        "                )\n",
        "            # update state\n",
        "            state = next_state\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def run(self, nb_episodes, use_replay_buffer=False, do_print=False):\n",
        "        start_time = time.time()\n",
        "\n",
        "        if do_print:\n",
        "            print(\"Starting RandomAgent run.\")\n",
        "\n",
        "        rewards = []\n",
        "        for i in range(nb_episodes):\n",
        "            # Running episode\n",
        "            reward = self.run_episode(episode_ix=i, use_replay_buffer=use_replay_buffer)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Printing progress\n",
        "            print_interval = nb_episodes // 10\n",
        "            if do_print and (i + 1) % print_interval == 0:\n",
        "                print(f\"Episode {(i+1)}/{nb_episodes} ({((i+1)/nb_episodes * 100):.2f}%) complete.\")\n",
        "        if do_print:\n",
        "            exec_time = time.time() - start_time\n",
        "            print(f\"> Finished ({exec_time:.0f} s.).\\n\")\n",
        "\n",
        "        return rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ze5rwkF8ZdrQ"
      },
      "outputs": [],
      "source": [
        "class ActorCriticAgent:\n",
        "    def __init__(self, env, alpha_theta=0, alpha_w=0, gamma=None):\n",
        "        self.env = env\n",
        "        self.alpha_theta = alpha_theta\n",
        "        self.alpha_w = alpha_w\n",
        "        self.gamma = DEFAULT_GAMMA if gamma is None else gamma\n",
        "        self.nb_bins = NB_BINS\n",
        "\n",
        "        self.nb_states = self.env.observation_space.shape[0]  # 4\n",
        "        self.nb_actions = self.env.action_space.n  # 2\n",
        "\n",
        "        self.theta = np.random.uniform(\n",
        "            -0.001, 0.001, (self.nb_states * self.nb_bins, self.nb_actions)\n",
        "        )  # (40, 2)\n",
        "        self.w = np.random.uniform(-0.001, 0.001, (self.nb_states * self.nb_bins,))  # (40,)\n",
        "\n",
        "        self.replay_buffer = np.zeros((0, 11))\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        def softmax(x):\n",
        "            e_x = np.exp(x - np.max(x))\n",
        "            return e_x / e_x.sum(axis=0)\n",
        "\n",
        "        return softmax(np.dot(self.theta.T, state).reshape(-1))\n",
        "\n",
        "    def run_episode(self, use_replay_buffer, episode_ix):\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        I = 1\n",
        "\n",
        "        # Initialize S (first state of episode)\n",
        "        state = self.env.reset()[0]\n",
        "\n",
        "        # Loop while S is not terminal (for each time step)\n",
        "        while not done and total_reward <= EPISODE_CAP:  # ensure episode is finite\n",
        "            # Discretizing the state\n",
        "            discr_state = discretize_state(state, nb_bins=self.nb_bins, nb_states=self.nb_states)\n",
        "\n",
        "            # Choose action: A ~ pi(.|s, theta)\n",
        "            policy = self.get_policy(discr_state)\n",
        "            action = np.random.choice(self.nb_actions, p=policy)\n",
        "\n",
        "            # Take action A, observe S', R\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            next_discr_state = discretize_state(\n",
        "                next_state, nb_bins=self.nb_bins, nb_states=self.nb_states\n",
        "            )\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Update delta: delta <- R + gamma * v_hat(S', w) - v_hat(S, w)    (if S' is terminal, then v_hat(S', w) = 0)\n",
        "            delta = (\n",
        "                reward\n",
        "                + self.gamma * float(np.dot(next_discr_state, self.w.T))\n",
        "                - float(np.dot(discr_state, self.w.T))\n",
        "            )\n",
        "\n",
        "            # Update w: w <- w + alpha_w * delta * grad v_hat(S, w)\n",
        "            self.w += self.alpha_w * delta * discr_state\n",
        "\n",
        "            # Update theta: theta <- theta + alpha_theta * I * delta * grad ln pi(A|S, theta)\n",
        "            self.theta += (\n",
        "                self.alpha_theta\n",
        "                * I\n",
        "                * delta\n",
        "                * np.dot(discr_state.reshape(-1, 1), policy.reshape(1, -1))\n",
        "            )\n",
        "\n",
        "            I *= self.gamma\n",
        "\n",
        "            # Store s, a, sprime, r and episode index in replay buffer\n",
        "            if use_replay_buffer:\n",
        "                self.replay_buffer = np.append(\n",
        "                    self.replay_buffer,\n",
        "                    [\n",
        "                        [\n",
        "                            state[0],\n",
        "                            state[1],\n",
        "                            state[2],\n",
        "                            state[3],\n",
        "                            action,\n",
        "                            next_state[0],\n",
        "                            next_state[1],\n",
        "                            next_state[2],\n",
        "                            next_state[3],\n",
        "                            reward,\n",
        "                            episode_ix,\n",
        "                        ]\n",
        "                    ],\n",
        "                    axis=0,\n",
        "                )\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def run(self, nb_episodes, use_replay_buffer=False, do_print=False):\n",
        "        start_time = time.time()\n",
        "\n",
        "        if do_print:\n",
        "            print(\"Starting ActorCriticAgent run.\")\n",
        "\n",
        "        rewards = []\n",
        "        for i in range(nb_episodes):\n",
        "            # Running episode\n",
        "            reward = self.run_episode(use_replay_buffer=use_replay_buffer, episode_ix=i)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Printing progress\n",
        "            print_interval = nb_episodes // 10\n",
        "            if do_print and (i + 1) % print_interval == 0:\n",
        "                print(f\"Episode {(i+1)}/{nb_episodes} ({((i+1)/nb_episodes * 100):.0f}%) complete.\")\n",
        "        if do_print:\n",
        "            exec_time = time.time() - start_time\n",
        "            print(f\"> Finished ({exec_time:.0f} s.).\\n\")\n",
        "\n",
        "        return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ-cdBrsgsFp"
      },
      "source": [
        "## Instantiating objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VzK5aZzxguhJ"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "alpha = 1 / 8\n",
        "actor_critic_agent = ActorCriticAgent(env, alpha_theta=alpha, alpha_w=alpha)\n",
        "random_agent = RandomAgent(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "273kuoopdL_6"
      },
      "source": [
        "## 5.&nbsp;Pre-train expert agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RVAVeJOCX-c",
        "outputId": "bc140733-7a16-4f9c-9cf7-581a160f4855"
      },
      "outputs": [],
      "source": [
        "# Pre-training actor-critic agent over 1000 episodes\n",
        "\n",
        "start_time = time.time()\n",
        "nb_episodes = 1000\n",
        "pre_training_returns = actor_critic_agent.run(nb_episodes, use_replay_buffer=False, do_print=True)\n",
        "exec_time = time.time() - start_time\n",
        "print(f\"Pre-training finished in {exec_time:.0f} seconds.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "7FqWmbR6dP4z",
        "outputId": "2b347caa-965d-4466-d32d-323949dc87ee"
      },
      "outputs": [],
      "source": [
        "# Showing return over episodes after expert agent pre-training\n",
        "plot_simple_returns(\n",
        "    returns=pre_training_returns, title=f\"Expert agent (actor-critic, alpha={alpha}) pre-training\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYkcRgHsdWQd"
      },
      "source": [
        "## 6.&nbsp;Make datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_yU2hWRHwo5"
      },
      "source": [
        "###  Collect data from expert and random agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lhzzkr_-mVM5",
        "outputId": "fc7e864c-5021-4b31-c52a-fff9645d4155"
      },
      "outputs": [],
      "source": [
        "nb_episodes = 500\n",
        "\n",
        "# Collecting total data for expert agent (we also store returns for plots)\n",
        "expert_returns = actor_critic_agent.run(nb_episodes, use_replay_buffer=True, do_print=True)\n",
        "expert_data = actor_critic_agent.replay_buffer\n",
        "\n",
        "# Collecting total data for random agent (we also store returns for plots)\n",
        "random_returns = random_agent.run(nb_episodes, use_replay_buffer=True, do_print=True)\n",
        "random_data = random_agent.replay_buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l4QGnx4H19m"
      },
      "source": [
        "### Make 9 datasets according to size and condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YvmLR_E0FoN7"
      },
      "outputs": [],
      "source": [
        "all_datasets = make_all_datasets(expert_data, random_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC1HDkDK8aAq"
      },
      "source": [
        "### Quick look at datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSs2CWE48a9Y",
        "outputId": "e7eb5a70-50ef-49d9-cf83-7d2235c21e42"
      },
      "outputs": [],
      "source": [
        "for (size_ix, condition_ix), dataset in all_datasets.items():\n",
        "    print(f\"{name_size(size_ix, True)} {name_condition(condition_ix)}:\")\n",
        "    print(f\"  > Number of episodes: {np.max(dataset[:,10]+1):.0f}\")\n",
        "    print(f\"  > Number of samples: {dataset.shape[0]}\")\n",
        "    if size_ix == SIZE_500:\n",
        "        print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIg3SnjDWnq6"
      },
      "source": [
        "## Offline agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALYiRU0yWubl"
      },
      "source": [
        "### Logistic regression agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "t-CRSvytmlMi"
      },
      "outputs": [],
      "source": [
        "# Logistic regression model\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, nb_features):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(nb_features, 1)\n",
        "\n",
        "    # Forward pass with linear layer + sigmoid\n",
        "    def forward(self, x):\n",
        "        y_predicted = torch.sigmoid(self.linear(x))\n",
        "        return y_predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HO4oAg6uWxh4"
      },
      "outputs": [],
      "source": [
        "# Logistic regression agent\n",
        "class LogisticRegressionAgent:\n",
        "    def __init__(self, env, learning_rate, use_discretized_states):\n",
        "        self.env = env\n",
        "        self.learning_rate = learning_rate\n",
        "        self.use_discretized_states = use_discretized_states\n",
        "\n",
        "        self.nb_states = self.env.observation_space.shape[0]  # 4\n",
        "        self.nb_actions = self.env.action_space.n  # 2\n",
        "\n",
        "        self.lr_model = None\n",
        "\n",
        "    def get_states(self, dataset):  # TODO print progress (cause encoding long)\n",
        "        res_states = None\n",
        "        # Getting s from our dataset (first 4 columns)\n",
        "        states = dataset[:, 0:4]\n",
        "\n",
        "        # If discretize is True, then discretize the state by one-hot encoding resulting in 40 column vector for each state sample\n",
        "        if self.use_discretized_states:\n",
        "            res_states = np.zeros((0, 40))\n",
        "            for state in states:\n",
        "                discr_state = discretize_state(state, nb_bins=NB_BINS, nb_states=self.nb_states)\n",
        "                res_states = np.append(res_states, [discr_state], axis=0)\n",
        "        # Otherwise, use continuous features (4 column vector for each state sample)\n",
        "        else:\n",
        "            res_states = states\n",
        "\n",
        "        # Transform feature array into torch tensor\n",
        "        res_states = torch.from_numpy(res_states.astype(np.float32))\n",
        "        # Return features\n",
        "        return res_states\n",
        "\n",
        "    def get_actions(self, dataset):\n",
        "        # Getting actions (column 4 in dataset) and transofrm to torch tensor\n",
        "        actions = torch.from_numpy(dataset[:, 4].astype(np.float32))\n",
        "        # Making targets as column vector\n",
        "        actions = actions.view(actions.shape[0], 1)\n",
        "\n",
        "        return actions\n",
        "\n",
        "    def train(self, dataset, nb_epochs=None, do_print=False, algo_name=\"[unknown algorithm]\"):\n",
        "        start_time = time.time()\n",
        "\n",
        "        nb_epochs = LR_EPOCHS if nb_epochs is None else nb_epochs\n",
        "\n",
        "        if do_print:\n",
        "            print(f\"Starting training of {algo_name}.\")\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        if SHUFFLE_BEFORE_TRAINING:\n",
        "            np.random.shuffle(dataset)\n",
        "\n",
        "        # Getting features and target\n",
        "        if do_print:\n",
        "            print(\"Encoding features...\")\n",
        "        states = self.get_states(dataset)\n",
        "        actions = self.get_actions(dataset)\n",
        "        if do_print:\n",
        "            print(\"> Finished.\")\n",
        "\n",
        "        # Create model\n",
        "        self.lr_model = LogisticRegression(nb_features=states.shape[1])\n",
        "\n",
        "        # Loss function and optimizer\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = torch.optim.SGD(self.lr_model.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        # Training loop\n",
        "        if do_print:\n",
        "            print(\"Starting training loop...\")\n",
        "        for epoch in range(nb_epochs):\n",
        "            # Forward pass\n",
        "            y_predicted = self.lr_model(states)\n",
        "            # Loss\n",
        "            loss = criterion(y_predicted, actions)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Update weights\n",
        "            optimizer.step()\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Print progress\n",
        "            print_interval = nb_epochs // 10\n",
        "            if do_print and (epoch + 1) % print_interval == 0:\n",
        "                print(\n",
        "                    f\"Epoch {(epoch+1)}/{nb_epochs} ({((epoch+1)/nb_epochs * 100):.0f}%) complete. Loss = {loss.item():.4f}.\"\n",
        "                )\n",
        "        if do_print:\n",
        "            exec_time = time.time() - start_time\n",
        "            print(f\"> Finished ({exec_time:.0f} s.).\\n\")\n",
        "\n",
        "    def run_episode(self):\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        state = self.env.reset()[0]\n",
        "\n",
        "        while not done and total_reward <= EPISODE_CAP:  # ensure episode is finite\n",
        "            # Discretize state and make it a torch tensor\n",
        "            if self.use_discretized_states:\n",
        "                state = discretize_state(state, nb_bins=NB_BINS, nb_states=self.nb_states)\n",
        "            torch_discr_state = torch.from_numpy(state.astype(np.float32))\n",
        "            # Predict action\n",
        "            with torch.no_grad():\n",
        "                y_predicted = self.lr_model(torch_discr_state)\n",
        "                predicted_action = 0\n",
        "                if y_predicted[0] > 0.5:\n",
        "                    predicted_action = 1\n",
        "\n",
        "            # Take action and observe reward and next state\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(predicted_action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "            # Increment reward\n",
        "            total_reward += reward\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def run(self, nb_episodes, do_print=False, algo_name=\"[unknown algorithm]\"):\n",
        "        if self.lr_model is None:\n",
        "            raise Exception(\"Sorry, you have to train the agent before you run it.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if do_print:\n",
        "            print(f\"Starting run of {algo_name}.\")\n",
        "\n",
        "        rewards = []\n",
        "        for i in range(nb_episodes):\n",
        "            # Running episode\n",
        "            reward = self.run_episode()\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Printing progress\n",
        "            print_interval = nb_episodes // 10\n",
        "            if do_print and (i + 1) % print_interval == 0:\n",
        "                print(f\"Episode {(i+1)}/{nb_episodes} ({((i+1)/nb_episodes * 100):.0f}%) complete.\")\n",
        "        if do_print:\n",
        "            exec_time = time.time() - start_time\n",
        "            print(f\"> Finished ({exec_time:.0f} s.).\\n\")\n",
        "\n",
        "        return rewards\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRn16Ypyw7jM"
      },
      "source": [
        "### Fitted Q-learning Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NyDfV5jfw-w1"
      },
      "outputs": [],
      "source": [
        "class FittedQLearningAgent:\n",
        "    def __init__(self, env, learning_rate, gamma=None):\n",
        "        self.env = env\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = DEFAULT_GAMMA if gamma is None else gamma\n",
        "\n",
        "        self.nb_states = self.env.observation_space.shape[0]  # 4\n",
        "        self.nb_actions = self.env.action_space.n  # 2\n",
        "\n",
        "        self.w = np.random.uniform(-0.001, 0.001, size=(self.nb_states * NB_BINS, self.nb_actions))\n",
        "\n",
        "    def train(self, dataset, k=None, do_print=False, algo_name=\"[unknown algorithm]\"):\n",
        "        start_time = time.time()\n",
        "\n",
        "        k = DEFAULT_K if k is None else k\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        if SHUFFLE_BEFORE_TRAINING:\n",
        "            np.random.shuffle(dataset)\n",
        "\n",
        "        if do_print:\n",
        "            print(f\"Starting training of {algo_name}.\")\n",
        "\n",
        "        # Train over all dataset k times\n",
        "        for i in range(k):\n",
        "            if do_print:\n",
        "                print(f\"Starting k-iteration {(i+1)}/{k}.\")\n",
        "\n",
        "            # Loop over all dataset\n",
        "            j = 0\n",
        "            for sample in dataset:\n",
        "                # State is stored as unencoded individual values in columns [0,3]\n",
        "                state = sample[0:4]\n",
        "                # Discretize state\n",
        "                discr_state = discretize_state(state, nb_bins=NB_BINS, nb_states=self.nb_states)\n",
        "                # Getting the action which was taken in the sample\n",
        "                action = int(sample[4])\n",
        "                # Getting next state from the sample (also unencoded in columns [5,8])\n",
        "                next_state = sample[5:9]\n",
        "                # Discretize next_state\n",
        "                discr_next_state = discretize_state(\n",
        "                    next_state, nb_bins=NB_BINS, nb_states=self.nb_states\n",
        "                )\n",
        "                # Get the reward from the dataset\n",
        "                reward = sample[9]\n",
        "\n",
        "                # Calculate q and next_q values\n",
        "                q_value = np.dot(discr_state, self.w[:, action])\n",
        "                next_q_values = np.dot(discr_next_state, self.w)\n",
        "                # Calculate error\n",
        "                error = reward + self.gamma * np.max(next_q_values) - q_value\n",
        "                # Update weights\n",
        "                self.w[:, action] += self.learning_rate * error * discr_state\n",
        "\n",
        "                print_interval = dataset.shape[0] // 10\n",
        "                if do_print and (j + 1) % print_interval == 0:\n",
        "                    print(\n",
        "                        f\"  Sample {(j+1)}/{dataset.shape[0]} ({((j+1)/dataset.shape[0] * 100):.0f}%) commplete\"\n",
        "                    )\n",
        "\n",
        "                j += 1\n",
        "\n",
        "            if do_print:\n",
        "                print(f\"> K-iteration {(i+1)}/{k} ({(((i+1)/k)*100):.0f}) complete.\")\n",
        "\n",
        "        if do_print:\n",
        "            exec_time = time.time() - start_time\n",
        "            print(f\"> Finished ({exec_time:.0f} s.).\\n\")\n",
        "\n",
        "    def select_action(self, discr_state):\n",
        "        return np.random.choice(\n",
        "            np.flatnonzero(\n",
        "                np.isclose(np.dot(self.w.T, discr_state), np.dot(self.w.T, discr_state).max())\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def run_episode(self):\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        state = self.env.reset()[0]\n",
        "\n",
        "        while not done and total_reward <= EPISODE_CAP:  # ensure episode is finite\n",
        "            # Discretize state\n",
        "            discr_state = discretize_state(state, nb_bins=NB_BINS, nb_states=self.nb_states)\n",
        "            # Get next action according to greedy policy\n",
        "            action = self.select_action(discr_state)\n",
        "\n",
        "            # Take action and observe reward and next state\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "            # Increment reward\n",
        "            total_reward += reward\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def run(self, nb_episodes, do_print=False, algo_name=\"[unknown algorithm]\"):\n",
        "        start_time = time.time()\n",
        "\n",
        "        if do_print:\n",
        "            print(f\"Starting run of {algo_name}.\")\n",
        "\n",
        "        rewards = []\n",
        "        for i in range(nb_episodes):\n",
        "            # Running episode\n",
        "            reward = self.run_episode()\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Printing progress\n",
        "            print_interval = nb_episodes // 10\n",
        "            if do_print and (i + 1) % print_interval == 0:\n",
        "                print(f\"Episode {(i+1)}/{nb_episodes} ({((i+1)/nb_episodes * 100):.0f}%) complete.\")\n",
        "        if do_print:\n",
        "            exec_time = time.time() - start_time\n",
        "            print(f\"> Finished ({exec_time:.0f} s.).\\n\")\n",
        "\n",
        "        return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai2oeiiJbgE6"
      },
      "source": [
        "### MLP Q-learning agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m91VSQH4bx0Y"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, learning_rate, input_dims, fc1_dims, fc2_dims, nb_actions):\n",
        "        # Calling super class init\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        # Storing members\n",
        "        self.learning_rate = learning_rate\n",
        "        self.input_dims = input_dims\n",
        "        self.fc1_dims = fc1_dims\n",
        "        self.fc2_dims = fc2_dims\n",
        "        self.nb_actions = nb_actions\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(self.input_dims, self.fc1_dims)\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
        "        self.fc3 = nn.Linear(self.fc2_dims, self.nb_actions)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        # Use MSE loss\n",
        "        self.loss = nn.MSELoss()\n",
        "        # Use GPU if possible\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        # Push to device\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # Dont activate it right away, might need the actual value\n",
        "        actions = self.fc3(x)\n",
        "\n",
        "        return actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5GDB990Hbmnn"
      },
      "outputs": [],
      "source": [
        "class QNetworkAgent:\n",
        "    def __init__(self, env, learning_rate, use_discretized_states, gamma=None):\n",
        "        self.env = env\n",
        "        self.learning_rate = learning_rate\n",
        "        self.gamma = DEFAULT_GAMMA if gamma is None else gamma\n",
        "        self.use_discretized_states = use_discretized_states\n",
        "\n",
        "        self.nb_states = self.env.observation_space.shape[0]  # 4\n",
        "        self.nb_actions = self.env.action_space.n  # 2\n",
        "\n",
        "        self.qnetwork = None\n",
        "\n",
        "    def get_states(self, dataset):\n",
        "        res_states = None\n",
        "        res_next_states = None\n",
        "        # If discretize is True, then discretize the state by one-hot encoding resulting in 40 column vector for each state sample\n",
        "        if self.use_discretized_states:\n",
        "            res_states = np.zeros((0, 40))\n",
        "            res_next_states = np.zeros((0, 40))\n",
        "            for i in range(dataset.shape[0]):\n",
        "                discr_state = discretize_state(\n",
        "                    dataset[i, 0:4], nb_bins=NB_BINS, nb_states=self.nb_states\n",
        "                )\n",
        "                discr_next_state = discretize_state(\n",
        "                    dataset[i, 5:9], nb_bins=NB_BINS, nb_states=self.nb_states\n",
        "                )\n",
        "                res_states = np.append(res_states, [discr_state], axis=0)\n",
        "                res_next_states = np.append(res_next_states, [discr_next_state], axis=0)\n",
        "        # Otherwise, use continuous features (4 column vector for each state sample)\n",
        "        else:\n",
        "            res_states = dataset[:, 0:4]\n",
        "            res_next_states = dataset[:, 5:9]\n",
        "\n",
        "        # Return features\n",
        "        return res_states, res_next_states\n",
        "\n",
        "    def get_actions(self, dataset):\n",
        "        # Getting actions (column 4 in dataset) and transofrm to torch tensor\n",
        "        return dataset[:, 4]\n",
        "\n",
        "    def get_rewards(self, dataset):\n",
        "        # Getting actions (column 4 in dataset) and transofrm to torch tensor\n",
        "        return dataset[:, 9]\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        dataset,\n",
        "        nb_epochs=None,\n",
        "        batch_size=None,\n",
        "        do_print=False,\n",
        "        algo_name=\"[unknown algorithm]\",\n",
        "    ):\n",
        "        nb_epochs = MLP_EPOCHS if nb_epochs is None else nb_epochs\n",
        "\n",
        "        # Shuffle the dataset\n",
        "        if SHUFFLE_BEFORE_TRAINING:\n",
        "            np.random.shuffle(dataset)\n",
        "\n",
        "        # Getting the state, next_state, action and reward (states can be encoded) into memory\n",
        "        state_data, next_state_data = self.get_states(dataset)\n",
        "        action_data = self.get_actions(dataset)\n",
        "        reward_data = self.get_rewards(dataset)\n",
        "\n",
        "        # Instantiating QNetwork\n",
        "        self.qnetwork = QNetwork(\n",
        "            learning_rate=self.learning_rate,\n",
        "            input_dims=40,\n",
        "            fc1_dims=FC1_DIMS,\n",
        "            fc2_dims=FC2_DIMS,\n",
        "            nb_actions=self.nb_actions,\n",
        "        )\n",
        "\n",
        "        for epoch in range(nb_epochs):\n",
        "            # Zeroing out the gradient\n",
        "            self.qnetwork.optimizer.zero_grad()\n",
        "\n",
        "            # Creating batches (if batch_size is 0, then batch is the full dataset)\n",
        "            batch_size = BATCH_SIZE if batch_size is None else batch_size\n",
        "            if batch_size > 0:\n",
        "                batch = np.random.choice(dataset.shape[0], batch_size, replace=False)\n",
        "                batch_index = np.arange(batch_size, dtype=np.int32)\n",
        "            else:\n",
        "                batch = np.arange(dataset.shape[0], dtype=np.int32)\n",
        "                batch_index = np.arange(dataset.shape[0], dtype=np.int32)\n",
        "\n",
        "            # Making the batches (if batch_size is 0, then the batch is the whole dataset)\n",
        "            states = torch.tensor(state_data[batch].astype(np.float32)).to(self.qnetwork.device)\n",
        "            next_states = torch.tensor(next_state_data[batch].astype(np.float32)).to(\n",
        "                self.qnetwork.device\n",
        "            )\n",
        "            rewards = torch.tensor(reward_data[batch].astype(np.float32)).to(self.qnetwork.device)\n",
        "            actions = action_data[batch]  # no tensor needed here\n",
        "\n",
        "            # Getting q values\n",
        "            q_eval = self.qnetwork.forward(states)\n",
        "            q_eval = q_eval[batch_index, actions]\n",
        "            q_next = self.qnetwork.forward(next_states)\n",
        "\n",
        "            # Calculate target\n",
        "            q_target = rewards + self.gamma * torch.max(q_next, dim=1)[0]\n",
        "            # Getting loss function\n",
        "            loss = self.qnetwork.loss(q_target, q_eval).to(self.qnetwork.device)\n",
        "            loss.backward()\n",
        "            self.qnetwork.optimizer.step()\n",
        "\n",
        "    def run_episode(self):\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        state = self.env.reset()[0]\n",
        "\n",
        "        while not done and total_reward <= EPISODE_CAP:  # ensure episode is finite\n",
        "            # Discretize state and make it a torch tensor\n",
        "            if self.use_discretized_states:\n",
        "                state = discretize_state(state, nb_bins=NB_BINS, nb_states=self.nb_states)\n",
        "\n",
        "            # Predict action\n",
        "            with torch.no_grad():\n",
        "                torch_state = torch.tensor(np.array([state]).astype(np.float32)).to(\n",
        "                    self.qnetwork.device\n",
        "                )\n",
        "                actions = self.qnetwork.forward(torch_state)\n",
        "                action = torch.argmax(actions).item()\n",
        "\n",
        "            # Take action and observe reward and next state\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "\n",
        "            done = terminated or truncated\n",
        "            state = next_state\n",
        "\n",
        "            # Increment reward\n",
        "            total_reward += reward\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def run(self, nb_episodes, do_print=False, algo_name=\"[unknown algorithm]\"):\n",
        "        if self.qnetwork is None:\n",
        "            raise Exception(\"Sorry, you have to train the agent before you run it.\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        if do_print:\n",
        "            print(f\"Starting run of {algo_name}.\")\n",
        "\n",
        "        rewards = []\n",
        "        for i in range(nb_episodes):\n",
        "            # Running episode\n",
        "            reward = self.run_episode()\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # Printing progress\n",
        "            print_interval = nb_episodes // 10\n",
        "            if do_print and (i + 1) % print_interval == 0:\n",
        "                print(f\"Episode {(i+1)}/{nb_episodes} ({((i+1)/nb_episodes * 100):.0f}%) complete.\")\n",
        "        if do_print:\n",
        "            exec_time = time.time() - start_time\n",
        "            print(f\"> Finished ({exec_time:.0f} s.).\\n\")\n",
        "\n",
        "        return rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "id": "AcLV7bJbHpY0",
        "outputId": "5cdbf6e1-738f-4e6c-87f1-a85cb500e919"
      },
      "outputs": [],
      "source": [
        "dataset = all_datasets[(SIZE_100, CONDITION_EXPERT)]\n",
        "qn = QNetworkAgent(env, learning_rate=0.01, use_discretized_states=True)\n",
        "qn.train(dataset=dataset, batch_size=100, nb_epochs=100, do_print=True)\n",
        "\n",
        "\n",
        "returns = qn.run(nb_episodes=1000, do_print=True, algo_name=\"q network\")\n",
        "print(\"mean return\")\n",
        "print(np.mean(returns))\n",
        "print(\"error:\")\n",
        "print(get_error(returns))\n",
        "\n",
        "plt.plot(returns)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXTJ3xoJmtGH",
        "outputId": "71d4336e-320e-4554-b7c5-b96ac5fff18c"
      },
      "outputs": [],
      "source": [
        "returns = qn.run(nb_episodes=10000, do_print=True, algo_name=\"q network\")\n",
        "print(\"mean return\")\n",
        "print(np.mean(returns))\n",
        "print(\"error:\")\n",
        "print(get_error(returns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcSvZTV0XxXh"
      },
      "source": [
        "## Train offline agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "uJ6QLRTBsiEA",
        "outputId": "fd1504f7-ea9d-4eaa-aeb0-a0eb1f704f82"
      },
      "outputs": [],
      "source": [
        "dataset_sizes = [SIZE_100, SIZE_250, SIZE_500]\n",
        "dataset_conditions = [CONDITION_EXPERT, CONDITION_MIXT, CONDITION_RANDOM]\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "algorithms = {}\n",
        "for learning_rate_ix, learning_rate in enumerate(LEARNING_RATES):\n",
        "    for dataset_size_ix in dataset_sizes:\n",
        "        for dataset_condition_ix in dataset_conditions:\n",
        "            lr_agent = LogisticRegressionAgent(\n",
        "                env, learning_rate=learning_rate, use_discretized_states=USE_DISCRETIZED_STATES\n",
        "            )\n",
        "            lr_agent.train(\n",
        "                all_datasets[(dataset_size_ix, dataset_condition_ix)],\n",
        "                do_print=True,\n",
        "                algo_name=name_algorithm(LR_AGENT, dataset_size_ix, dataset_condition_ix),\n",
        "            )\n",
        "\n",
        "            q_agent = FittedQLearningAgent(env, learning_rate=learning_rate)\n",
        "            q_agent.train(\n",
        "                all_datasets[(dataset_size_ix, dataset_condition_ix)],\n",
        "                do_print=True,\n",
        "                algo_name=name_algorithm(Q_AGENT, dataset_size_ix, dataset_condition_ix),\n",
        "            )\n",
        "\n",
        "            algorithms[(learning_rate_ix, dataset_size_ix, dataset_condition_ix, LR_AGENT)] = (\n",
        "                lr_agent\n",
        "            )\n",
        "            algorithms[(learning_rate_ix, dataset_size_ix, dataset_condition_ix, Q_AGENT)] = q_agent\n",
        "\n",
        "exec_time = time.time() - start_time\n",
        "print(f\"Training finished in {exec_time:.0f} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbD2y-dVX2bQ"
      },
      "source": [
        "## Test offline agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeK9JglE1m7v",
        "outputId": "4057fa4a-835a-43c4-e424-13cd519fe860"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "nb_episodes = 100\n",
        "returns = {}\n",
        "for algo_key, agent in algorithms.items():\n",
        "    returns[algo_key] = agent.run(\n",
        "        nb_episodes=nb_episodes,\n",
        "        do_print=True,\n",
        "        algo_name=name_algorithm(algo_key[3], algo_key[1], algo_key[2]),\n",
        "    )\n",
        "\n",
        "exec_time = time.time() - start_time\n",
        "print(f\"Testing finished in {exec_time:.0f} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPGTKFya1md_"
      },
      "source": [
        "## Plot results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "389q-w2wFMCL",
        "outputId": "81dde422-adcb-494d-c2e0-03367890ab79"
      },
      "outputs": [],
      "source": [
        "def plot_results(returns):\n",
        "    fig, axis = plt.subplots(nrows=len(LEARNING_RATES), ncols=1, figsize=(16, 12))\n",
        "    fig.suptitle(\"Figure title\")\n",
        "\n",
        "    sizes = [SIZE_100, SIZE_250, SIZE_500]\n",
        "    conditions = [CONDITION_RANDOM, CONDITION_MIXT, CONDITION_EXPERT]\n",
        "    agents = [LR_AGENT, Q_AGENT]  # TODO: add MLP AGENT\n",
        "\n",
        "    for lr_ix, lr in enumerate(LEARNING_RATES):\n",
        "        ax = axis[lr_ix]\n",
        "        ax.set_title(f\"Title {LEARNING_RATES[lr_ix]}\")\n",
        "\n",
        "        ax.axhline(\n",
        "            y=np.mean(expert_returns),\n",
        "            color=\"b\",\n",
        "            linestyle=\"--\",\n",
        "            label=\"Expert agent (actor-critic)\",\n",
        "        )\n",
        "        ax.axhline(y=np.mean(random_returns), color=\"r\", linestyle=\"--\", label=\"Random agent\")\n",
        "\n",
        "        mean_returns = {}\n",
        "        se_returns = {}\n",
        "        for condition_ix in conditions:\n",
        "            for agent_ix in agents:\n",
        "                bar_label = name_bar(agent_ix, condition_ix, True)\n",
        "                mean_returns[bar_label] = []\n",
        "                se_returns[bar_label] = []\n",
        "                for size_ix in sizes:\n",
        "                    mean_returns[bar_label].append(\n",
        "                        np.mean(returns[(lr_ix, size_ix, condition_ix, agent_ix)])\n",
        "                    )\n",
        "                    se_returns[bar_label].append(\n",
        "                        get_error(returns[(lr_ix, size_ix, condition_ix, agent_ix)])\n",
        "                    )\n",
        "\n",
        "        x = np.arange(len(sizes))\n",
        "        width = 0.1\n",
        "        multiplier = 0\n",
        "\n",
        "        for bar_label, means in mean_returns.items():\n",
        "            offset = width * multiplier\n",
        "            rects = ax.bar(x + offset, means, width, yerr=se_returns[bar_label], label=bar_label)\n",
        "            # ax.bar_label(rects, padding=3)\n",
        "            multiplier += 1\n",
        "\n",
        "        ax.set_xticks(\n",
        "            x + (width * (len(conditions) * len(agents) / 2)),\n",
        "            [name_size(size_ix, True) for size_ix in sizes],\n",
        "        )\n",
        "        ax.legend(loc=\"upper left\", ncols=3)\n",
        "        ax.set_ylim(0, 250)\n",
        "        ax.set_ylabel(\"Cumulative undiscounted return\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_results(returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zO9N18_EYQL"
      },
      "source": [
        "## Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPAMbHeKMm29"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
