{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB6hHfxMvlqT"
      },
      "source": [
        "# Sarsa/Q-Learning/Actor-Critic\n",
        "\n",
        "_By_\n",
        "\n",
        "_Henri Lemoine; 261056402; henri.lemoine@mail.mcgill.ca_\n",
        "\n",
        "_Frederic Baroz; 261118133; frederic.baroz@mail.mcgill.ca_\n",
        "\n",
        "We provide our results in this notebook. We describe the process alongside with the code and provide a discussion (report) at the end of each question.\n",
        "\n",
        "Because running the experiments sometimes takes several minutes, we provide screen-shots of the results within the discussion sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVurvTU2vveb"
      },
      "source": [
        "## 2.&nbsp;Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cl8pP5-Lvz-b"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import logging\n",
        "import time\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as mcolors\n",
        "import IPython\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QoN8yDiPBr2"
      },
      "source": [
        "## 3.&nbsp;Common constants & config\n",
        "\n",
        "**Note on standard error**\n",
        "\n",
        "We understood there was some freedom as to whether to show standard error or standard deviation. We thus again provide a `USE_STD` boolean to allow for both. Standard error is given by: $\\ se = \\frac{std}{\\sqrt{N}}$.\n",
        "\n",
        "**Note on seeding:**\n",
        "\n",
        "- The seeding behavior has changed several times throughout versions of the gym environments. According to the `env.reset()` section in the [documentation](https://gymnasium.farama.org/api/env/#gymnasium.Env.reset), the seed should be set just after creating the environment through a call to `env.reset(seed = SEED)` and never again (subsequent calls to `env.reset()` at the start of each episode should be done without passing a seed).\n",
        "- Calling `env.reset(seed = SEED)` only seeds the environmnent and since we use random number generators in our agent, python and/or numpy should be seeded as well. We provide a `seed_everything()` function that seeds the environment, python's and numpy's random number generators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3GYosnDRPEDn"
      },
      "outputs": [],
      "source": [
        "# Use standard deviation (True) or standard error (false)\n",
        "USE_STD: bool = False\n",
        "# Seed used for python, numpy and gymnasium random number generators\n",
        "SEED: int = 42\n",
        "\n",
        "\n",
        "# This utility function allows to seed all random at the same time\n",
        "def seed_everything(seed, *gym_environments):\n",
        "    \"\"\"\n",
        "    :gymnasium_environment: an instance of gym environment returned by the gym.make() function\n",
        "    :seed: Seed to be set to all RNG. If None, uses the default SEED.\n",
        "    \"\"\"\n",
        "    if seed is None:\n",
        "        seed = SEED\n",
        "\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    for env in gym_environments:\n",
        "        env.reset(seed=seed)\n",
        "\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, force=True)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCGQ_xbUvSuG"
      },
      "source": [
        "## 4.&nbsp;Question 1: Tabular RL\n",
        "\n",
        "In this problem, you will compare the performance of SARSA and expected SARSA on the Frozen\n",
        "Lake domain from the Gym environment suite:\n",
        "\n",
        "https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "\n",
        "Use a tabular representation of the state space. Exploration should be softmax (Boltzmann). You will do 10 independent runs. Each run consists of 500 segments, in each segment there are 10 episodes of training, followed by 1 episode in which you simply run the optimal policy so far (i.e. you pick actions greedily based on the current value estimates). Pick 3 settings of the temperature parameter used in the exploration and 3 settings of the learning rate.\n",
        "\n",
        "- One u-shaped graph that shows the effect of the parameters on the final training performance, expressed as the return of the agent (averaged over the last 10 training episodes and the 10 runs); note that this will typically end up as an upside-down u.\n",
        "- One u-shaped graph that shows the effect of the parameters on the final testing performance, expressed as the return of the agent (during the final testing episode, averaged over the 10 runs)\n",
        "- Learning curves (mean and standard deviation computed based on the 10 runs) for what you pick as the best parameter setting for each algorithm\n",
        "\n",
        "Write a small report that describes your experiment, your choices of parameters, and the conclu- sions you draw from the graphs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLWVXV-Yumbk"
      },
      "source": [
        "### 4.1.&nbsp;Q1 constants\n",
        "\n",
        "Class `Q1C` encapsulating constants used in question 1.\n",
        "\n",
        "- `USE_SEED` allows to set whether to use the seed or not in Q1. It is not used in Q2 as it is a requirement.\n",
        "- `MODE_TEST`and `MODE_TRAIN` is used to run a series of episodes in each mode. `MODE_TRAIN` makes the agent use the Boltzman softmax policy whereas in `MODE_TEST` makes the agent use the optimal policy. This constant is also use to decide what to plot in graphs.\n",
        "- We set a series of constants for calculating the return metrics and set a `DEFAULT_RETURN_METRICS`. These constants are only used for plotting the results.\n",
        "  - `RETURN_METRICS_CUMULATIVE` returns the _undiscounted sum of rewards received throughout episodes_;\n",
        "  - `RETURN_METRICS_AVERAGE`returns the _average of the rewards received throughout episodes_;\n",
        "  - `RETURN_METRICS_DISCOUNTED` returns the _discounted sum of rewards received tourhout episodes_ (discounted using gamma).\n",
        "- `DEFAULT_GAMMA` is the discount factor used in Sarsa and Expected Sarsa updates. Both questions 1 and 2 are episodic but provide rewards differently (+1 at the very end VS +1 throughout the episode) and we decided to use disctinct constants in each question in order to have the possibility to use distinct values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FI6084fLutpA"
      },
      "outputs": [],
      "source": [
        "class Q1C:\n",
        "    # Use or do not use seeding\n",
        "    USE_SEED = True\n",
        "\n",
        "    # Episode modes\n",
        "    MODE_TRAIN = 0\n",
        "    MODE_TEST = 1\n",
        "\n",
        "    # Types of returns\n",
        "    RETURN_METRICS_CUMULATIVE = 0\n",
        "    RETURN_METRICS_AVERAGE = 1\n",
        "    RETURN_METRICS_DISCOUNTED = 2\n",
        "    DEFAULT_RETURN_METRICS = RETURN_METRICS_CUMULATIVE\n",
        "\n",
        "    # Gamma\n",
        "    DEFAULT_GAMMA = 0.99"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUTpp-SGwFZl"
      },
      "source": [
        "### 4.2.&nbsp;Sarsa & Expected Sarsa agents\n",
        "\n",
        "We implemented one class for both Sarsa and Expected Sarsa as these algorithms are very similar.\n",
        "\n",
        "The argument `is_expected_sarsa` defines if the agent uses the standard TD(0) Sarsa or the Expected Sarsa algorithm.\n",
        "\n",
        "- `alpha` is the learning rate and `beta`the temperature for Boltzman softmax action selection.\n",
        "- `select_action()` implements the Boltzman softmax action selection policy and calls `get_action_softmax_probabilities()` to get the probability of picking each action.\n",
        "- `run_episodes()` is called to run a series of episodes. The parameter `mode` defines if those episodes are training or testing.\n",
        "- `update_value_using_sarsa()` is called by `run_episodes()` if `is_expected_sarsa` is `False`.\n",
        "- `update_value_using_expected_sarsa()` is called by `run_episodes()` if `is_expected_sarsa` is `True`.\n",
        "- `update_return()` calculate the return for each of the series of episodes that the agent is currently running. It uses `DEFAULT_RETURN_METRICS` to select whether it calculates undiscounted or discounted cumulative rewards, or averaged rewards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HbZw1AbAwEv2"
      },
      "outputs": [],
      "source": [
        "class SarsaAgent:\n",
        "    def __init__(\n",
        "        self, environment, alpha, beta, gamma=None, is_expected_sarsa=None, return_metrics=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        :environmnent: the frozen lake environment.\n",
        "        :alpha: the learning rate used in policy evaluation.\n",
        "        :beta: the temperature parameter used for Boltzman soft-max action selection.\n",
        "        :use_expected_sarsa:(optional, defaults to False) indicates whether this agent is Sarsa or Expected Sarsa.\n",
        "        :gamma:(optional, defaults to 0.98) discount factor for cumulative discounted return.\n",
        "        :return_metrics:(optional, defaults to...) specify how return should be calculated (cumulative, averaged or discounted cumulative).\n",
        "        :return: None.\n",
        "        \"\"\"\n",
        "\n",
        "        # setting class parameters\n",
        "        self.environment = environment\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        # optional parameters\n",
        "        self.is_expected_sarsa = False if is_expected_sarsa is None else is_expected_sarsa\n",
        "        self.gamma = Q1C.DEFAULT_GAMMA if gamma is None else gamma\n",
        "        self.return_metrics = (\n",
        "            Q1C.DEFAULT_RETURN_METRICS if return_metrics is None else return_metrics\n",
        "        )\n",
        "\n",
        "        # the state-action values, matrix of shape: number of states x number of actions\n",
        "        # note that if beta is very small, the softmax exponent can result in nan (exp(>=710) = inf)\n",
        "        if self.beta < 0.01:\n",
        "            self.q_values = np.zeros(\n",
        "                (environment.observation_space.n, environment.action_space.n), dtype=np.longdouble\n",
        "            )\n",
        "        else:\n",
        "            self.q_values = np.zeros(\n",
        "                (environment.observation_space.n, environment.action_space.n), dtype=np.float64\n",
        "            )\n",
        "\n",
        "    # --------------------------------------\n",
        "    # ACTION SELECTION\n",
        "    # --------------------------------------\n",
        "    def select_action(self, state, mode):\n",
        "        \"\"\"\n",
        "        This method selects an action from a given state.\n",
        "        Uses Boltzman softmax when in training mode and greedy action selection in testing mode.\n",
        "\n",
        "        :state: the state from which to select an action.\n",
        "        :mode: the mode this episode currently lies in.\n",
        "        :return: the index of the selected action [0,3].\n",
        "        \"\"\"\n",
        "\n",
        "        if mode == Q1C.MODE_TRAIN:\n",
        "            action_probabilities = self.get_action_softmax_probabilities(state)\n",
        "            return np.random.choice([*range(len(action_probabilities))], 1, p=action_probabilities)[\n",
        "                0\n",
        "            ]\n",
        "        else:\n",
        "            return np.random.choice(\n",
        "                np.where(self.q_values[state, :] == np.max(self.q_values[state, :]))[0]\n",
        "            )\n",
        "\n",
        "    def get_action_softmax_probabilities(self, state):\n",
        "        # getting all q values of actions for the particular state\n",
        "        action_q_values = self.q_values[state, :]\n",
        "        # calculating the Boltzman softmax denominator\n",
        "        denominator = sum([np.exp(qv / self.beta) for qv in action_q_values])\n",
        "        # for each action calculate the Boltzman softmax probability distribution and return the array\n",
        "        return [np.exp(qv / self.beta) / denominator for qv in action_q_values]\n",
        "\n",
        "    # --------------------------------------\n",
        "    # RUNNING EPISODES\n",
        "    # --------------------------------------\n",
        "    def run_episodes(self, nb_episodes, mode):\n",
        "        \"\"\"\n",
        "        :mode:Int: if the episodes should be run in training or testing mode.\n",
        "        :nb_episodes:Int the number of successive episodes to run.\n",
        "\n",
        "        :return:List(Float): numpy 1d array containing the return of each of the episodes we ran.\n",
        "        \"\"\"\n",
        "\n",
        "        # initialize 0 return for each episode\n",
        "        episode_returns = np.zeros(nb_episodes)\n",
        "\n",
        "        # looping through all episodes\n",
        "        for episode_index in range(nb_episodes):\n",
        "            # at the start of each episode: reset the environment and set current state\n",
        "            # passing the seed if we use a seed\n",
        "            current_state, _ = self.environment.reset()\n",
        "\n",
        "            # select the first action given initial state\n",
        "            current_action = self.select_action(current_state, mode)\n",
        "\n",
        "            # set a flag to check whether we have reached a terminal state\n",
        "            # every episode is very likely to terminate in slippery mode, but infinite\n",
        "            # loop could happen more easily with deterministic environmnent => we thus use a is_truncated flag as well\n",
        "            is_terminal_state = False\n",
        "            is_truncated = False\n",
        "\n",
        "            # define step index to be incremented (only for average return)\n",
        "            step_index = 0\n",
        "\n",
        "            # loop until we have reached a terminal state\n",
        "            while not is_terminal_state and not is_truncated:\n",
        "                # first take the current action and obtain the reward, next_state and\n",
        "                # whether we end up in a terminal state\n",
        "                next_state, reward, is_terminal_state, is_truncated, _ = self.environment.step(\n",
        "                    current_action\n",
        "                )\n",
        "                # we now get the next action with the same action selection method\n",
        "                next_action = self.select_action(next_state, mode)\n",
        "\n",
        "                # update the state-action value only if in training mode, otherwise do not update\n",
        "                # perform Expected Sarsa update or standard Sarsa update\n",
        "                if mode == Q1C.MODE_TRAIN:\n",
        "                    if self.is_expected_sarsa:\n",
        "                        self.update_value_using_expected_sarsa(\n",
        "                            current_state, current_action, reward, next_state\n",
        "                        )\n",
        "                    else:\n",
        "                        self.update_value_using_sarsa(\n",
        "                            current_state, current_action, reward, next_state, next_action\n",
        "                        )\n",
        "\n",
        "                # update the current state and the current_action to be the next state and action\n",
        "                current_action = next_action\n",
        "                current_state = next_state\n",
        "\n",
        "                # saving the return for graphs\n",
        "                episode_returns[episode_index] = self.update_return(\n",
        "                    episode_returns[episode_index], reward, step_index\n",
        "                )\n",
        "\n",
        "                step_index += 1\n",
        "\n",
        "        return episode_returns\n",
        "\n",
        "    # --------------------------------------\n",
        "    # UPDATE ACTION-STATE VALUES\n",
        "    # --------------------------------------\n",
        "    def update_value_using_sarsa(self, s, a, r, s_prime, a_prime):\n",
        "        \"\"\"\n",
        "        This method updates the q-table using TD(0) Sarsa update rule.\n",
        "\n",
        "        :s: the current state.\n",
        "        :a: the current action taken.\n",
        "        :r: the reward received from taking action a from state s.\n",
        "        :s_prime: the destination state.\n",
        "        :a_prime: the next action taken.\n",
        "        :is_terminal_state: whether the s_prime is a terminal state.\n",
        "        \"\"\"\n",
        "\n",
        "        # Calculate the TD-error\n",
        "        error = r + self.gamma * self.q_values[s_prime, a_prime] - self.q_values[s, a]\n",
        "        self.q_values[s, a] = self.q_values[s, a] + self.alpha * error\n",
        "\n",
        "    def update_value_using_expected_sarsa(self, s, a, r, s_prime):\n",
        "        \"\"\"\n",
        "        Updating the q-table using Expected Sarsa update rule.\n",
        "\n",
        "        :s: current state.\n",
        "        :a: current action.\n",
        "        :r: reward received for taking current action from current state.\n",
        "        :s_prime: next state.\n",
        "        \"\"\"\n",
        "        action_values = self.q_values[s_prime, :]\n",
        "        action_probabilities = self.get_action_softmax_probabilities(s_prime)\n",
        "        expected_value = 0\n",
        "        for action_index, action_value in enumerate(action_values):\n",
        "            expected_value += action_probabilities[action_index] * action_value\n",
        "\n",
        "        error = r + self.gamma * expected_value - self.q_values[s, a]\n",
        "        self.q_values[s, a] = self.q_values[s, a] + self.alpha * error\n",
        "\n",
        "    # --------------------------------------\n",
        "    # EPISODE RETURN (for graph)\n",
        "    # --------------------------------------\n",
        "    def update_return(self, current_return, reward, step_index):\n",
        "        \"\"\"\n",
        "        This method update the current episode's return with the reward earned at each step.\n",
        "        We implement three different return metrics: cumulative undiscounted return, cumulative discounted return, averaged return\n",
        "        \"\"\"\n",
        "        if self.return_metrics == Q1C.RETURN_METRICS_CUMULATIVE:\n",
        "            return current_return + reward\n",
        "        elif self.return_metrics == Q1C.RETURN_METRICS_DISCOUNTED:\n",
        "            return current_return + reward * (self.gamma**step_index)\n",
        "        else:\n",
        "            return current_return + (1 / (step_index + 1) * (reward - current_return))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFxecU_GVII1"
      },
      "source": [
        "### 4.3. Testing the implementation\n",
        "\n",
        "We wanted to have a visual representation of q values and the learned policy. It helped assess whether the agent was performing adequately before running the actual experiment. We decided to include it in this document.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTy7fw4tVUWO"
      },
      "source": [
        "#### 4.3.1.&nbsp;Viewer class\n",
        "\n",
        "We construct viewer for the FrozenLake 4x4 environment that builds a representation of the grid and displays q-values for each state and action.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "a9Jt3VAhn8I7"
      },
      "outputs": [],
      "source": [
        "class FrozenGridViewer:\n",
        "    def __init__(self, agent):\n",
        "        self.agent = agent\n",
        "        self.html = \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def indexToCoord(index, nb_columns):\n",
        "        x = index % nb_columns\n",
        "        y = index // nb_columns\n",
        "        return x, y\n",
        "\n",
        "    @staticmethod\n",
        "    def coordToIndex(y, x, nb_columns):\n",
        "        return y * nb_columns + x\n",
        "\n",
        "    @staticmethod\n",
        "    def get_cell_colors(y, x):\n",
        "        cell_index = FrozenGridViewer.coordToIndex(y, x, 4)\n",
        "        if cell_index == 5 or cell_index == 7 or cell_index == 11 or cell_index == 12:\n",
        "            return \"1887b8\"\n",
        "        elif cell_index == 0:\n",
        "            return \"86ff40\"\n",
        "        elif cell_index == 15:\n",
        "            return \"fcff40\"\n",
        "        else:\n",
        "            return \"c7eeff\"\n",
        "\n",
        "    def build_grid(self):\n",
        "        html = \"\"\"<table style=\"border:1px solid black; border-collapse:collapse;\">\"\"\"\n",
        "\n",
        "        for y in range(4):\n",
        "            html += \"<tr>\"\n",
        "            for x in range(4):\n",
        "                bg_col = FrozenGridViewer.get_cell_colors(y, x)\n",
        "                html += f'<td style=\"border:1px solid black;background-color:#{bg_col};\">{self.build_cell(y, x)}</td>'\n",
        "            html += \"</tr>\"\n",
        "\n",
        "        html += \"</table>\"\n",
        "        return html\n",
        "\n",
        "    def build_cell(self, y, x):\n",
        "        cell_index = FrozenGridViewer.coordToIndex(y, x, 4)\n",
        "        action_values = self.agent.q_values[cell_index, :]\n",
        "        max_actions = np.where(action_values == np.max(action_values))[0]\n",
        "\n",
        "        bold_css_0 = (\n",
        "            \"font-weight:bold;font-size:12px;color:red;\"\n",
        "            if 0 in max_actions and action_values[0] != 0\n",
        "            else \"\"\n",
        "        )\n",
        "        bold_css_1 = (\n",
        "            \"font-weight:bold;font-size:12px;color:red;\"\n",
        "            if 1 in max_actions and action_values[1] != 0\n",
        "            else \"\"\n",
        "        )\n",
        "        bold_css_2 = (\n",
        "            \"font-weight:bold;font-size:12px;color:red;\"\n",
        "            if 2 in max_actions and action_values[2] != 0\n",
        "            else \"\"\n",
        "        )\n",
        "        bold_css_3 = (\n",
        "            \"font-weight:bold;font-size:12px;color:red;\"\n",
        "            if 3 in max_actions and action_values[3] != 0\n",
        "            else \"\"\n",
        "        )\n",
        "\n",
        "        html = '<div style=\"font-size:9px;\">'\n",
        "        html += f'<div style=\"display:block;width:100px;height:25px;text-align:center;line-height:20px;{bold_css_3}\">{round(action_values[3], 4)}</div>'\n",
        "        html += f'<div style=\"display:block;width:50px;height:25px;float:left;text-align:center;line-height:20px;{bold_css_0}\">{round(action_values[0], 4)}</div>'\n",
        "        html += f'<div style=\"display:block;width:50px;height:25px;float:right;text-align:center;line-height:20px;{bold_css_2}\">{round(action_values[2], 4)}</div>'\n",
        "        html += '<div style=\"display:block;height:0px;width:0px;clear:both;\"></div>'\n",
        "        html += f'<div style=\"display:block;width:100px;height:25px;text-align:center;line-height:20px;{bold_css_1}\">{round(action_values[1], 4)}</div>'\n",
        "        html += \"</div>\"\n",
        "        return html\n",
        "\n",
        "    def display(self):\n",
        "        return self.build_grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlRGGUltV4lP"
      },
      "source": [
        "#### 4.3.2.&nbsp;Training agents\n",
        "\n",
        "We train 4 agents over 1000 episodes:\n",
        "\n",
        "- Sarsa on deterministic environment.\n",
        "- Sarsa on stochastic environment.\n",
        "- Expected Sarsa on deterministic environment.\n",
        "- Expected Sarsa on stochastic environment.\n",
        "\n",
        "All agents'parameters are arbitrarily set to $\\alpha = 0.1$ and $\\beta = 0.01$.\n",
        "\n",
        "**_Approx. 4 seconds to run in Colab._**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYSzdTPGV_6A",
        "outputId": "534e28ca-50cc-4067-a352-04ede17d7b11"
      },
      "outputs": [],
      "source": [
        "env_deterministic = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=False)\n",
        "env_stochastic = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=True)\n",
        "\n",
        "if Q1C.USE_SEED:\n",
        "    seed_everything(None, env_deterministic, env_stochastic)\n",
        "\n",
        "sarsa_deterministic = SarsaAgent(env_deterministic, alpha=0.1, beta=0.01, is_expected_sarsa=False)\n",
        "sarsa_stochastic = SarsaAgent(env_stochastic, alpha=0.1, beta=0.01, is_expected_sarsa=False)\n",
        "exp_sarsa_deterministic = SarsaAgent(\n",
        "    env_deterministic, alpha=0.1, beta=0.01, is_expected_sarsa=True\n",
        ")\n",
        "exp_sarsa_stochastic = SarsaAgent(env_stochastic, alpha=0.1, beta=0.01, is_expected_sarsa=True)\n",
        "\n",
        "starttime = time.time()\n",
        "\n",
        "_ = sarsa_deterministic.run_episodes(1000, Q1C.MODE_TRAIN)\n",
        "_ = sarsa_stochastic.run_episodes(1000, Q1C.MODE_TRAIN)\n",
        "_ = exp_sarsa_deterministic.run_episodes(1000, Q1C.MODE_TRAIN)\n",
        "_ = exp_sarsa_stochastic.run_episodes(1000, Q1C.MODE_TRAIN)\n",
        "\n",
        "exec_time = time.time() - starttime\n",
        "logger.info(f\"Training 4 agents over 1000 episodes took {round(exec_time, 2)} seconds.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF7od36B0VpT"
      },
      "source": [
        "#### 4.3.3.&nbsp;Displaying learned policy\n",
        "\n",
        "Here we construct an HTML table to show the learned policy for the 4 agent we just trained.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "id": "hc_Q-0xy0TmV",
        "outputId": "d551eec6-f4e9-49a4-9816-4e07058a7f5a"
      },
      "outputs": [],
      "source": [
        "fg1 = FrozenGridViewer(sarsa_deterministic)\n",
        "fg2 = FrozenGridViewer(sarsa_stochastic)\n",
        "fg3 = FrozenGridViewer(exp_sarsa_deterministic)\n",
        "fg4 = FrozenGridViewer(exp_sarsa_stochastic)\n",
        "\n",
        "html = '<table style=\"border:1px solid black;\">'\n",
        "\n",
        "html += \"<tr>\"\n",
        "html += '<td colspan=2 style=\"background-color:#cccccc;color:#000000;font-size:14px;text-align:center;padding:3px;font-weight:bold;\">Sarsa</td>'\n",
        "html += \"</tr>\"\n",
        "\n",
        "html += \"<tr>\"\n",
        "html += '<td style=\"background-color:#efefef;color:#000000;font-size:14px;text-align:center;padding:3px;\">Non-slippery</td>'\n",
        "html += '<td style=\"background-color:#efefef;color:#000000;font-size:14px;text-align:center;padding:3px;\">Slippery</td>'\n",
        "html += \"</tr>\"\n",
        "\n",
        "html += \"<tr>\"\n",
        "html += '<td style=\"padding:5px\">' + fg1.display() + \"</td>\"\n",
        "html += '<td style=\"padding:5px\">' + fg2.display() + \"</td>\"\n",
        "html += \"</tr>\"\n",
        "\n",
        "\n",
        "html += \"<tr>\"\n",
        "html += '<td colspan=2 style=\"background-color:#cccccc;color:#000000;font-size:14px;text-align:center;padding:3px;font-weight:bold;\">Expected Sarsa</td>'\n",
        "html += \"</tr>\"\n",
        "\n",
        "html += \"<tr>\"\n",
        "html += '<td style=\"background-color:#efefef;color:#000000;font-size:14px;text-align:center;padding:3px;\">Non-slippery</td>'\n",
        "html += '<td style=\"background-color:#efefef;color:#000000;font-size:14px;text-align:center;padding:3px;\">Slippery</td>'\n",
        "html += \"</tr>\"\n",
        "\n",
        "html += \"<tr>\"\n",
        "html += '<td style=\"padding:5px\">' + fg3.display() + \"</td>\"\n",
        "html += '<td style=\"padding:5px\">' + fg4.display() + \"</td>\"\n",
        "html += \"</tr>\"\n",
        "\n",
        "html += \"<tr>\"\n",
        "html += '<td colspan=2 style=\"font-size:12px;\">'\n",
        "html += '<div style=\"display:block; max-width:800px; padding:5px; margin:auto\"><strong>FrozenLake v1 4x4 grid with action-state values and policies after training.</strong> '\n",
        "html += 'The <span style=\"background-color:#86ff40;padding:1px;\">green cell</span> is the starting position. The <span style=\"background-color:#fcff40;padding:1px;\">yellow cell</span> represents the goal. <span style=\"background-color:#c7eeff;padding:1px;\">Light-blue cells</span> are normal cells and <span style=\"background-color:#1887b8;padding:1px;\">dark-blue cells</span> are holes. '\n",
        "html += \"In every cell, state-action values are represented at the position for each action (north, east, south, west). Bold red values indicate maximum values and thus the learned policy.</div>\"\n",
        "html += \"</td>\"\n",
        "html += \"</tr>\"\n",
        "\n",
        "html += \"</table>\"\n",
        "\n",
        "display(IPython.display.HTML(html))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "297qkYy60iSW"
      },
      "source": [
        "### 4.4.&nbsp;Experiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DsChZOplrdJ"
      },
      "source": [
        "#### 4.4.1.&nbsp;Experiment class\n",
        "\n",
        "We create a class to run the experiment on a number of different agents. An agent is defined by the type of algorithm (Sarsa or Expected Sarsa), the value for $\\alpha$ and the value for $\\beta$.\n",
        "\n",
        "**Utility methods:**\n",
        "\n",
        "- `init_results()` creates for each agent a 2d numpy array of shape (number of runs, total number of episodes).\n",
        "- `make_agents()` creates and returns a Sarsa and Expected Sarsa agent for each combination of $\\alpha$ and $\\beta$.\n",
        "- `get_segment_boundaries()` takes the segment index as an argument and return the index of its first episode, of its last training episode and of it last testing episode.\n",
        "\n",
        "**Running experiment:**\n",
        "\n",
        "- `run_experiment()` runs the experiment for all the agents and stores the episode returns (according to the default return metrics) in the `results` member. At the start of each run, agents are instantiated, the environment is re-created, and everything is seeded with a new seed (if `Q1C.USE_SEED` is `True`).\n",
        "\n",
        "**Accessing data:**\n",
        "\n",
        "- `get_parameter_evaluation_data()` returns the data arranged in a suitable format for the first and second part of question 1: assessing the effect of hyperparameters. The argument `mode` controls whether we return the returns of the training or testing episodes of the last segment (averaged over all the episode of the last segment and the runs).\n",
        "- `get_learning_curve_data()` returns the data arranged for the third part of question 1: learning curves. `best_sarsa` and `best_expected_sarsa` are 2-element arrays where the first element is the index of $\\alpha$ and the second element is the index of $\\beta$ that we considered the _best_ parameters. `only_testing` determines if it returns the return of all the episodes (testing & training) or only the testing episodes.\n",
        "\n",
        "**Graphs:**\n",
        "\n",
        "- `make_param_plot()` makes and display the plot for the first and second part of question 1, as determined by the value of `mode` (training or testing).\n",
        "- `make_learning_curve_plot()` makes and display the plot for the third part of question 1. Again, `best_sarsa` and `best_expected_sarsa` are 2-element arrays containing the index for the best $\\alpha$ and $\\beta$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "VD3G2hxgkqJR"
      },
      "outputs": [],
      "source": [
        "class SarsaExperiment:\n",
        "    ALGO_TYPE_SARSA = 0\n",
        "    ALGO_TYPE_EXPECTED_SARSA = 1\n",
        "\n",
        "    def __init__(\n",
        "        self, alphas, betas, nb_runs, nb_segments, nb_training_episodes, nb_testing_episodes\n",
        "    ):\n",
        "        # arrays for different values of alpha and beta\n",
        "        self.alphas = alphas\n",
        "        self.betas = betas\n",
        "        # run settings\n",
        "        self.nb_runs = nb_runs\n",
        "        self.nb_segments = nb_segments\n",
        "        self.nb_training_episodes = nb_training_episodes\n",
        "        self.nb_testing_episodes = nb_testing_episodes\n",
        "\n",
        "        # standard dict for storing the results\n",
        "        # for each algorithm, stores a 2d np.array of shape (run, episode_return)\n",
        "        # the dictionnary keys are of form ('algo_type', alpha_index, beta_index)\n",
        "        self.results = {}\n",
        "        self.init_results()\n",
        "\n",
        "        # Instantiate the FrozenLake 4x4 environment\n",
        "        self.environment = None\n",
        "\n",
        "    # --------------------------------------\n",
        "    # UTILITY METHODS\n",
        "    # --------------------------------------\n",
        "    def init_results(self):\n",
        "        \"\"\"\n",
        "        Creates and sets a dictionary to store the returns of the experiment, where each key is an agent algorithm and each value is a 2d numpy array of shape (run_inde, episode_index).\n",
        "        \"\"\"\n",
        "        total_nb_episodes_per_run = (self.nb_segments * self.nb_training_episodes) + (\n",
        "            self.nb_segments * self.nb_testing_episodes\n",
        "        )\n",
        "        for alpha_index, _ in enumerate(self.alphas):\n",
        "            for beta_index, _ in enumerate(self.betas):\n",
        "                self.results[(SarsaExperiment.ALGO_TYPE_SARSA, alpha_index, beta_index)] = np.zeros(\n",
        "                    (self.nb_runs, total_nb_episodes_per_run)\n",
        "                )\n",
        "                self.results[\n",
        "                    (SarsaExperiment.ALGO_TYPE_EXPECTED_SARSA, alpha_index, beta_index)\n",
        "                ] = np.zeros((self.nb_runs, total_nb_episodes_per_run))\n",
        "\n",
        "    def make_agents(self):\n",
        "        \"\"\"\n",
        "        Instantiate all the agent algorithms. One algorithm is defined by a combination of the agent type (Sarsa or Expected Sarsa), and its value of alpha and beta.\n",
        "        \"\"\"\n",
        "        agents = {}\n",
        "        for alpha_index, alpha in enumerate(self.alphas):\n",
        "            for beta_index, beta in enumerate(self.betas):\n",
        "                agents[(SarsaExperiment.ALGO_TYPE_SARSA, alpha_index, beta_index)] = SarsaAgent(\n",
        "                    self.environment, alpha, beta, is_expected_sarsa=False\n",
        "                )\n",
        "                agents[(SarsaExperiment.ALGO_TYPE_EXPECTED_SARSA, alpha_index, beta_index)] = (\n",
        "                    SarsaAgent(self.environment, alpha, beta, is_expected_sarsa=True)\n",
        "                )\n",
        "        return agents\n",
        "\n",
        "    def get_segment_boundaries(self, segment_index):\n",
        "        \"\"\"\n",
        "        Return the first episode, the last training episode and the last testing episode of the segment.\n",
        "\n",
        "        segment_index:Int: the segment we want to find the boundary of.\n",
        "        \"\"\"\n",
        "        start = (segment_index * self.nb_training_episodes) + (\n",
        "            segment_index * self.nb_testing_episodes\n",
        "        )\n",
        "        training_end = start + self.nb_training_episodes\n",
        "        testing_end = training_end + self.nb_testing_episodes\n",
        "        return start, training_end, testing_end\n",
        "\n",
        "    # --------------------------------------\n",
        "    # RUNNING EXPERIMENT\n",
        "    # --------------------------------------\n",
        "    def run_experiment(self):\n",
        "        \"\"\"\n",
        "        Runs the experiment for all algorithms. At each new run, makes a new environment, new agents, and uses new seed (if USE_SEED = True)\n",
        "        \"\"\"\n",
        "        logger.info(\"Starting experiment.\")\n",
        "        exp_starttime = time.time()\n",
        "\n",
        "        # Loop through all runs\n",
        "        for run_index in range(self.nb_runs):\n",
        "            run_starttime = time.time()\n",
        "            logger.info(f\"> Starting run {run_index}.\\n\")\n",
        "\n",
        "            # At start of run, instantiate all 18 agents (1 for each combination of sarsa type, alpha and beta)\n",
        "            self.environment = gym.make(\n",
        "                \"FrozenLake-v1\", desc=None, map_name=\"4x4\", is_slippery=True\n",
        "            )\n",
        "            agents = self.make_agents()\n",
        "\n",
        "            # Setting the seed\n",
        "            if Q1C.USE_SEED:\n",
        "                seed_everything(SEED + run_index, self.environment)\n",
        "\n",
        "            # keeping a count of segment to log messages every 50 segments (avoid modulo because slow)\n",
        "            segment_count = 0\n",
        "            # Loop through all segments\n",
        "            for segment_index in range(self.nb_segments):\n",
        "                if segment_count == 0:\n",
        "                    segm_starttime = time.time()\n",
        "                    logger.info(f\"    > Starting segment {segment_index}.\")\n",
        "\n",
        "                # Find start and stop indices for total episodes\n",
        "                start, training_end, testing_end = self.get_segment_boundaries(segment_index)\n",
        "                # for each agent, stores the returns of each episode in results\n",
        "                for agent_key, agent in agents.items():\n",
        "                    # run segment's training episodes\n",
        "                    training_returns = agent.run_episodes(\n",
        "                        self.nb_training_episodes, mode=Q1C.MODE_TRAIN\n",
        "                    )\n",
        "                    # run segment's testing episodes\n",
        "                    testing_returns = agent.run_episodes(\n",
        "                        self.nb_testing_episodes, mode=Q1C.MODE_TEST\n",
        "                    )\n",
        "                    # store episodes' returns for training and testing\n",
        "                    self.results[agent_key][run_index, start:training_end] = training_returns\n",
        "                    self.results[agent_key][run_index, training_end:testing_end] = testing_returns\n",
        "\n",
        "                if segment_count == 49:\n",
        "                    exec_time = time.time() - segm_starttime\n",
        "                    logger.info(\n",
        "                        f\"    > Finished 50 segments at segment {segment_index} in {round(exec_time, 2)} seconds.\\n\"\n",
        "                    )\n",
        "                    segment_count = 0\n",
        "                elif segment_index == self.nb_segments - 1:\n",
        "                    exec_time = time.time() - segm_starttime\n",
        "                    logger.info(\n",
        "                        f\"    > Finished last segments of run at segment {segment_index} in {round(exec_time, 2)} seconds.\\n\"\n",
        "                    )\n",
        "                    segment_count = 0\n",
        "                else:\n",
        "                    segment_count += 1\n",
        "\n",
        "            run_exec_time = time.time() - run_starttime\n",
        "            logger.info(f\"> Finished run {run_index} in {round(run_exec_time, 2)} seconds.\\n\\n\")\n",
        "\n",
        "        exp_exec_time = time.time() - exp_starttime\n",
        "        logger.info(f\"Finished experiment in {round(exp_exec_time, 2)} seconds.\")\n",
        "\n",
        "    # --------------------------------------\n",
        "    # ACCESSING DATA\n",
        "    # --------------------------------------\n",
        "    def get_parameter_evaluation_data(self, mode):\n",
        "        \"\"\"\n",
        "        This method returns the return data for parameter curves. It returns the mean of the episodes' return over the episodes of the last segment and the runs.\n",
        "\n",
        "        :mode: Int: If MODE_TRAIN, plots the return of the training episodes of the last segment (averaged over the runs). If MODE_TEST, plots the return of the testing episodes of the last segment (averaged over runs).\n",
        "        \"\"\"\n",
        "        # dictionary containing the data for plots\n",
        "        #   with keys representing algorithms (Sarsa or Expected Sarsa with each value for alpha) for a total of 6 algorithms (= 6 curves)\n",
        "        #   with values being an array of 3 values containing the averaged return for each value of beta\n",
        "        data = {}\n",
        "        for beta_index, _ in enumerate(self.betas):\n",
        "            data[(SarsaExperiment.ALGO_TYPE_SARSA, beta_index)] = np.zeros(len(self.alphas))\n",
        "            data[(SarsaExperiment.ALGO_TYPE_EXPECTED_SARSA, beta_index)] = np.zeros(\n",
        "                len(self.alphas)\n",
        "            )\n",
        "\n",
        "        for agent_key, agent_results in self.results.items():\n",
        "            # get the boundaries of the last segment's training/testing episodes\n",
        "            last_segment_start, last_segment_training_end, last_segment_testing_end = (\n",
        "                self.get_segment_boundaries(self.nb_segments - 1)\n",
        "            )\n",
        "            # get a 2d array of all the runs x all the returns of the episodes in the boundaries\n",
        "            # calculate the mean on the flattened array and storing it in the data dictionnary\n",
        "            if mode == Q1C.MODE_TRAIN:\n",
        "                mean = np.mean(agent_results[:, last_segment_start:last_segment_training_end])\n",
        "            else:\n",
        "                mean = np.mean(agent_results[:, last_segment_training_end:last_segment_testing_end])\n",
        "\n",
        "            data[(agent_key[0], agent_key[2])][agent_key[1]] = mean\n",
        "        return data\n",
        "\n",
        "    def get_learning_curve_data(self, best_sarsa, best_expected_sarsa, only_testing):\n",
        "        \"\"\"\n",
        "        This method returns the data for agents' learning curves as measured by their return averaged over the runs.\n",
        "\n",
        "        :best_sarsa: List(Float): a list of 2 elements. The first element is the index of the chosen alpha from the experiment's alphas. The second element is the index of the chosen beta from the experiment's betas.\n",
        "        :best_expected_sarsa: List(Float): similar list as best_sarsa, but for expected_sarsa's parameters.\n",
        "        :only_testing: Bool: specifies if only testing episodes or all episodes are plotted. Defaults to False.\n",
        "        \"\"\"\n",
        "        if only_testing:\n",
        "            best_sarsa_results = np.zeros(\n",
        "                (self.nb_runs, self.nb_segments * self.nb_testing_episodes)\n",
        "            )\n",
        "            best_expected_sarsa_results = np.zeros(\n",
        "                (self.nb_runs, self.nb_segments * self.nb_testing_episodes)\n",
        "            )\n",
        "\n",
        "            for segment_index in range(self.nb_segments):\n",
        "                # start & end are the start/end indices of the testing episodes of the current segment within the total results\n",
        "                # this_start & this_end are the start/end indices of the data for only test episodes used in this graph\n",
        "                _, start, end = self.get_segment_boundaries(segment_index)\n",
        "                this_start = segment_index * self.nb_testing_episodes\n",
        "                this_end = segment_index * self.nb_testing_episodes + self.nb_testing_episodes\n",
        "\n",
        "                best_sarsa_results[:, this_start:this_end] = self.results[\n",
        "                    (SarsaExperiment.ALGO_TYPE_SARSA, best_sarsa[0], best_sarsa[1])\n",
        "                ][:, start:end]\n",
        "                best_expected_sarsa_results[:, this_start:this_end] = self.results[\n",
        "                    (\n",
        "                        SarsaExperiment.ALGO_TYPE_EXPECTED_SARSA,\n",
        "                        best_expected_sarsa[0],\n",
        "                        best_expected_sarsa[1],\n",
        "                    )\n",
        "                ][:, start:end]\n",
        "        else:\n",
        "            best_sarsa_results = self.results[\n",
        "                (SarsaExperiment.ALGO_TYPE_SARSA, best_sarsa[0], best_sarsa[1])\n",
        "            ]\n",
        "            best_expected_sarsa_results = self.results[\n",
        "                (\n",
        "                    SarsaExperiment.ALGO_TYPE_EXPECTED_SARSA,\n",
        "                    best_expected_sarsa[0],\n",
        "                    best_expected_sarsa[1],\n",
        "                )\n",
        "            ]\n",
        "\n",
        "        best_sarsa_mean = np.mean(best_sarsa_results, axis=0)\n",
        "        best_expected_sarsa_mean = np.mean(best_expected_sarsa_results, axis=0)\n",
        "\n",
        "        best_sarsa_std = np.std(best_sarsa_results, axis=0)\n",
        "        best_expected_sarsa_std = np.std(best_expected_sarsa_results, axis=0)\n",
        "\n",
        "        if USE_STD:\n",
        "            best_sarsa_std = best_sarsa_std / np.sqrt(self.nb_runs)\n",
        "            best_expected_sarsa_std = best_expected_sarsa_std / np.sqrt(self.nb_runs)\n",
        "\n",
        "        return best_sarsa_mean, best_sarsa_std, best_expected_sarsa_mean, best_expected_sarsa_std\n",
        "\n",
        "    # --------------------------------------\n",
        "    # GRAPHS\n",
        "    # --------------------------------------\n",
        "    def make_params_plot(self, mode, title):\n",
        "        \"\"\"\n",
        "        This methods draws a graph showing agents' performances depending on hyperparameters values alpha (learning rate) and beta (Boltzman soft-max temperature).\n",
        "\n",
        "        :mode: Int: If MODE_TRAIN, plots the return of the training episodes of the last segment (averaged over the runs). If MODE_TEST, plots the return of the testing episodes of the last segment (averaged over runs).\n",
        "        :title: Str: The title of the figure.\n",
        "        \"\"\"\n",
        "\n",
        "        colors = list(mcolors.TABLEAU_COLORS.keys())\n",
        "        plt.style.use(\"seaborn\")\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=[13, 6])\n",
        "        i = 0\n",
        "        for agent_key, agent_data in self.get_parameter_evaluation_data(mode).items():\n",
        "            agent_name = (\n",
        "                \"Expected Sarsa\"\n",
        "                if agent_key[0] == SarsaExperiment.ALGO_TYPE_EXPECTED_SARSA\n",
        "                else \"Sarsa(0)\"\n",
        "            )\n",
        "            agent_style = \"--\" if agent_key[0] == SarsaExperiment.ALGO_TYPE_EXPECTED_SARSA else \"-\"\n",
        "\n",
        "            ax.plot(\n",
        "                self.alphas,\n",
        "                agent_data,\n",
        "                linestyle=agent_style,\n",
        "                color=colors[i],\n",
        "                label=f\"{agent_name}, β = {self.betas[agent_key[1]]}\",\n",
        "            )\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        ax.legend()\n",
        "        ax.set_xlabel(\"Learning rate α\")\n",
        "        if Q1C.DEFAULT_RETURN_METRICS == Q1C.RETURN_METRICS_AVERAGE:\n",
        "            ax.set_ylabel(\"Return \\n(average reward per episode)\")\n",
        "        elif Q1C.DEFAULT_RETURN_METRICS == Q1C.RETURN_METRICS_DISCOUNTED:\n",
        "            ax.set_ylabel(\"Return \\n(discounted cumulative return)\")\n",
        "        else:\n",
        "            ax.set_ylabel(\"Return \\n(undiscounted cumulative return)\")\n",
        "\n",
        "        ax.set_title(title, fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def make_learning_curve_plot(\n",
        "        self, best_sarsa, best_expected_sarsa, title, only_testing=False, draw_individual=True\n",
        "    ):\n",
        "        \"\"\"\n",
        "        This method draws a graph for agents' learning curves as measured by their return averaged over the runs.\n",
        "\n",
        "        :best_sarsa: List(Float): a list of 2 elements. The first element is the index of the chosen alpha from the experiment's alphas. The second element is the index of the chosen beta from the experiment's betas.\n",
        "        :best_expected_sarsa: List(Float): similar list as best_sarsa, but for expected_sarsa's parameters.\n",
        "        :title: String: the title of the figure.\n",
        "        :only_testing: Bool: specifies if only testing episodes or all episodes are plotted. Defaults to False.\n",
        "        :draw_individual: Bool: specifies if should provide 3 plots instead of one: for both algorithms and each of the algorithms individually (readability purpose).\n",
        "        \"\"\"\n",
        "        colors = list(mcolors.TABLEAU_COLORS.keys())\n",
        "        plt.style.use(\"seaborn\")\n",
        "\n",
        "        if draw_individual:\n",
        "            fig, (ax1, ax2, ax3) = plt.subplots(ncols=1, nrows=3, figsize=[16, 18])\n",
        "            fig.suptitle(title, fontsize=16, fontweight=\"bold\", y=0.94)\n",
        "        else:\n",
        "            fig, ax1 = plt.subplots(figsize=[16, 8])\n",
        "            fig.suptitle(title, fontsize=16, fontweight=\"bold\")\n",
        "\n",
        "        best_sarsa_mean, best_sarsa_std, best_expected_sarsa_mean, best_expected_sarsa_std = (\n",
        "            self.get_learning_curve_data(best_sarsa, best_expected_sarsa, only_testing)\n",
        "        )\n",
        "\n",
        "        ax1.plot(\n",
        "            best_sarsa_mean,\n",
        "            label=f\"Sarsa, α = {self.alphas[best_sarsa[0]]}, β = {self.betas[best_sarsa[1]]}\",\n",
        "            linestyle=\"-\",\n",
        "            color=\"tab:blue\",\n",
        "        )\n",
        "        ax1.fill_between(\n",
        "            np.arange(best_sarsa_mean.shape[0]),\n",
        "            best_sarsa_mean + best_sarsa_std,\n",
        "            best_sarsa_mean - best_sarsa_std,\n",
        "            alpha=0.2,\n",
        "            color=\"tab:blue\",\n",
        "        )\n",
        "        ax1.plot(\n",
        "            best_expected_sarsa_mean,\n",
        "            label=f\"Expected Sarsa, α = {self.alphas[best_expected_sarsa[0]]}, β = {self.betas[best_expected_sarsa[1]]}\",\n",
        "            linestyle=\"-\",\n",
        "            color=\"tab:orange\",\n",
        "        )\n",
        "        ax1.fill_between(\n",
        "            np.arange(best_expected_sarsa_mean.shape[0]),\n",
        "            best_expected_sarsa_mean + best_expected_sarsa_std,\n",
        "            best_expected_sarsa_mean - best_expected_sarsa_std,\n",
        "            alpha=0.2,\n",
        "            color=\"tab:orange\",\n",
        "        )\n",
        "\n",
        "        ax1.legend()\n",
        "\n",
        "        if draw_individual:\n",
        "            ax2.plot(\n",
        "                best_sarsa_mean,\n",
        "                label=f\"Sarsa, α = {self.alphas[best_sarsa[0]]}, β = {self.betas[best_sarsa[1]]}\",\n",
        "                linestyle=\"-\",\n",
        "                color=\"tab:blue\",\n",
        "            )\n",
        "            ax2.fill_between(\n",
        "                np.arange(best_sarsa_mean.shape[0]),\n",
        "                best_sarsa_mean + best_sarsa_std,\n",
        "                best_sarsa_mean - best_sarsa_std,\n",
        "                alpha=0.2,\n",
        "                color=\"tab:blue\",\n",
        "            )\n",
        "            ax2.legend()\n",
        "\n",
        "            ax3.plot(\n",
        "                best_expected_sarsa_mean,\n",
        "                label=f\"Expected Sarsa, α = {self.alphas[best_expected_sarsa[0]]}, β = {self.betas[best_expected_sarsa[1]]}\",\n",
        "                linestyle=\"-\",\n",
        "                color=\"tab:orange\",\n",
        "            )\n",
        "            ax3.fill_between(\n",
        "                np.arange(best_expected_sarsa_mean.shape[0]),\n",
        "                best_expected_sarsa_mean + best_expected_sarsa_std,\n",
        "                best_expected_sarsa_mean - best_expected_sarsa_std,\n",
        "                alpha=0.2,\n",
        "                color=\"tab:orange\",\n",
        "            )\n",
        "            ax3.legend()\n",
        "\n",
        "        if only_testing:\n",
        "            ax1.set_xlabel(\"Episodes (testing episodes only)\")\n",
        "            if draw_individual:\n",
        "                ax1.set_xlabel(\"\")\n",
        "                ax2.set_xlabel(\"\")\n",
        "                ax3.set_xlabel(\"Episodes (testing episodes only)\")\n",
        "        else:\n",
        "            ax1.set_xlabel(\"Episodes (training and testing episodes)\")\n",
        "            if draw_individual:\n",
        "                ax1.set_xlabel(\"\")\n",
        "                ax2.set_xlabel(\"\")\n",
        "                ax3.set_xlabel(\"Episodes (training and testing episodes)\")\n",
        "\n",
        "        if Q1C.DEFAULT_RETURN_METRICS == Q1C.RETURN_METRICS_AVERAGE:\n",
        "            ax1.set_ylabel(\"Return \\n(average reward per episode)\")\n",
        "            if draw_individual:\n",
        "                ax2.set_ylabel(\"Return \\n(average reward per episode)\")\n",
        "                ax3.set_ylabel(\"Return \\n(average reward per episode)\")\n",
        "        elif Q1C.DEFAULT_RETURN_METRICS == Q1C.RETURN_METRICS_DISCOUNTED:\n",
        "            ax1.set_ylabel(\"Return \\n(discounted cumulative return)\")\n",
        "            if draw_individual:\n",
        "                ax2.set_ylabel(\"Return \\n(discounted cumulative return)\")\n",
        "                ax3.set_ylabel(\"Return \\n(discounted cumulative return)\")\n",
        "        else:\n",
        "            ax1.set_ylabel(\"Return \\n(undiscounted cumulative return)\")\n",
        "            if draw_individual:\n",
        "                ax2.set_ylabel(\"Return \\n(undiscounted cumulative return)\")\n",
        "                ax3.set_ylabel(\"Return \\n(undiscounted cumulative return)\")\n",
        "\n",
        "        if draw_individual:\n",
        "            ax1.set_title(\"A. Sarsa and Expected Sarsa\", fontsize=12, fontweight=\"bold\")\n",
        "            ax2.set_title(\"B. Sarsa only\", fontsize=12, fontweight=\"bold\")\n",
        "            ax3.set_title(\"C. Expected Sarsa only\", fontsize=12, fontweight=\"bold\")\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kewas0jy2JNs"
      },
      "source": [
        "####4.4.2.&nbsp;Running the experiment\n",
        "\n",
        "**_Between 20 and 40 min. to run in Colab depending on the parameter selection (very small value of $\\beta$ result in the use of long floats) and the time when the experiment was performed (shorter during the night)._** The report at the end provides screen shots of our results in order to avoid running the experiment at the time of corrections.\n",
        "\n",
        "We logged the progress of the environment. To hide it, use `logging.basicConfig(level = logging.WARNING, force=True)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTXVcbEBT2D6",
        "outputId": "f2508ce6-84bc-4e3c-ca3b-2f8250da9f28"
      },
      "outputs": [],
      "source": [
        "# Standard settings\n",
        "alphas = [0.1, 0.5, 0.9]\n",
        "betas = [0.001, 0.005, 0.01]\n",
        "nb_runs = 10\n",
        "\n",
        "# Mega run settings\n",
        "# alphas = [0.01, 0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "# betas = [0.001, 0.005, 0.01]\n",
        "# nb_runs = 50\n",
        "\n",
        "exp = SarsaExperiment(alphas, betas, nb_runs, 500, 10, 1)\n",
        "exp.run_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "907nBG262FQy"
      },
      "source": [
        "####4.4.3.&nbsp;Graphs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eETjygGxePwe"
      },
      "source": [
        "##### A.&nbsp;Effect of parameters on training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "KY0b0_PsT53A",
        "outputId": "16d06fd9-4661-4371-d780-9c2681440caa"
      },
      "outputs": [],
      "source": [
        "exp.make_params_plot(\n",
        "    Q1C.MODE_TRAIN,\n",
        "    title=f\"Effects of hyperparameters on Sarsa and Expected Sarsa performances\\n(averaged over last {exp.nb_training_episodes} training episodes and {exp.nb_runs} runs)\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytRy3uiGfchV"
      },
      "source": [
        "##### B.&nbsp;Effect of parameters on testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "k0Vx_WrquiBw",
        "outputId": "5d98a320-472e-4127-e6c6-3e1941763429"
      },
      "outputs": [],
      "source": [
        "exp.make_params_plot(\n",
        "    Q1C.MODE_TEST,\n",
        "    title=f\"Effects of hyperparameters on Sarsa and Expected Sarsa performances\\n(last testing episode averaged over {exp.nb_runs} runs)\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHhxCwJoeY4N"
      },
      "source": [
        "##### C.&nbsp;Learning curves\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSrB6ADMefMi"
      },
      "source": [
        "Let us plot the return for each episode, averaged over the 10 runs. We first consider all episodes (training episodes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bfgZcEv6eaoo",
        "outputId": "abf8078e-b59f-405b-a728-920e46969229"
      },
      "outputs": [],
      "source": [
        "exp.make_learning_curve_plot(\n",
        "    [1, 1],\n",
        "    [1, 2],\n",
        "    title=f\"Learning rate for Sarsa and Expected Sarsa for best α and β\\n(averaged over {exp.nb_runs} runs)\",\n",
        "    draw_individual=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7otJMg1et-t"
      },
      "source": [
        "Training episodes do not allow for easy visualization of the learning curve as they are exploratory w.r.t. to the temperature β. Let us only plot the return of testing episodes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "Rz8y1VbX1BQr",
        "outputId": "8cf2acf9-a27c-49a1-fd99-57d20c43650e"
      },
      "outputs": [],
      "source": [
        "exp.make_learning_curve_plot(\n",
        "    [1, 1],\n",
        "    [1, 2],\n",
        "    title=f\"Learning rate for Sarsa and Expected Sarsa for best α and β\\n(averaged over {exp.nb_runs} runs)\",\n",
        "    only_testing=True,\n",
        "    draw_individual=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6uzcYaCgdjB"
      },
      "source": [
        "### 4.5.&nbsp;Report\n",
        "\n",
        "We implemented Sarsa(0) and Expected Sarsa in the same class and tested different values for the learning rate $\\alpha$ and the Boltzman softmax temperature $\\beta$. We run the experiment with 18 agents, 1 for each combination of (a) the algorithm (Sarsa or Expected Sarsa), (b) the value for $\\alpha$ and (c) the value for $\\beta$, over 10 runs, each consisting of 500 segment of 10 training episode and 1 testing episode.\n",
        "\n",
        "#### **Visualizing the policy**\n",
        "\n",
        "This part was not required. Since the Frozen Lake environement is very stochastic, we first desired to get a visual representation of the learnt policy. In **section 4.3.**, we implemented a \"viewer\" to represent the grid-world and display values for each action in each state. We ran 1000 training episodes for 2 different algorithms, Sarsa(0) and Expected Sarsa, in 2 different environments, deterministic and non deterministic. Each algorithm was parametrized with $\\alpha = 0.1$ and $\\beta = 0.01$\n",
        "\n",
        "<img src=\"https://i.imgur.com/TzKrNXj.png\" alt=\"Grid world to show Sarsa's and Expected Sarsa's optimal policy in stochastic and non-stochastic environments.\" width=600/>\n",
        "\n",
        "**_Figure 1:_** _Grid world showing Sarsa's and Expected Sarsa's optimal policy in stochastic and non-stochastic environments. The optimal policy is represented by the red, bold values._\n",
        "\n",
        "In the **_non-stochastic environment_**, both Sarsa and Expected Sarsa find their way to the goal in a very straightforward manner. Zero values in cells other than holes and the goal are because the optimal action is so dominant that other action get very small values and round up to zero.\n",
        "\n",
        "The agents are much more hesitant in the **_stochastic environment_**, which results from its very high stochasticity (1/3 chance of actually doing what the agent intended to do). In some cases, it is obvious what the agent intended to do. For instance, in the first-row second-column cell (0,1), both agents decide to go up and thus avoid falling in the hole of the cell below, while maintaining a probability of ending up in the cell to the right (because of the stochastic environment), and thus of progressing toward the goal. In the second-row third-column cell (1,2), both Sarsa and Expected Sarsa decide to go left, directly into a hole, but there is actually less chance of falling into a hole when picking this action (1/3), compared to the action of going either up or down (2/3), as this cell is surrounded by two holes.\n",
        "\n",
        "In other cases, it is much more difficult to understand the agents' choices. For instance in the first-row last-column cell (0,3), Expected Sarsa decides to go down directly into a hole, which is definitely not very beneficial, compared to Sarsa's more \"safe\" choice. We conclude that in such a highly stochastic environment, the agent just do not perform well here given the 1000 training episodes of our experiment.\n",
        "\n",
        "#### **Selection of hyperparameters**\n",
        "\n",
        "We used values of $\\alpha = [0.1, 0.5, 0.9]$ and $\\beta = [0.001, 0.005, 0.01]$ and inspected the impact of hyperparameters by plotting $\\alpha$ values against the cumulative (undiscounted) return, averaged over the 10 runs for the last 10 training episodes (**Figure 2**) and for the last testing episode (**Figure 3**). Each graph thus shows 6 curves, each of them representing a combination of the algorithm and $\\beta$. We chose this range for $\\alpha$ and $\\beta$ after several tries in order to show the typical upside-down v-shape curves.\n",
        "\n",
        "The discount factor $\\gamma$ was not specified. While $\\gamma=1$ would not be \"inappropriate\" in such episodic tasks it can lead to less stability in the learning process. On the other hand, if $\\gamma$ is too low, it may slow the learning process, particularly in the frozen lake environment, where only +1 rewards are generated when reaching the goal (which can be relatively far from the start state). We chose to use a value very close to 1, that is $\\gamma=0.99$. This parameter can be modified in the Q1C constant class (**section 4.1.**).\n",
        "\n",
        "For the return to be plotted, we allowed to show _cumulative undiscounted return_, _cumulative discounted return_ (using the prespecified `Q1C.DEFAULT_GAMMA`) or _average return_ over the episode. We chose to show _cumulative undiscounted return_ because we thought it would allow for a more meaningful interpretation in the frame of the Frozen Lake environment: since there is only +1 rewards when reaching the goal and zero otherwise, cumulative undiscounted return is analogous to the probability of successful episodes, when averaged over the runs.\n",
        "\n",
        "<img src=\"https://i.imgur.com/49cCUJr.png\" alt=\"Effects of hyperparameters on training performance.\" width=600/>\n",
        "\n",
        "**_Figure 2:_** _Effects of hyperparameters on training performance._\n",
        "\n",
        "<img src=\"https://i.imgur.com/5Soo6Pu.png\" alt=\"Effects of hyperparameters on training performance.\" width=600/>\n",
        "\n",
        "**_Figure 3:_** _Effects of hyperparameters on training performance._\n",
        "\n",
        "We observe the upside-down V-shaped all curves except for when $\\beta$ is 0.001. We may not have enough $\\alpha$ values to have identified the top of the curve or it may result from the highly stochastic environment and the limited sample size (10 runs). Curiously, with those parameters, Sarsa seem to outperform Expected Sarsa, which is contrary to the theory. Again, this could be that we have not found a particular $\\alpha$ value that really maximizes the performance of Expected Sarsa or because of randomness in our limite sample.\n",
        "\n",
        "When considering training episodes, the best agents showed to find the goal between 55% and 65% of the time (undiscounted reward, analogous to the percent of time the agent gets a +1 reward for having found the goal). When considering testing alone, results were slightly better with Sarsa reaching the goal 70% of times.\n",
        "\n",
        "Sarsa performed best with $\\beta = 0.01$, which is more exploratory, when considering training and with $\\beta = 0.005$, which is more greedy w.r.t. the optimal policy, when considering testing. Interestingly, Expected Sarsa showed opposite results with $\\beta = 0.005$ when considering training and with $\\beta = 0.01$ when considering testing.\n",
        "\n",
        "The best $\\alpha$ value 0.5. $\\alpha$ values that are too low or too high can result in a slower learning process by either expanding the time needed to reach convergence or \"overshooting\" the optimal state-action value at each update. Again, we limited the experiment to 3 values for $\\alpha$ and we might have missed a better value, for instance around 0.1.\n",
        "\n",
        "#### **Learning curves**\n",
        "\n",
        "We chose the parameters that maximized return when considering only the testing episodes, that is Sarsa with $\\alpha = 0.5$ and $\\beta = 0.005$ and Expected Sarsa with $\\alpha = 0.5$ and $\\beta = 0.01$.\n",
        "\n",
        "<img src=\"https://i.imgur.com/A07LL1K.png\" alt=\"Learning curves of Sarsa and Expected Sarsa for all episodes of the experiment. A. Sarsa and Expected Sarsa. B. Sarsa only. C. Expected Sarsa only.\" width=600/>\n",
        "\n",
        "**Figure 4:** _Learning curves as measured by cumulative (undiscounted) episode return for Sarsa and Expected Sarsa for all episodes (training & testing) in the experiment, averaged over the 10 runs. A. Sarsa and Expected Sarsa. B. Sarsa only. C. Expected Sarsa only._\n",
        "\n",
        "Because readability is hampered by the number of episodes, we show learning curves considering only testing episodes.\n",
        "\n",
        "<img src=\"https://i.imgur.com/wQS0VDQ.png\" alt=\"Learning curves of Sarsa and Expected Sarsa for only testing episodes.\" width=600/>\n",
        "\n",
        "**Figure 5:** _Learning curves as measured by cumulative (undiscounted) episode return for Sarsa and Expected Sarsa for only testing episodes, averaged over the 10 runs._\n",
        "\n",
        "We show return as measured by cumulative undiscounted reward for each episode, over the entire learning process (Figure 4) and only over the testing episodes (Figure 5). We observe the agents learn quickly reaching maximal return of approximately 0.6, after the 1250th episode, approximately. Again, considering we plot cumulative reward, averaged over the runs, and considering our environment only returns +1 rewards when reaching the goal and 0 otherwise, this is analogous to the probability of reaching the goal. Expected Sarsa performs slightly better than Sarsa as expected (Expected Sarsa trades computation time for slightly better results compared with Sarsa). Variance is however relatively high and does not seem to reduce much as the experiment continues. We concluded this being the result of our highly stochastic environment, and of our sample size, limited to 10 runs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Oc_PlhtK_BG"
      },
      "source": [
        "## 5.&nbsp;Question 2: Function approximation in RL\n",
        "\n",
        "Implement and compare empirically Q-learning and actor-critic with linear function approximation on the cart-pole domain from the Gym environment suite:\n",
        "\n",
        "https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
        "\n",
        "For this experiment, you should use a function approximator in which you discretize the state\n",
        "variables into 10 bins each; weights start initialized randomly between $−0.001$ and $0.001$. You will need to use the same seed for this initialization for all parameters settings, but will have 10 different seeds (for the different runs). Use 3 settings of the learning rate parameter $\\alpha$ = 1/4,1/8,1/16. Perform 10 independent runs, each of 1000 episodes. Each episode should start at a random initial state. The exploration policy should be $\\epsilon$-greedy and you should use 3 values of $\\epsilon$ of your choice.\n",
        "Plot for each algorithm 3 graphs, one for each $\\epsilon$, containing the average and standard error of the learning curves for each value of $\\alpha$ (each graph will have 3 curves). Make sure all graphs are on the same scale. Based on these graphs, pick what you consider the best parameter choice for both $\\epsilon$ and $\\alpha$ and show the best learning curve for Q-learning and actor-critic on the same graph. Write a small report that describes your experiment, your choices of parameters, and the conclusions you draw from this experimentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12uZhDeAwsJz"
      },
      "source": [
        "### 5.1.&nbsp;Q2 constants\n",
        "\n",
        "Class `Q2C` encapsulating constants used in question 2.\n",
        "\n",
        "- `DEFAULT_GAMMA` is the discount factor used in Q-learning and Actor-Critic updates. As mentioned in Q1, both Q1 and Q2 are episodic but provide rewards differently (+1 at the very end VS +1 throughout the episode) and we decided to use disctinct constants in each question in order to have the possibility to use distinct values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iHOTiVyFLC81"
      },
      "outputs": [],
      "source": [
        "class Q2C:\n",
        "    SAVE_FILES = True\n",
        "    SHOW_GRAPHS = True\n",
        "\n",
        "    # Gamma\n",
        "    DEFAULT_GAMMA = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMaeyFrwLG0L"
      },
      "source": [
        "### 5.2. Q2 Plot Helper\n",
        "\n",
        "We defined a function $\\text{plot}$ to generalize plotting tasks. The first agument _data_ contains the data and is of shape: (number of figures, number of curves, number of steps, number of runs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LVdraYvfMvvv"
      },
      "outputs": [],
      "source": [
        "def plot(\n",
        "    data: np.array,\n",
        "    title: str,\n",
        "    main_labels: List[str],\n",
        "    ax_titles: List[str],\n",
        "    algs_info: List[Tuple[str, str, int]],\n",
        "    range_y: List[Tuple[float, float]] = None,\n",
        "    size: List[int] = (10, 10),\n",
        "    fill_std: List[int] = None,\n",
        "    legend_loc: List[str] = None,\n",
        "    filename: str = None,\n",
        "    show: bool = False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        data (np.array): All that needs to be plotted. Shape: (n_figs, n_curves, n_steps, n_runs).\n",
        "        title (str): The title of the plot.\n",
        "        main_labels (List[str]): A list of strings, the first element being the label of the x axis, and the rest being the labels of the y axes.\n",
        "        ax_titles (List[str]): A list of titles for each subplot.\n",
        "        algs_info (List[Tuple[str, str, int]]): A list of tuples containing the information of each curve. Each tuple contains the following: (name, color, marker type). The marker type is an integer denoting the type of marker to be used. 0: normal full line, 1: dashed line, 2: scatter plot.\n",
        "        range_y (List[Tuple[float, float]], optional): A list of tuples of length n_figs, each denoting the range of the y axis of the corresponding subplot. Defaults to None.\n",
        "        size (List[int], optional): The size of the plot. Defaults to (10, 10).\n",
        "        fill_std (List[int], optional): A list of integers of length n_figs, each denoting whether and how to fill the standard deviation of the corresponding subplot. If None, no standard deviation will be filled. If 0, the standard deviation will be filled with a transparent color. If 1, the standard deviation will be a dashed line above and bellow the mean. If 2, just plot the standard deviation with a dashed line. Defaults to None.\n",
        "        legend_loc (List[str], optional): A list of keys indicating for each figure where to place the legend. If None or if list element not existing, use best location. Defaults to None.\n",
        "        filename (str, optional): The name of the file to save the plot to. If none, the plot will not be saved. Defaults to None.\n",
        "        show (bool, optional): Whether to show the plot or not. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    nb_figs, nb_curves, nb_steps, nb_runs = data.shape\n",
        "    range_x = (0, nb_steps - 1)\n",
        "\n",
        "    fig, axes = plt.subplots(nrows=nb_figs, ncols=1, figsize=size)\n",
        "    fig.suptitle(title, fontsize=18, fontweight=\"bold\", y=0.94)\n",
        "\n",
        "    for i in range(nb_figs):\n",
        "        if nb_figs == 1:\n",
        "            ax = axes\n",
        "        else:\n",
        "            ax = axes[i]\n",
        "\n",
        "        ax.set_title(ax_titles[i], fontsize=12, fontweight=\"bold\", loc=\"left\")\n",
        "\n",
        "        for j in range(nb_curves):\n",
        "            if algs_info[j][2] == 0:\n",
        "                mean = np.mean(data[i, j, :, :], axis=1)\n",
        "\n",
        "                std = np.std(data[i, j, :, :], axis=1)\n",
        "                if USE_STD is False:\n",
        "                    std = np.std(data[i, j, :, :], axis=1) / np.sqrt(nb_runs)\n",
        "\n",
        "                ax.plot(mean, label=algs_info[j][0], color=algs_info[j][1])\n",
        "\n",
        "                if fill_std is not None:\n",
        "                    if fill_std[i] == None:\n",
        "                        pass\n",
        "                    elif fill_std[i] == 0:\n",
        "                        ax.fill_between(\n",
        "                            np.arange(nb_steps),\n",
        "                            mean - std,\n",
        "                            mean + std,\n",
        "                            alpha=0.2,\n",
        "                            color=algs_info[j][1],\n",
        "                        )\n",
        "                    elif fill_std[i] == 1:\n",
        "                        ax.plot(mean - std, color=algs_info[j][1], linestyle=\"--\")\n",
        "                        ax.plot(mean + std, color=algs_info[j][1], linestyle=\"--\")\n",
        "                    elif fill_std[i] == 2:\n",
        "                        ax.plot(std, color=algs_info[j][1], linestyle=\"--\")\n",
        "                    else:\n",
        "                        raise ValueError(\"Invalid fill_std value.\")\n",
        "\n",
        "            elif algs_info[j][2] == 1:\n",
        "                ax.axhline(\n",
        "                    y=np.mean(data[i, j, -1, :]),\n",
        "                    label=algs_info[j][0],\n",
        "                    color=algs_info[j][1],\n",
        "                    linestyle=\"--\",\n",
        "                )\n",
        "\n",
        "            elif algs_info[j][2] == 2:\n",
        "                ax.scatter(\n",
        "                    np.arange(nb_steps),\n",
        "                    np.mean(data[i, j, :, :], axis=1),\n",
        "                    s=10,\n",
        "                    label=algs_info[j][0],\n",
        "                    color=algs_info[j][1],\n",
        "                )\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Invalid marker type.\")\n",
        "\n",
        "        if len(algs_info[j][0]) > 0:\n",
        "            ax.legend(prop={\"size\": 10})  # loc='upper right',\n",
        "\n",
        "            if legend_loc is not None and i < len(legend_loc):\n",
        "                ax.legend(loc=legend_loc[i])\n",
        "\n",
        "        if i == nb_figs - 1:\n",
        "            ax.set_xlabel(main_labels[0], fontsize=14, labelpad=10)\n",
        "        ax.set_ylabel(main_labels[i], fontsize=14, labelpad=10)\n",
        "        # ax.set_ylabel(main_labels[i+1], fontsize=14, labelpad=10)\n",
        "\n",
        "        ax.set_xlim(range_x)\n",
        "        if range_y is not None and range_y[i] is not None:\n",
        "            ax.set_ylim(range_y[i])\n",
        "\n",
        "        ax.locator_params(nbins=10, axis=\"x\")\n",
        "        ax.locator_params(nbins=5, axis=\"y\")\n",
        "        ax.grid()\n",
        "\n",
        "    if filename is not None:\n",
        "        plt.savefig(filename)\n",
        "    if show:\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYNRtMrrNSzL"
      },
      "source": [
        "### 5.3. Q-Learning And Actor-Critic Agents\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znn9bozWNo_z"
      },
      "source": [
        "#### 5.3.1. Q-Learning Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "5ogaw4k3MPVW"
      },
      "outputs": [],
      "source": [
        "class QLearningAgent:\n",
        "    def __init__(self, env, alpha=0, epsilon=0, gamma=None, nb_bins=10):\n",
        "        self.alpha = alpha\n",
        "        self.epsilon = epsilon\n",
        "        self.env = env\n",
        "        self.gamma = Q2C.DEFAULT_GAMMA if gamma is None else gamma\n",
        "        self.nb_bins = nb_bins  # 10\n",
        "\n",
        "        self.nb_states = self.env.observation_space.shape[0]  # 4\n",
        "        self.nb_actions = env.action_space.n  # 2\n",
        "\n",
        "        self.w = np.random.uniform(\n",
        "            -0.001, 0.001, size=(self.nb_states * self.nb_bins, self.nb_actions)\n",
        "        )\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return np.random.randint(self.nb_actions)\n",
        "        else:\n",
        "            # return np.argmax(np.dot(self.w.T, state)) # This is wrong because it is deterministic when there are multiple maxima\n",
        "            return np.random.choice(\n",
        "                np.flatnonzero(np.isclose(np.dot(self.w.T, state), np.dot(self.w.T, state).max()))\n",
        "            )\n",
        "\n",
        "    def discretize(self, state):\n",
        "        cart_pos, cart_vel, pole_angle, pole_vel = state\n",
        "        cart_pos = np.digitize(cart_pos, np.linspace(-2.4, 2.4, self.nb_bins - 1))\n",
        "        cart_vel = np.digitize(cart_vel, np.linspace(-3.5, 3.5, self.nb_bins - 1))\n",
        "        pole_angle = np.digitize(pole_angle, np.linspace(-0.4, 0.4, self.nb_bins - 1))\n",
        "        pole_vel = np.digitize(pole_vel, np.linspace(-3.5, 3.5, self.nb_bins - 1))\n",
        "        arr = np.zeros(self.nb_bins * self.nb_states)\n",
        "        arr[cart_pos + self.nb_bins * 0] = 1\n",
        "        arr[cart_vel + self.nb_bins * 1] = 1\n",
        "        arr[pole_angle + self.nb_bins * 2] = 1\n",
        "        arr[pole_vel + self.nb_bins * 3] = 1\n",
        "        return arr  # (40,)\n",
        "\n",
        "    def run_episode(self, state=None):\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        I = 1\n",
        "\n",
        "        # Initialize S (first state of episode)\n",
        "        if state is None:\n",
        "            state = self.env.reset()[0]\n",
        "            state = self.discretize(state)\n",
        "\n",
        "        # Loop while S is not terminal (for each time step)\n",
        "        while not done and total_reward <= 200:\n",
        "            action = self.select_action(state)\n",
        "\n",
        "            # first take the current action and obtain the reward, next_state and\n",
        "            # whether we end up in a terminal state\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            next_state = self.discretize(next_state)\n",
        "            done = terminated or truncated\n",
        "            # we now get the next action with the same action selection method\n",
        "            # next_action = self.select_action(next_state)\n",
        "\n",
        "            q_value = np.dot(state, self.w[:, action])\n",
        "            next_q_values = np.dot(next_state, self.w)\n",
        "\n",
        "            error = reward + I * np.max(next_q_values) - q_value\n",
        "            self.w[:, action] += self.alpha * error * state\n",
        "\n",
        "            I *= self.gamma\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "\n",
        "        return total_reward\n",
        "\n",
        "    def run(self, nb_episodes=1000, do_print=True):\n",
        "        rewards = []\n",
        "        for i in range(nb_episodes):\n",
        "            reward = self.run_episode()\n",
        "            rewards.append(reward)\n",
        "            if do_print and i % 100 == 0:\n",
        "                print(\n",
        "                    f\"Episode {i+1}/{nb_episodes} - Avg Reward: {np.array(rewards[-100:]).mean():.2f}\"\n",
        "                )\n",
        "        return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Q54CvTyNU0T"
      },
      "source": [
        "#### 5.3.2. Actor-Critic Agent\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DrTTC0ADLnGF"
      },
      "outputs": [],
      "source": [
        "class ActorCriticAgent:\n",
        "    def __init__(self, env, alpha_theta=0, alpha_w=0, gamma=1, nb_bins=10):\n",
        "        self.env = env\n",
        "        self.alpha_theta = alpha_theta\n",
        "        self.alpha_w = alpha_w\n",
        "        self.gamma = gamma\n",
        "        self.nb_bins = nb_bins\n",
        "\n",
        "        self.nb_states = self.env.observation_space.shape[0]  # 4\n",
        "        self.nb_actions = self.env.action_space.n  # 2\n",
        "\n",
        "        self.theta = np.random.uniform(\n",
        "            -0.001, 0.001, (self.nb_states * self.nb_bins, self.nb_actions)\n",
        "        )  # (40, 2)\n",
        "        self.w = np.random.uniform(-0.001, 0.001, (self.nb_states * self.nb_bins,))  # (40,)\n",
        "\n",
        "    def discretize(self, state):\n",
        "        cart_pos, cart_vel, pole_angle, pole_vel = state\n",
        "        cart_pos = np.digitize(cart_pos, np.linspace(-2.4, 2.4, self.nb_bins - 1))\n",
        "        cart_vel = np.digitize(cart_vel, np.linspace(-3.5, 3.5, self.nb_bins - 1))\n",
        "        pole_angle = np.digitize(pole_angle, np.linspace(-0.4, 0.4, self.nb_bins - 1))\n",
        "        pole_vel = np.digitize(pole_vel, np.linspace(-3.5, 3.5, self.nb_bins - 1))\n",
        "        arr = np.zeros(self.nb_bins * self.nb_states)\n",
        "        arr[cart_pos + self.nb_bins * 0] = 1\n",
        "        arr[cart_vel + self.nb_bins * 1] = 1\n",
        "        arr[pole_angle + self.nb_bins * 2] = 1\n",
        "        arr[pole_vel + self.nb_bins * 3] = 1\n",
        "        return arr  # (40,)\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        def softmax(x):\n",
        "            e_x = np.exp(x - np.max(x))\n",
        "            return e_x / e_x.sum(axis=0)\n",
        "\n",
        "        return softmax(np.dot(self.theta.T, state).reshape(-1))\n",
        "\n",
        "    def run_episode(self, state=None):\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        I = 1\n",
        "\n",
        "        # Initialize S (first state of episode)\n",
        "        if state is None:\n",
        "            state = self.env.reset()[0]\n",
        "            state = self.discretize(state)\n",
        "\n",
        "        # Loop while S is not terminal (for each time step)\n",
        "        while not done and total_reward <= 200:\n",
        "            # Choose action: A ~ pi(.|s, theta)\n",
        "            policy = self.get_policy(state)\n",
        "            action = np.random.choice(self.nb_actions, p=policy)\n",
        "\n",
        "            # Take action A, observe S', R\n",
        "            next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "            next_state = self.discretize(next_state)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            # Update delta: delta <- R + gamma * v_hat(S', w) - v_hat(S, w)    (if S' is terminal, then v_hat(S', w) = 0)\n",
        "            delta = (\n",
        "                reward\n",
        "                + self.gamma * float(np.dot(next_state, self.w.T))\n",
        "                - float(np.dot(state, self.w.T))\n",
        "            )\n",
        "\n",
        "            # Update w: w <- w + alpha_w * delta * grad v_hat(S, w)\n",
        "            self.w += self.alpha_w * delta * state\n",
        "\n",
        "            # Update theta: theta <- theta + alpha_theta * I * delta * grad ln pi(A|S, theta)\n",
        "            self.theta += (\n",
        "                self.alpha_theta * I * delta * np.dot(state.reshape(-1, 1), policy.reshape(1, -1))\n",
        "            )\n",
        "\n",
        "            I *= self.gamma\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "        return total_reward\n",
        "\n",
        "    def run(self, nb_episodes=1000, do_print=True):\n",
        "        rewards = []\n",
        "        for i in range(nb_episodes):\n",
        "            reward = self.run_episode()\n",
        "            rewards.append(reward)\n",
        "            if do_print and i % 100 == 0:\n",
        "                print(f\"Episode {i+1}/{nb_episodes} - Reward: {reward}\")\n",
        "        return rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMz3xEz-NcZr"
      },
      "source": [
        "### 5.4. Experiment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpT4En-kZovH"
      },
      "source": [
        "#### 5.4.1. Running the experiment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2oy5h-_LqUL",
        "outputId": "b66f4089-089c-4919-c600-1fe709850101"
      },
      "outputs": [],
      "source": [
        "nb_episodes = 1000\n",
        "nb_runs = 10\n",
        "alphas = [1 / 16, 1 / 8, 1 / 4]\n",
        "epsilons = [0.01, 0.04, 0.1]\n",
        "\n",
        "# Environment\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "# Data\n",
        "data_Q2 = np.zeros((4, len(epsilons), nb_episodes, nb_runs))\n",
        "\n",
        "for k in range(nb_runs):\n",
        "    print(f\"Run {k+1}/{nb_runs}\")\n",
        "    for i, alpha in enumerate(alphas):\n",
        "        print(f\"\\tAlpha: {alpha}\")\n",
        "        for j, epsilon in enumerate(epsilons):\n",
        "            seed_everything(k, env)\n",
        "\n",
        "            q_learning = QLearningAgent(env, alpha=alpha, epsilon=epsilon)\n",
        "            data_Q2[j][i][:, k] = q_learning.run(nb_episodes=nb_episodes, do_print=False)\n",
        "\n",
        "            print(\n",
        "                f\"\\t\\tQ-Learning - Epsilon: {epsilon} - Avg Reward Last 100 Episodes: {data_Q2[j][i][-100:, k].mean():.2f}\"\n",
        "            )\n",
        "\n",
        "        seed_everything(k, env)\n",
        "\n",
        "        actor_critic = ActorCriticAgent(env, alpha_theta=alpha, alpha_w=alpha)\n",
        "        data_Q2[3][i][:, k] = actor_critic.run(nb_episodes=nb_episodes, do_print=False)\n",
        "\n",
        "        print(\n",
        "            f\"\\t\\tActor-Critic - Avg Reward Last 100 Episodes: {data_Q2[3][i][-100:, k].mean():.2f}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "574OtNUHZxX-"
      },
      "source": [
        "#### 5.4.2. Graphs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-497imWIZ2YI"
      },
      "source": [
        "##### 5.4.2.1. Effect of hyperparameters on performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-dRSG90QY1V5",
        "outputId": "bd99f27e-2799-46f2-dfa7-58ba366c087e"
      },
      "outputs": [],
      "source": [
        "plot(\n",
        "    data=data_Q2,\n",
        "    title=\"Question 2\\nQ-learning & Actor-Critic\",\n",
        "    main_labels=[\"Returns\", \"Returns\", \"Returns\", \"Returns\", \"Returns\", \"Returns\"],\n",
        "    ax_titles=[\n",
        "        f\"A. Q-learning with $\\epsilon = {epsilons[0]}$ averaged over ${nb_runs}$ runs\",\n",
        "        f\"B. Q-learning with $\\epsilon = {epsilons[1]}$ averaged over ${nb_runs}$ runs\",\n",
        "        f\"C. Q-learning with $\\epsilon = {epsilons[2]}$ averaged over ${nb_runs}$ runs\",\n",
        "        f\"F. Actor-Critic with Softmax averaged over {nb_runs} runs\",\n",
        "    ],\n",
        "    algs_info=[\n",
        "        (rf\"$\\alpha = {alphas[0]}$\", \"tab:blue\", 0),\n",
        "        (rf\"$\\alpha = {alphas[1]}$\", \"tab:green\", 0),\n",
        "        (rf\"$\\alpha = {alphas[2]}$\", \"tab:orange\", 0),\n",
        "    ],\n",
        "    size=(10, 30),\n",
        "    range_y=[(0, 200), (0, 200), (0, 200), (0, 200), (0, 200), (0, 200)],\n",
        "    fill_std=[0, 0, 0, 0, 0, 0],\n",
        "    filename=\"q2.png\" if Q2C.SAVE_FILES else None,\n",
        "    show=Q2C.SHOW_GRAPHS,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpKEipiDdsYu"
      },
      "source": [
        "#### 5.4.2.2. Best Parameters\n",
        "\n",
        "We can see what the best hyperparameters are to maximize returns. For Q-Learning, $\\alpha = 1/8$ and $\\epsilon = 0.01$ are optimal; for Actor-Critic, $\\alpha = 1/4$ gives the highest average results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 776
        },
        "id": "9B8r20x3-WZ7",
        "outputId": "be83a91a-4393-47a9-d9e5-eae53f0928c8"
      },
      "outputs": [],
      "source": [
        "data_Q2_best = np.zeros((1, 2, nb_episodes, nb_runs))\n",
        "\n",
        "best_alpha_q = 1\n",
        "best_eps_q = 0\n",
        "best_alpha_ac = 2\n",
        "\n",
        "data_Q2_best[0][0] = data_Q2[best_eps_q][best_alpha_q]\n",
        "data_Q2_best[0][1] = data_Q2[3][best_alpha_ac]\n",
        "\n",
        "# Plot #2: Q-learning and Actor-Critic best results\n",
        "SAVE_FILES = True\n",
        "SHOW_GRAPHS = True\n",
        "\n",
        "plot(\n",
        "    data=data_Q2_best,\n",
        "    title=\"Question 2\",\n",
        "    main_labels=[\"Reward\"],\n",
        "    ax_titles=[f\"Best Q-learning and best Actor-Critic averaged over {nb_runs} runs\"],\n",
        "    algs_info=[\n",
        "        (\n",
        "            rf\"Q-learning with $\\alpha = {alphas[best_alpha_q]}$ and $\\epsilon = {epsilons[best_eps_q]}$\",\n",
        "            \"tab:orange\",\n",
        "            0,\n",
        "        ),\n",
        "        (rf\"Actor-Critic with $\\alpha = {alphas[best_alpha_ac]}$ and Softmax\", \"tab:red\", 0),\n",
        "    ],\n",
        "    size=(10, 12),\n",
        "    range_y=[(0, 200)],\n",
        "    fill_std=[0],\n",
        "    filename=\"q2_best.png\" if SAVE_FILES else None,\n",
        "    show=SHOW_GRAPHS,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H07rdS13TgHh"
      },
      "source": [
        "### 5.5. Report\n",
        "\n",
        "We implemented Q-learning and Actor-Critic with linear function approximation on the cart-pole domain from the Gym environment suite. We used a function approximator in which we discretize the state variables into 10 bins each, with weights initialized randomly using a uniform distribution between $−0.001$ and $0.001$.\n",
        "\n",
        "#### **Selection of hyperparameters**\n",
        "\n",
        "We used 3 settings of the learning rate parameter $\\alpha$ = 1/4, 1/8, 1/16. For the Q-Learning agents, we used $\\epsilon$-greedy exploration with $\\epsilon$ = 0.01, 0.04, 0.1; for the Actor-Critic Agents, we used Softmax exploration instead.\n",
        "\n",
        "We performed 10 independent runs, each of 1000 episodes.\n",
        "\n",
        "We plot 3 graphs for $\\epsilon$-greedy, and one for Actor-Critic, each containing the average and standard error of the learning curves for each value of $\\alpha$.\n",
        "\n",
        "**_Figure 1:_** _Effects of hyperparameters on training performance._\n",
        "<img src=\"https://i.imgur.com/TQRO6Vq.png\" width=600/>\n",
        "\n",
        "As we can see, for Q-Learning, lower values for $\\epsilon$ did better. Actor-Critic was surprisingly performant, though its performance deteriorated over time after an initial jump in performance. This suggests that decreasing slightly the gamma value might have increased performance ($\\gamma$ = 1 for our experiments).\n",
        "\n",
        "For Q-Learning, $\\alpha = 1/8$ and $\\epsilon = 0.01$ gave the highest mean returns over the 10 runs and 1000 episodes; for Actor-Critic, $\\alpha = 1/4$ gives the highest mean returns. However, for Q-Learnnig with $\\epsilon = 0.01$, we can see that though $\\alpha = 1/8$ gave better average results, it gave much less stable results, and results that were ultimately roughly equivalent to $\\alpha = 1/16$, which could be used to argue that it was a better hyperparameter for our problem. Still, we decided to choose $\\alpha = 1/8$.\n",
        "\n",
        "**_Figure 2:_** _Best hyperparameters for Q-Learning and Actor-Critic on training performance._\n",
        "<img src=\"https://i.imgur.com/193ONJS.png\" width=600/>\n",
        "\n",
        "Here, we notice more easily the striking difference of learning speeds between Q-Learning and Actor-Critic. Actor-Critic initially learns quickly to get good returns, but deteriorates after the 50th episode; comparatively, the optimal Q-Learning agent learns slowly, but is caught up to the optimal Actor-Critic agent after 1000 episodes.\n",
        "\n",
        "Additionally, we observe that the Q-Learning curve is less stable than the Actor-Critic curve. This is expected, and a known strength of Actor-Critic agents over Q-Learning agents.\n",
        "\n",
        "#### **Conclusion**\n",
        "\n",
        "In summary, we applied Q-Learning and Actor-Critic algorithms with linear function approximation to the cart-pole domain. We then tested various hyperparameters to assess their performance. The Q-Learning agents explored using $\\epsilon$-greedy, while the Actor-Critic agents used Softmax exploration.\n",
        "\n",
        "Our findings show that the Q-Learning algorithm with $\\alpha = 1/8$ and $\\epsilon = 0.01$ provided the best overall performance in terms of mean returns, although it was less stable than the Actor-Critic algorithm. In contrast, the Actor-Critic algorithm with $\\alpha = 1/4$ achieved the highest mean returns and displayed faster initial learning, but its performance deteriorated over time.\n",
        "\n",
        "Comparing the two algorithms, we note that Actor-Critic agents learn more quickly at the beginning and have more consistent performance than Q-Learning agents. This is because Actor-Critic agents use a value function and a policy function to learn a better representation of the environment and exploit it more efficiently. In contrast, Q-Learning agents rely solely on a value function, which can lead to instability issues, particularly when using function approximation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "jVurvTU2vveb",
        "pLWVXV-Yumbk",
        "yUTpp-SGwFZl",
        "jFxecU_GVII1",
        "znn9bozWNo_z",
        "_Q54CvTyNU0T"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
